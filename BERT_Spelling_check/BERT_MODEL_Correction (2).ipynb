{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, BertModel\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\")\n",
    "labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "outputs = model(**inputs, labels=labels)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification, BertPreTrainedModel, BertConfig, BertTokenizer, BertModel\n",
    "from transformers import BertForMaskedLM, BertPreTrainedModel, BertForSequenceClassification\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OMS': 11523, 'MOD': 1995, 'MST': 11261, 'ADT': 8745, '009': 1} 11261 11261\n"
     ]
    }
   ],
   "source": [
    "#加入CWC語料\n",
    "#用誤代語料，然後產生標記檔 \n",
    "news = open('1209\\\\newsCorpus.txt', 'r', encoding='utf-8')\n",
    "err = open('1209\\\\對比資料庫.txt', 'r', encoding='utf-8')  \n",
    "#test_err = open('1209\\\\test_err.txt', 'w', encoding='utf-8')\n",
    "max_len = 110\n",
    "err_type = []\n",
    "        \n",
    "test_label = []\n",
    "with open('1209\\\\train_dataset.txt', 'w', encoding='utf-8') as train_data:\n",
    "        t = {}\n",
    "        e_num = 0\n",
    "        while True: \n",
    "            err_line = err.readline()\n",
    "            if not err_line:break\n",
    "            lineTo = err_line.split(',')\n",
    "            err_sen = lineTo[1].replace(' ','')\n",
    "            if len(lineTo[1].replace(' ',''))<=3 and len(lineTo[0].replace(' ',''))<=3:continue\n",
    "            #if err_sen.find(lineTo[11].replace(' ','')) < 0 or err_sen.count(lineTo[11].replace(' ',''))>1:continue\n",
    "            if ('＆' in lineTo[1]) or ('&' in lineTo[1]) or ('+' in lineTo[1])  or ('＄' in lineTo[1]) or ('$' in lineTo[1]):continue\n",
    "            if len(lineTo[0].replace(' ',''))>110:continue\n",
    "            if len(lineTo)>0 : \n",
    "                if len(lineTo[9])==3:\n",
    "                    if lineTo[9] in t.keys():t[lineTo[9]] += 1\n",
    "                    else:t[lineTo[9]] = 1\n",
    "                        \n",
    "                if len(lineTo[9])==3:\n",
    "                    if lineTo[9]=='MST':\n",
    "                        err_sen = lineTo[1].split()\n",
    "                        cor_sen = lineTo[0].split()\n",
    "                        #print(err_sen, cor_sen)                        \n",
    "                        \n",
    "                        labels = ''\n",
    "                        for i in range(len(err_sen)):\n",
    "                            if cor_sen[i] != err_sen[i]:labels += '1'*len(err_sen[i])\n",
    "                            else:labels += '0'*len(err_sen[i])\n",
    "                        for i in range(max_len-len(lineTo[1].replace(' ',''))):\n",
    "                            labels += '0'\n",
    "                        \n",
    "                        #train_data.write(lineTo[0].replace(' ','')+','+lineTo[1].replace(' ','')+','+labels+'\\n')#寫修正句&偏誤句\n",
    "                        train_data.write(lineTo[1].replace(' ','')+','+labels+','+lineTo[0].replace(' ','')+'\\n')#寫誤代句\n",
    "                        e_num += 1\n",
    "        \n",
    "        n_num = 0\n",
    "        \n",
    "        while True:\n",
    "            labels = ''\n",
    "            news_line = news.readline()\n",
    "            if n_num >= 11261:break\n",
    "            if not news_line:break\n",
    "            for _ in range(max_len):labels += '0'\n",
    "            train_data.write(news_line.replace('\\n','') + ',' + labels + '\\n')#寫新聞句            \n",
    "            n_num += 1 \n",
    "            \n",
    "news.close()\n",
    "err.close()\n",
    "print(t, e_num, n_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200050 0\n"
     ]
    }
   ],
   "source": [
    "e_num = 0\n",
    "n_num = 0\n",
    "confusion = open(\"1209\\\\字音混淆集.txt\",'r',encoding='utf-8') #經過字形與字音相似度計算後，為相似字的表\n",
    "\n",
    "dict={}\n",
    "while(True):\n",
    "    line = confusion.readline().strip()\n",
    "    \n",
    "    if line:\n",
    "        line = line.split('　')\n",
    "        if len(line)!=1:\n",
    "            dict[line[0]] = line[1]\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "confusion.close()\n",
    "\n",
    "table = open(\"1209\\\\wordtest4.txt\",'r',encoding='utf-8') #要挑的字表\n",
    "s=0\n",
    "dict2={}\n",
    "\n",
    "while(True):\n",
    "    line = table.readline().strip()\n",
    "    \n",
    "    if not line:break\n",
    "    line = line.split(',')\n",
    "    dict2[s] = line[0]\n",
    "    s+=1\n",
    "        \n",
    "table.close()\n",
    "\n",
    "import random\n",
    "max_len = 110\n",
    "def test_(c): #產生錯字\n",
    "    if random.random() <= 0.8: #有0.8的機率是相似的錯字\n",
    "        line = dict[c].split(' ')\n",
    "        return line[random.randint(0,len(line)-1)]\n",
    "    else:    #有0.2的機率是隨機抽字\n",
    "        a = random.randint(0,len(dict2)-1)\n",
    "        while(c==dict2[a]):\n",
    "            a = random.randint(0,len(dict2)-1)\n",
    "        return dict2[a]\n",
    "\n",
    "file =  open(\"1209\\\\newsCorpus.txt\",'r',encoding='utf-8') #要被變成訓練資料的句子，也是校正層解答\n",
    "answer_list=''\n",
    "a=0\n",
    "file2 =  open(\"1209\\\\train_dataset.txt\",'w',encoding='utf-8') #產生的訓練資料句\n",
    "\n",
    "while True:\n",
    "    \n",
    "    line2=''\n",
    "    answer=[]\n",
    "    line = file.readline()\n",
    "    if not line:break\n",
    "    if e_num >= 200050: break\n",
    "    for ch in ['， ', ', ', ' ,', ',']:#\n",
    "        line = line.replace(ch, '，')\n",
    "    line = line.replace('︵', '(')\n",
    "    line = line.replace('︶', ')')\n",
    "    line = line.replace(':', '：')\n",
    "    if len(line)>max_len:continue\n",
    "    e_num += 1\n",
    "    for i in line.replace('\\n', ''):\n",
    "        if a==0:\n",
    "            if random.randint(1,15) == 10: #有1/15的機率 把這個字當成錯字\n",
    "                a+=1\n",
    "                if dict.get(i) != None: #若這個字不再字表中，則選擇下一個字為錯字 會有a去計數\n",
    "                    line2+=test_(i)\n",
    "                    a-=1\n",
    "                    answer.append(1)\n",
    "                else:\n",
    "                    line2+=i\n",
    "                    answer.append(0)\n",
    "            else:\n",
    "                line2+=i\n",
    "                answer.append(0)\n",
    "                \n",
    "        else:   #若a>0以上，則要一直挑錯字，直到a==0\n",
    "            if dict.get(i) != None: #\n",
    "                line2+=test_(i)\n",
    "                a-=1\n",
    "                answer.append(1)\n",
    "            else:\n",
    "                line2+=i\n",
    "                answer.append(0)\n",
    "                \n",
    "    for j in answer:  #寫入答案\n",
    "        answer_list+=str(j)\n",
    "    for i in range(max_len-len(answer_list)):answer_list += '0'\n",
    "    file2.write(line2 + ',' + answer_list + ',' + line.replace('\\n', '') + '\\n')\n",
    "    answer_list=''\n",
    "'''\n",
    "news = open('1209\\\\newsCorpus.txt', 'r', encoding='utf-8')\n",
    "n_num = 0\n",
    "while True:\n",
    "    labels = ''\n",
    "    news_line = news.readline()\n",
    "    if not news_line:break\n",
    "    if n_num >= 1e5:break\n",
    "    for _ in range(max_len):labels += '0'\n",
    "    file2.write(news_line.replace('\\n','') + ',' + labels + '\\n')#寫新聞句 \n",
    "    n_num += 1\n",
    "    \n",
    "news.close()\n",
    "'''\n",
    "file2.close()  \n",
    "file.close()\n",
    "print(e_num, n_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['衛生處表示，再過一段時日，衛生處將以各醫療院所的營運[MASK]效，比[MASK]這一制度實施前後的差異。', '李總統[MASK]輝先生[MASK]天上[MASK]十一時前往台北新光三越百貨公司，參[MASK]造型藝術家楊英[MASK]的個展。'] ['[MASK]共與蘇聯以馬來西亞政[MASK]的客人身份出席上次的東協會議。'] [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]] [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]] ['衛生處表示，再過一段時日，衛生處將以各醫療院所的營運績效，比較這一制度實施前後的差異。', '李總統登輝先生今天上午十一時前往台北新光三越百貨公司，參觀造型藝術家楊英風的個展。'] ['中共與蘇聯以馬來西亞政府的客人身份出席上次的東協會議。'] 200049 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x, y, tar = [], [], []\n",
    "with open('1209\\\\train_dataset.txt', 'r', encoding='utf-8') as train_data:\n",
    "    while True:\n",
    "        line = train_data.readline()\n",
    "        if not line:break\n",
    "        line = line.replace('\\n', '')\n",
    "        data = line.split(',')\n",
    "        tmp = list(data[0])\n",
    "        for i in range(len(tmp)):            \n",
    "            if data[1][i] == '1':                \n",
    "                tmp[i] = '[MASK]'\n",
    "                data[0] = ''.join(tmp)\n",
    "        x.append(data[0])\n",
    "        tar.append(data[2])\n",
    "        y.append([int(i) for i in data[1]])\n",
    "        #y.append([int(i) for i in data[1]])\n",
    "\n",
    "\n",
    "x_train, x_val, y_train, y_val, t_train, t_val =\\\n",
    "    train_test_split(x, y, tar, test_size=0.00000075, random_state=2021)\n",
    "print(x_train[:2], x_val[:2], y_train[:2], y_val[:2], t_train[:2], t_val[:2], len(x_train), len(x_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    - Remove entity mentions (eg. '@united')\n",
    "    - Correct errors (eg. '&amp;' to '&')\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (Str): the processed string.\n",
    "    \"\"\"\n",
    "    # Remove '@name'\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = max_len\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "def preprocessing_for_bert(data):\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(sent),  # Preprocess sentence\n",
    "            add_special_tokens=False,\n",
    "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            return_attention_mask=True\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    \n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\islab\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers-4.1.1-py3.8.egg\\transformers\\tokenization_utils_base.py:2179: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6127, 4495, 5993, 6134, 4850, 8024, 1086, 6882,  671, 3667, 3229, 3189,\n",
      "         8024, 6127, 4495, 5993, 2200,  809, 1392, 7015, 4615, 7368, 2792, 4638,\n",
      "         4245, 6880,  103, 3126, 8024, 3683,  103, 6857,  671, 1169, 2428, 2179,\n",
      "         3177, 1184, 2527, 4638, 2345, 4530,  511,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [3330, 5244, 5186,  103, 6740, 1044, 4495,  103, 1921,  677,  103, 1282,\n",
      "          671, 3229, 1184, 2518, 1378, 1266, 3173, 1045,  676, 6632, 4636, 6515,\n",
      "         1062, 1385, 8024, 1347,  103, 6863, 1798, 5971, 6123, 2157, 3501, 5739,\n",
      "          103, 4638,  943, 2245,  511,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0]]) tensor([[6127, 4495, 5993, 6134, 4850, 8024, 1086, 6882,  671, 3667, 3229, 3189,\n",
      "         8024, 6127, 4495, 5993, 2200,  809, 1392, 7015, 4615, 7368, 2792, 4638,\n",
      "         4245, 6880, 5245, 3126, 8024, 3683, 6733, 6857,  671, 1169, 2428, 2179,\n",
      "         3177, 1184, 2527, 4638, 2345, 4530,  511,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0],\n",
      "        [3330, 5244, 5186, 4633, 6740, 1044, 4495,  791, 1921,  677, 1286, 1282,\n",
      "          671, 3229, 1184, 2518, 1378, 1266, 3173, 1045,  676, 6632, 4636, 6515,\n",
      "         1062, 1385, 8024, 1347, 6223, 6863, 1798, 5971, 6123, 2157, 3501, 5739,\n",
      "         7591, 4638,  943, 2245,  511,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0]])\n"
     ]
    }
   ],
   "source": [
    "train_inputs, train_masks = preprocessing_for_bert(x_train)\n",
    "val_inputs, val_masks = preprocessing_for_bert(x_val)\n",
    "train_target, _ = preprocessing_for_bert(t_train)\n",
    "val_target, _ = preprocessing_for_bert(t_val)\n",
    "\n",
    "#train_inputs, train_masks = preprocessing_for_bert_At(x_train)\n",
    "#val_inputs, val_masks = preprocessing_for_bert_At(x_val)\n",
    "print(train_inputs[:2], train_target[:2])\n",
    "\n",
    "text = tokenizer.decode(train_inputs[0])\n",
    "\n",
    "tokens = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-7.2949, -7.7219, -7.5998,  ..., -6.1942, -6.6094, -6.4655],\n",
      "         [-7.6703, -8.2830, -7.9534,  ..., -6.8050, -6.5187, -7.4043],\n",
      "         [-7.6618, -8.8557, -8.4727,  ..., -5.8701, -6.5379, -7.7515],\n",
      "         ...,\n",
      "         [-4.7679, -6.0405, -5.6331,  ..., -3.4793, -2.6060, -3.0014],\n",
      "         [-4.3975, -5.5132, -5.1869,  ..., -3.4103, -2.4649, -3.7611],\n",
      "         [-4.5347, -5.6231, -5.2919,  ..., -3.4118, -2.4757, -3.9307]]])\n",
      "輸入 tokens ： ['報', '導', '中', '說', '，', '[MASK]', '他', '國', '家', '[MASK]', '其', '是', '利', '比', '亞', '，'] ...\n",
      "--------------------------------------------------\n",
      "Top 1 (99%)：['報', '導', '中', '說', '，', '其', '他', '國', '家', '[MASK]', '其', '是', '利', '比', '亞', '，'] ...\n",
      "Top 2 ( 0%)：['報', '導', '中', '說', '，', '歐', '他', '國', '家', '[MASK]', '其', '是', '利', '比', '亞', '，'] ...\n",
      "Top 3 ( 0%)：['報', '導', '中', '說', '，', '鄰', '他', '國', '家', '[MASK]', '其', '是', '利', '比', '亞', '，'] ...\n",
      "Top 1 (92%)：['報', '導', '中', '說', '，', '鄰', '他', '國', '家', '尤', '其', '是', '利', '比', '亞', '，'] ...\n",
      "Top 2 ( 7%)：['報', '導', '中', '說', '，', '鄰', '他', '國', '家', '，', '其', '是', '利', '比', '亞', '，'] ...\n",
      "Top 3 ( 0%)：['報', '導', '中', '說', '，', '鄰', '他', '國', '家', '特', '其', '是', '利', '比', '亞', '，'] ...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "這段程式碼載入已經訓練好的 masked 語言模型並對有 [MASK] 的句子做預測\n",
    "\"\"\"\n",
    "PRETRAINED_MODEL_NAME = \"hfl/chinese-roberta-wwm-ext\"\n",
    "# 除了 tokens 以外我們還需要辨別句子的 segment ids\n",
    "tokens_tensor = torch.tensor(torch.unsqueeze(train_inputs[0], 0))  # (1, seq_len)\n",
    "segments_tensors = torch.zeros_like(tokens_tensor)  # (1, seq_len)\n",
    "print(tokens_tensor.size(), segments_tensors.size())\n",
    "maskedLM_model = BertForMaskedLM.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "clear_output()\n",
    "\n",
    "# 使用 masked LM 估計 [MASK] 位置所代表的實際 token \n",
    "maskedLM_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = maskedLM_model(tokens_tensor, segments_tensors)\n",
    "    predictions = outputs[0]\n",
    "    # (1, seq_len, num_hidden_units)\n",
    "#del maskedLM_model\n",
    "\n",
    "# 將 [MASK] 位置的機率分佈取 top k 最有可能的 tokens 出來\n",
    "masked_index = 5\n",
    "k = 3\n",
    "probs, indices = torch.topk(torch.softmax(predictions[0, masked_index], -1), k)\n",
    "probs_2, indices_2 = torch.topk(torch.softmax(predictions[0, 9], -1), k)\n",
    "predicted_tokens = tokenizer.convert_ids_to_tokens(indices.tolist())\n",
    "predicted_tokens_2 = tokenizer.convert_ids_to_tokens(indices_2.tolist())\n",
    "print(predictions)\n",
    "\n",
    "# 顯示 top k 可能的字。一般我們就是取 top 1 當作預測值\n",
    "print(\"輸入 tokens ：\", tokens[:16], '...')\n",
    "print('-' * 50)\n",
    "for i, (t, p) in enumerate(zip(predicted_tokens, probs), 1):\n",
    "    tokens[masked_index] = t\n",
    "    print(\"Top {} ({:2}%)：{}\".format(i, int(p.item() * 100), tokens[:16]), '...')\n",
    "\n",
    "for i, (t, p) in enumerate(zip(predicted_tokens_2, probs_2), 1):\n",
    "    tokens[9] = t\n",
    "    print(\"Top {} ({:2}%)：{}\".format(i, int(p.item() * 100), tokens[:16]), '...')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[6127, 4495, 5993, 6134, 4850, 8024, 1086, 6882,  671, 3667, 3229, 3189,\n",
       "           8024, 6127, 4495, 5993, 2200,  809, 1392, 7015, 4615, 7368, 2792, 4638,\n",
       "           4245, 6880,  103, 3126, 8024, 3683,  103, 6857,  671, 1169, 2428, 2179,\n",
       "           3177, 1184, 2527, 4638, 2345, 4530,  511,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0]]),\n",
       "  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       "  tensor([[6127, 4495, 5993, 6134, 4850, 8024, 1086, 6882,  671, 3667, 3229, 3189,\n",
       "           8024, 6127, 4495, 5993, 2200,  809, 1392, 7015, 4615, 7368, 2792, 4638,\n",
       "           4245, 6880, 5245, 3126, 8024, 3683, 6733, 6857,  671, 1169, 2428, 2179,\n",
       "           3177, 1184, 2527, 4638, 2345, 4530,  511,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0]]),\n",
       "  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])),\n",
       " '衛 生 處 表 示 ， 再 過 一 段 時 日 ， 衛 生 處 將 以 各 醫 療 院 所 的 營 運 [MASK] 效 ， 比 [MASK] 這 一 制 度 實 施 前 後 的 差 異 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " (tensor([[ 103, 1066, 5645, 5979, 5474,  809, 7679,  889, 6205,  765, 3124,  103,\n",
       "           4638, 2145,  782, 6716,  819, 1139, 2375,  677, 3613, 4638, 3346, 1295,\n",
       "           3298, 6359,  511,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0]]),\n",
       "  tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       "  tensor([[ 704, 1066, 5645, 5979, 5474,  809, 7679,  889, 6205,  765, 3124, 2424,\n",
       "           4638, 2145,  782, 6716,  819, 1139, 2375,  677, 3613, 4638, 3346, 1295,\n",
       "           3298, 6359,  511,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0]]),\n",
       "  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])),\n",
       " '[MASK] 共 與 蘇 聯 以 馬 來 西 亞 政 [MASK] 的 客 人 身 份 出 席 上 次 的 東 協 會 議 。 [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(y_train)\n",
    "val_labels = torch.tensor(y_val)\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_labels, train_target, train_masks)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_labels, val_target, val_masks)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "train_data[:1], tokenizer.decode(train_data[:1][0][0]), val_data[:1], tokenizer.decode(val_data[:1][0][0]), len(val_data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(y_train)\n",
    "val_labels = torch.tensor(y_val)\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 64\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_labels, train_masks)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_labels, val_masks)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "train_data[:1], tokenizer.decode(train_data[:1][0][0]), val_data[:1], tokenizer.decode(val_data[:1][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "474"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "maskedLM_model = maskedLM_model.to(device)\n",
    "def get_test_result(dataloader):\n",
    "    y_real_s = []\n",
    "    y_pred_s = []\n",
    "    cor_count = 0\n",
    "    \n",
    "    data = {\n",
    "    \"真實標記\":[],\n",
    "    \"預測標記\":[]\n",
    "    }\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    for batch in dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_labels, b_target_ids, b_masks = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        for b_input_id, b_label, b_target_id, b_mask in zip(b_input_ids, b_labels, b_target_ids, b_masks):\n",
    "            \n",
    "            masked_index = torch.nonzero(b_label).cpu().numpy()[:,0]\n",
    "            if len(masked_index) == 0:continue\n",
    "            # Compute logits\n",
    "            with torch.no_grad():\n",
    "                logits = maskedLM_model(input_ids = torch.unsqueeze(b_input_id, 0), attention_mask = torch.unsqueeze(b_mask, 0), labels=torch.unsqueeze(b_target_id, 0) )   \n",
    "            # Compute loss\n",
    "            #losses = logits[0]\n",
    "            m_logits = logits[1].cpu().numpy()\n",
    "            mask_logits = torch.tensor([m_logits[0][i] for i in masked_index])\n",
    "            real_ids = torch.tensor([b_target_id.cpu().numpy()[i] for i in masked_index])\n",
    "            pred_ids = torch.max(mask_logits, 1)[1].data\n",
    "            if torch.equal(real_ids, pred_ids):cor_count += 1\n",
    "            '''\n",
    "            ## print the result\n",
    "            print(torch.max(mask_logits, 1)[1].data, real_ids)\n",
    "            print(loss(mask_logits, real_ids), losses)\n",
    "            #loss(mask_logits, real_ids)\n",
    "            text = tokenizer.decode(b_input_id)\n",
    "            tokens = tokenizer.tokenize(text)\n",
    "\n",
    "            print(\"輸入 tokens ：\", tokens[:])\n",
    "            print('-' * 50)\n",
    "            for mask_id in masked_index:\n",
    "                probs, indices = torch.topk(torch.softmax(logits[1][0][mask_id], -1), 1)\n",
    "                predicted_tokens = tokenizer.convert_ids_to_tokens(indices.tolist())\n",
    "                for i, (t, p) in enumerate(zip(predicted_tokens, probs), 1):\n",
    "                    tokens[mask_id] = t\n",
    "            print(\"{}\".format(tokens[:]))\n",
    "            '''\n",
    "        # Get the predictions    \n",
    "        #pred = torch.max(logits[1], 1)[1].data\n",
    "        #print(loss, pred)\n",
    "        #y_pred_s += list(pred.cpu().numpy())          \n",
    "        #y_real_s += list(b_labels.cpu().numpy())\n",
    "\n",
    "        #sentence = tokenizer.decode(b_input_ids)\n",
    "    #df = pd.DataFrame(data)\n",
    "    del maskedLM_model\n",
    "    return cor_count\n",
    "\n",
    "# Compute the average accuracy and loss over the validation set.\n",
    "\n",
    "#y_real_s, y_pred_s = get_test_result(val_dataloader)\n",
    "get_test_result(train_dataloader)\n",
    "#df.to_csv('test/Bert{}EpochErr_{}.csv'.format(epochs,mode))\n",
    "#df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    \n",
    "    #model = BertForTokenClassification.from_pretrained(PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "    #model = BertClassifier(PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "    model = BertForMaskedLM.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def train(model, train_dataloader, epochs=4):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        \n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_labels, b_attn_mask = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits.view(-1, 2), b_labels.view(-1))\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        print('[epoch %d] loss: %.3f' % (epoch_i + 1, total_loss))\n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"\n",
    "NUM_LABELS = 2\n",
    "epochs = 20\n",
    "set_seed(42) \n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs)\n",
    "losses = train(bert_classifier, train_dataloader, epochs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "valIter = iter(val_dataloader)\n",
    "data = next(valIter)\n",
    "ids = data[0].to(device)\n",
    "label = data[1].to(device)\n",
    "mask = data[2].to(device)\n",
    "model.eval()\n",
    "output = model(input_ids=ids, labels=label)#, labels=label)\n",
    "print(output)\n",
    "output[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|                                                                                           | 0/40 [11:23<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-7d17f36dd7ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mset_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mbert\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitialize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-7d17f36dd7ea>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, epochs)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers-4.1.1-py3.8.egg\\transformers\\optimization.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    293\u001b[0m                 \u001b[1;31m# In-place operations to update the averages at the same time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"eps\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(model, train_dataloader, epochs=4):\n",
    "    Loss = []\n",
    "    \n",
    "    model.train() #訓練模式\n",
    "    for i in tqdm(range(epochs)):  \n",
    "        batch_counts = 0\n",
    "        running_loss = 0.0\n",
    "        for step, batch in enumerate(train_dataloader): #訓練資料\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_labels, b_target_ids, b_masks = tuple(t.to(device) for t in batch)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(input_ids=b_input_ids, labels=b_target_ids, attention_mask=b_masks)\n",
    "            loss = out[0]\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "        torch.save(model.state_dict(), 'ckpt/bert_weight_'+str(i+1)+'.h5')\n",
    "        Loss.append(running_loss)\n",
    "        print('[epoch %d] loss: %.3f' % (i + 1, running_loss))\n",
    "        \n",
    "    return Loss\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"hfl/chinese-roberta-wwm-ext\"\n",
    "NUM_LABELS = 2\n",
    "epochs = 40\n",
    "set_seed(42)\n",
    "bert, optimizer, scheduler = initialize_model(epochs)\n",
    "losses = train(bert, train_dataloader, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "print(losses)\n",
    "x = np.arange(1, len(losses)+1)\n",
    "y = losses\n",
    "plt.xlabel('EPOCH')\n",
    "plt.ylabel('LOSS')  \n",
    "plt.xticks(x)\n",
    "plt.plot(x, y)\n",
    "plt.savefig('test/Berttransloss{}.jpg'.format(epochs))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=21128, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('ckpt/bert_weight_34.h5')\n",
    "bert.load_state_dict(checkpoint)\n",
    "bert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "def get_test_result(dataloader):\n",
    "    bert.eval()\n",
    "    y_real_s = []\n",
    "    y_pred_s = []\n",
    "    cor_count = 0\n",
    "    err_count = 0\n",
    "    data = {\n",
    "    \"真實標記\":[],\n",
    "    \"預測標記\":[]\n",
    "    }\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    for batch in dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_labels, b_target_ids, b_masks = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        for b_input_id, b_label, b_target_id, b_mask in zip(b_input_ids, b_labels, b_target_ids, b_masks):\n",
    "            \n",
    "            masked_index = torch.nonzero(b_label).cpu().numpy()[:,0]\n",
    "            token = [int(i) for i in b_input_id.cpu().numpy() if i != 0] \n",
    "            sentence = tokenizer.decode(b_target_id[:len(token)]) #轉成中文句\n",
    "            realList = sentence.split()\n",
    "            corList = sentence.split()\n",
    "                \n",
    "            if len(masked_index) == 0 or '[UNK]' in realList or 'ｓａｍｇｅｔａｎｇ' in realList:continue\n",
    "            \n",
    "            # Compute logits\n",
    "            with torch.no_grad():\n",
    "                logits = bert(input_ids = torch.unsqueeze(b_input_id, 0), attention_mask = torch.unsqueeze(b_mask, 0), labels=torch.unsqueeze(b_target_id, 0) )   \n",
    "            # Compute loss\n",
    "            #losses = logits[0]\n",
    "            m_logits = logits[1].cpu().numpy()\n",
    "            mask_logits = torch.tensor([m_logits[0][i] for i in masked_index])\n",
    "            real_ids = torch.tensor([b_target_id.cpu().numpy()[i] for i in masked_index])\n",
    "            pred_ids = torch.max(mask_logits, 1)[1].data\n",
    "        \n",
    "            text = tokenizer.decode(pred_ids)\n",
    "            text = text.split()\n",
    "            count = 0\n",
    "            #print(''.join(realList))\n",
    "            for i in masked_index:\n",
    "                corList[i] = ' [' + text[count] + '] '\n",
    "                realList[i] = ' [' + realList[i] + '] '\n",
    "                count+=1\n",
    "            data[\"真實標記\"].append(''.join(realList))\n",
    "            data[\"預測標記\"].append(''.join(corList))\n",
    "            cor_count += sum(real_ids == pred_ids)\n",
    "            err_count += list(b_label.cpu().numpy()).count(1)\n",
    "            '''\n",
    "            ## print the result\n",
    "            print(torch.max(mask_logits, 1)[1].data, real_ids)\n",
    "            print(loss(mask_logits, real_ids), losses)\n",
    "            #loss(mask_logits, real_ids)\n",
    "            text = tokenizer.decode(b_input_id)\n",
    "            tokens = tokenizer.tokenize(text)\n",
    "\n",
    "            print(\"輸入 tokens ：\", tokens[:])\n",
    "            print('-' * 50)\n",
    "            for mask_id in masked_index:\n",
    "                probs, indices = torch.topk(torch.softmax(logits[1][0][mask_id], -1), 1)\n",
    "                predicted_tokens = tokenizer.convert_ids_to_tokens(indices.tolist())\n",
    "                for i, (t, p) in enumerate(zip(predicted_tokens, probs), 1):\n",
    "                    tokens[mask_id] = t\n",
    "            print(\"{}\".format(tokens[:]))\n",
    "            '''\n",
    "        # Get the predictions    \n",
    "        #pred = torch.max(logits[1], 1)[1].data\n",
    "        #print(loss, pred)\n",
    "        #y_pred_s += list(pred.cpu().numpy())          \n",
    "        #y_real_s += list(b_labels.cpu().numpy())\n",
    "\n",
    "        #sentence = tokenizer.decode(b_input_ids)\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv('test/RoBertaCorrTestSIGHAN14finetuned.csv')\n",
    "    print(err_count)\n",
    "    return cor_count, df\n",
    "\n",
    "# Compute the average accuracy and loss over the validation set.\n",
    "\n",
    "#y_real_s, y_pred_s = get_test_result(val_dataloader)\n",
    "#num, df = get_test_result(val_dataloader)\n",
    "#df.to_csv('test/Bert{}EpochErr_{}.csv'.format(epochs,mode))\n",
    "#df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1062 1062\n",
      "相對的、每位產齡婦女的生育嬰兒個數卻[MASK]續下滑。這表示全球出現適合年齡生育的婦女不想生育的現象。 相對的、每位產齡婦女的生育嬰兒個數卻持續下滑。這表示全球出現適合年齡生育的婦女不想生育的現象。\n",
      "問[MASK]就在於，從１９５０－１９６０年雖然產齡婦女率低，但平均每位產齡婦女生育的嬰兒率高。 問題就在於，從１９５０－１９６０年雖然產齡婦女率低，但平均每位產齡婦女生育的嬰兒率高。\n",
      "以前戰爭時代沒什麼醫[MASK]設備像現在那麼普遍，但一口家庭生孩子的比率增多，相反地死亡的數量也更多。 以前戰爭時代沒什麼醫療設備像現在那麼普遍，但一口家庭生孩子的比率增多，相反地死亡的數量也更多。\n",
      "在[MASK]，我想最重要的還是能於培養出一群傑出的資源，能於為國家爭榮、願意貢獻一份力量的人才。 在此，我想最重要的還是能於培養出一群傑出的資源，能於為國家爭榮、願意貢獻一份力量的人才。\n",
      "後來因為人民生得越多，人口大幅[MASK]增加，產生了不少社會問題 後來因為人民生得越多，人口大幅地增加，產生了不少社會問題\n",
      "成人市場已經飽和，現在「少子化」商機才是引人注目。能讓企業們轉移注目，「少子化」可不是小問題[MASK]！ 成人市場已經飽和，現在「少子化」商機才是引人注目。能讓企業們轉移注目，「少子化」可不是小問題囉！\n",
      "企業[MASK]突然發現現在的青少年很會花錢。他們去購物時下手闊綽而不會像成人愛討價還價。 企業們突然發現現在的青少年很會花錢。他們去購物時下手闊綽而不會像成人愛討價還價。\n",
      "第三，現在青少年有著多元化學習的機會，出國旅行對他們來說也不成大問題了，所以[MASK]少年所領取的知識比父母上一代豐富極了！ 第三，現在青少年有著多元化學習的機會，出國旅行對他們來說也不成大問題了，所以青少年所領取的知識比父母上一代豐富極了！\n",
      "人的意識方面上，為了減少急[MASK]的的生孩子率，需要呼籲適當的生育政策。 人的意識方面上，為了減少急變的的生孩子率，需要呼籲適當的生育政策。\n",
      "而這樣的[MASK]況到底該稱之為「社會危機」，還是它能為全人類帶來什麼樣的好處嗎？ 而這樣的情況到底該稱之為「社會危機」，還是它能為全人類帶來什麼樣的好處嗎？\n",
      "中國人自古至今有一個刻[MASK]觀念，認為生育男性才有未來，女孩嫁出去就是「潑出去的水」，沒有作用，更無法傳宗接代。 中國人自古至今有一個刻版觀念，認為生育男性才有未來，女孩嫁出去就是「潑出去的水」，沒有作用，更無法傳宗接代。\n",
      "在１９９０愛莎妮亞人口超過１５０萬人，但是現在我[MASK]的人口不到１３５萬人。 在１９９０愛莎妮亞人口超過１５０萬人，但是現在我們的人口不到１３５萬人。\n",
      "我們獨立之後的經濟情況很差，那時[MASK]我們還繼續使用俄羅斯的貨幣，必須面對通貨膨脹的問題。 我們獨立之後的經濟情況很差，那時候我們還繼續使用俄羅斯的貨幣，必須面對通貨膨脹的問題。\n",
      "人口越少，管理整個國家越複雜，哪裡都缺人才，甚至於我們政府得[MASK]移民法律，讓外籍人移民到愛沙尼亞。 人口越少，管理整個國家越複雜，哪裡都缺人才，甚至於我們政府得改移民法律，讓外籍人移民到愛沙尼亞。\n",
      "「少子化」也有一些好處，比方說，我們就[MASK]空間的問題了，房子、院子都越來越大，人民不會碰到因空間不夠而造成衝突這種問題。 「少子化」也有一些好處，比方說，我們就沒空間的問題了，房子、院子都越來越大，人民不會碰到因空間不夠而造成衝突這種問題。\n",
      "追[MASK]到早期的社會，直到現今，人類將隨著各方面的進步而不斷產生「少子化」的現象衍生。 追溯到早期的社會，直到現今，人類將隨著各方面的進步而不斷產生「少子化」的現象衍生。\n",
      "有許多已開發國家將工廠移轉到人口較多的國家，給開發中國家提[MASK]就業的來源。 有許多已開發國家將工廠移轉到人口較多的國家，給開發中國家提供就業的來源。\n",
      "即使再多的人口，但如果懵懂無知的話不但對國家展毫無助益，甚至在一個極端派的煽動之下很容易成為[MASK]社會主義者。 即使再多的人口，但如果懵懂無知的話不但對國家展毫無助益，甚至在一個極端派的煽動之下很容易成為反社會主義者。\n",
      "「少子化」正面的影響可以說是比較少，但是最主要就是可以減少許多的經費及資源，也可改善環境的汙[MASK]。 「少子化」正面的影響可以說是比較少，但是最主要就是可以減少許多的經費及資源，也可改善環境的汙染。\n",
      "而負面的影響最主要的就是人口數的減少會使人[MASK]的工作量增加、所要背負的負擔加重及人口老年化等的問題。 而負面的影響最主要的就是人口數的減少會使人們的工作量增加、所要背負的負擔加重及人口老年化等的問題。\n",
      "時代的改變，慢慢將人類的觀念換新以及改革，人們早已不像從前那樣，不懂得如何避孕，然後意外[MASK]懷了寶寶。 時代的改變，慢慢將人類的觀念換新以及改革，人們早已不像從前那樣，不懂得如何避孕，然後意外地懷了寶寶。\n",
      "韓國政府認為生太多子女會嚴重[MASK]危害國家的經濟成長，且降低國民的生活品質，因此提倡「少生孩子」。 韓國政府認為生太多子女會嚴重地危害國家的經濟成長，且降低國民的生活品質，因此提倡「少生孩子」。\n",
      "因此，若韓國政府未準備好能夠因應「少子化」現象的政策，將會嚴重[MASK]危害韓國政府的財政支出，甚至導致國家破產。 因此，若韓國政府未準備好能夠因應「少子化」現象的政策，將會嚴重地危害韓國政府的財政支出，甚至導致國家破產。\n",
      "人口負成長確實會造成不少困難，經濟[MASK]退是其嚴重的結果。 人口負成長確實會造成不少困難，經濟衰退是其嚴重的結果。\n",
      "但反觀想，人口減少或許也代表這個國家不會如人口過多的國家[MASK]亂，國家政府更不用為那麼多人操心。 但反觀想，人口減少或許也代表這個國家不會如人口過多的國家的亂，國家政府更不用為那麼多人操心。\n",
      "對於開發中國家的人，政府可以試試教育他們人口太多的風險及後果，但我覺得針對這[MASK]問題，要以政策來解決較於適當及簡單。 對於開發中國家的人，政府可以試試教育他們人口太多的風險及後果，但我覺得針對這個問題，要以政策來解決較於適當及簡單。\n",
      "例如：每個家庭只限有兩個小孩、或超過三個孩子要繳付更多稅等等。這樣的法律才能直接並且有效[MASK]將這嚴重的問題舒緩。 例如：每個家庭只限有兩個小孩、或超過三個孩子要繳付更多稅等等。這樣的法律才能直接並且有效地將這嚴重的問題舒緩。\n",
      "聯合國報告表示全球人口一直不[MASK]的增加，但是大量的新增人口來自於開發中國家。 聯合國報告表示全球人口一直不斷的增加，但是大量的新增人口來自於開發中國家。\n",
      "再來，在電視新聞或報[MASK]我們常常看到很多婚姻問題。比如說，老公打老婆、離婚，等等。 再來，在電視新聞或報紙我們常常看到很多婚姻問題。比如說，老公打老婆、離婚，等等。\n",
      "比如說，發生戰爭如果很少人也無法打贏吧，因為武器多[MASK]進也好也要有人去控制。 比如說，發生戰爭如果很少人也無法打贏吧，因為武器多先進也好也要有人去控制。\n",
      "正面的話就是人人都有工作不用怕沒錢賺，這樣[MASK]民就會少許多。 正面的話就是人人都有工作不用怕沒錢賺，這樣遊民就會少許多。\n",
      "如果市場做適當處理，把產品減少，這樣不會發生有工作可是沒人[MASK]的事情。 如果市場做適當處理，把產品減少，這樣不會發生有工作可是沒人做的事情。\n",
      "首先是工作態度要表現到最佳狀態，以好受到上司的肯定及信[MASK]。 首先是工作態度要表現到最佳狀態，以好受到上司的肯定及信賴。\n",
      "我還記得你[MASK]時說你父親生病了，他身體有沒有好一點，請你跟他代提我跟他說「早點好！」。 我還記得你那時說你父親生病了，他身體有沒有好一點，請你跟他代提我跟他說「早點好！」。\n",
      "是的話，很好因為我那個時候還沒有放假，我可以好好[MASK]帶你去台北很多好玩跟有名的地方參觀參觀。 是的話，很好因為我那個時候還沒有放假，我可以好好地帶你去台北很多好玩跟有名的地方參觀參觀。\n",
      "好[MASK]不說，下個月見。 好吧不說，下個月見。\n",
      "他們一邊吃一邊說話聊天。文平和小王從十二點吃飯和說話說到三點[MASK]後都還沒說完。 他們一邊吃一邊說話聊天。文平和小王從十二點吃飯和說話說到三點然後都還沒說完。\n",
      "到ＳＯＧＯ百貨公司來逛街，王宜家看到了一件褲子他很喜歡但是價錢太貴了他沒有那麼[MASK]錢。 到ＳＯＧＯ百貨公司來逛街，王宜家看到了一件褲子他很喜歡但是價錢太貴了他沒有那麼多錢。\n",
      "我們在[MASK]上認識已經很久了，雖然我們從來沒見過面但對我來說你是我很熟的朋友。 我們在信上認識已經很久了，雖然我們從來沒見過面但對我來說你是我很熟的朋友。\n",
      "如果你答應我的計畫還是有[MASK]麼問題請你告訴我。我們可以改時間。 如果你答應我的計畫還是有什麼問題請你告訴我。我們可以改時間。\n",
      "我會穿紅色的衣服，牛仔褲跟黃色的帽子。我也戴黑色的眼[MASK]。 我會穿紅色的衣服，牛仔褲跟黃色的帽子。我也戴黑色的眼鏡。\n",
      "我們要見面的那天快就要到了，所以我這一次寫信給你是想跟妳安排一下關[MASK]我們要見面的事。 我們要見面的那天快就要到了，所以我這一次寫信給你是想跟妳安排一下關於我們要見面的事。\n",
      "那天我們兩個決定要見面的一天，我[MASK]那時候每天都在想這件事。 那天我們兩個決定要見面的一天，我從那時候每天都在想這件事。\n",
      "我知道一家又好吃又便宜的泰國餐廳，我們就去[MASK]裡吃飯吧！ 我知道一家又好吃又便宜的泰國餐廳，我們就去那裡吃飯吧！\n",
      "那家餐廳就在中山捷運站[MASK]近，所以我們就約在捷運站五號出口好了。 那家餐廳就在中山捷運站附近，所以我們就約在捷運站五號出口好了。\n",
      "既然我們離士林很近，我想我們吃完飯以後再去那裡逛一逛，因為[MASK]裡的小吃不但非常多，又好吃！ 既然我們離士林很近，我想我們吃完飯以後再去那裡逛一逛，因為那裡的小吃不但非常多，又好吃！\n",
      "我覺得我們那一天都穿著紅色的衣服，跟白色的褲子，好了頭上[MASK]一頂藍色的帽子，這樣我想一定不會認錯人了！ 我覺得我們那一天都穿著紅色的衣服，跟白色的褲子，好了頭上戴一頂藍色的帽子，這樣我想一定不會認錯人了！\n",
      "如果我家的狗還沒把我的天藍色襯衫咬破的話，我會[MASK]著白色的褲子在誠品外等你抵達。 如果我家的狗還沒把我的天藍色襯衫咬破的話，我會配著白色的褲子在誠品外等你抵達。\n",
      "因為您還沒有看過１０１，我會先幫我們買去樓頂的票[MASK]看看風景。 因為您還沒有看過１０１，我會先幫我們買去樓頂的票去看看風景。\n",
      "我現在打算說我們這個禮拜六早上十點鐘在台北市政[MASK]的捷運站三號出口見面。 我現在打算說我們這個禮拜六早上十點鐘在台北市政府的捷運站三號出口見面。\n",
      "從那裡，我們可以走到那附近的新光三[MASK]百貨公司逛一逛，那裡有很多非常漂亮的衣服和作品。 從那裡，我們可以走到那附近的新光三越百貨公司逛一逛，那裡有很多非常漂亮的衣服和作品。\n",
      "好吧！就這樣！我等不[MASK]了，希望禮拜六快一點來，就那會兒見吧！ 好吧！就這樣！我等不及了，希望禮拜六快一點來，就那會兒見吧！\n",
      "你收到我[MASK]給妳的禮物嗎？我希望沒被郵局掉了。我地址當然寫對了，可是我不知道如果我郵票放夠了。 你收到我寄給妳的禮物嗎？我希望沒被郵局掉了。我地址當然寫對了，可是我不知道如果我郵票放夠了。\n",
      "我很高興聽到你要這個[MASK]末要來看我，我安排很多很好玩的活動。 我很高興聽到你要這個週末要來看我，我安排很多很好玩的活動。\n",
      "我很期待跟你一起出去玩。期[MASK]得我不睡覺。當然我會一定開心。 我很期待跟你一起出去玩。期地得我不睡覺。當然我會一定開心。\n",
      "我們八月十三號下午三點在名古屋站[MASK]號出口見面。 我們八月十三號下午三點在名古屋站一號出口見面。\n",
      "對我來說，能找到那麼好的朋友，特別是韓國哥是一件很幸福[MASK]事。 對我來說，能找到那麼好的朋友，特別是韓國哥是一件很幸福的事。\n",
      "我那時的生活過的很差，真[MASK]運氣不好。我一月出了車禍（被父母罵），二月的成績退步了，還有三月跟愛人分手了。 我那時的生活過的很差，真的運氣不好。我一月出了車禍（被父母罵），二月的成績退步了，還有三月跟愛人分手了。\n",
      "可是我要感謝我哥一直都沒有放棄，一直在我的[MASK]邊。 可是我要感謝我哥一直都沒有放棄，一直在我的身邊。\n",
      "在日常生活裡人們會[MASK]會受到別人的幫助。 在日常生活裡人們會總會受到別人的幫助。\n",
      "還有我打噴嚏的時候媽媽會勸導我不要大聲打噴[MASK]因為會讓別人覺得你很不禮貌。從媽媽的教養以後我下一次打噴替的聲音會小聲一點。 還有我打噴嚏的時候媽媽會勸導我不要大聲打噴嚏因為會讓別人覺得你很不禮貌。從媽媽的教養以後我下一次打噴替的聲音會小聲一點。\n",
      "我要對媽媽很尊敬。我會說「媽媽我愛你」。我永[MASK]不會忘記你所帶給我的恩。 我要對媽媽很尊敬。我會說「媽媽我愛你」。我永的不會忘記你所帶給我的恩。\n",
      "如果沒有母親哪裡會有我，她辛辛苦苦的生下我，所以母親的恩是我一[MASK]都回不完的。 如果沒有母親哪裡會有我，她辛辛苦苦的生下我，所以母親的恩是我一生都回不完的。\n",
      "我母親是一個很嚴格的人，有時候讓我很煩不懂[MASK]的想法，總是逼我去做我不想做的事，長大了才了解她所做的一切是為我好的。 我母親是一個很嚴格的人，有時候讓我很煩不懂她的想法，總是逼我去做我不想做的事，長大了才了解她所做的一切是為我好的。\n",
      "八月十二號是泰國的母親節，今年沒幫你慶祝但在這[MASK]您身體健康要多保重。 八月十二號是泰國的母親節，今年沒幫你慶祝但在這祝您身體健康要多保重。\n",
      "而且我的中文沒有那麼流利，每次跟別人說[MASK]有錯誤。 而且我的中文沒有那麼流利，每次跟別人說都有錯誤。\n",
      "在我的生活中，有很多人有幫助我。但是除了我的父母以外我最感謝的人是我的五年[MASK]老師。 在我的生活中，有很多人有幫助我。但是除了我的父母以外我最感謝的人是我的五年級老師。\n",
      "因為我要離開我全部的朋友，我也有一定怕，因為我要去一個不認[MASK]的地方，真的是人生地不熟。 因為我要離開我全部的朋友，我也有一定怕，因為我要去一個不認識的地方，真的是人生地不熟。\n",
      "我的作文進步了很多因為她幫助我。她也給我介紹更難[MASK]書，所以我讀書的能力進步很多了。 我的作文進步了很多因為她幫助我。她也給我介紹更難的書，所以我讀書的能力進步很多了。\n",
      "為了我，我媽媽花很多時間在我[MASK]上，沒有真的讓自己休息。 為了我，我媽媽花很多時間在我身上，沒有真的讓自己休息。\n",
      "雖然我還是大學生，我一定會很用功[MASK]拿好成績，所以我畢業的時候可以找到好的工作。 雖然我還是大學生，我一定會很用功地拿好成績，所以我畢業的時候可以找到好的工作。\n",
      "找到好的工作我就可以[MASK]很多錢，於是我可以幫我的媽媽買好的東西，像衣服，車，房屋等等。 找到好的工作我就可以賺很多錢，於是我可以幫我的媽媽買好的東西，像衣服，車，房屋等等。\n",
      "我最感謝的人是我[MASK]位鄰居。 我最感謝的人是我一位鄰居。\n",
      "我把小貓抱起來，趕快[MASK]出去到馬路邊求救，可是有很少的車，有的都不會停下來幫我。 我把小貓抱起來，趕快抱出去到馬路邊求救，可是有很少的車，有的都不會停下來幫我。\n",
      "雖然一個[MASK]貓看不起是什麼，可是這個小貓是我最大的半，它從小陪我，所以我把小貓當孩子一樣。 雖然一個小貓看不起是什麼，可是這個小貓是我最大的半，它從小陪我，所以我把小貓當孩子一樣。\n",
      "媽媽做每件事不管怎麼樣辛苦都沒有抱怨。她默默[MASK]做到完成。 媽媽做每件事不管怎麼樣辛苦都沒有抱怨。她默默地做到完成。\n",
      "有一次，我被車子撞了。那時候[MASK]剛剛好在附近，所以她很快就趕過來看我。 有一次，我被車子撞了。那時候她剛剛好在附近，所以她很快就趕過來看我。\n",
      "我們真的不太像普通的表姊妹，可能是從小一起長到大的關係。我都知道[MASK]喜歡什麼，所以她喜歡的東西我都會買給她。 我們真的不太像普通的表姊妹，可能是從小一起長到大的關係。我都知道她喜歡什麼，所以她喜歡的東西我都會買給她。\n",
      "而戴黃色的帽子。你那天穿什麼衣服？拜託你穿容易找到的衣服。要不然我找不[MASK]你、再說我怕我們浪費很多時間。 而戴黃色的帽子。你那天穿什麼衣服？拜託你穿容易找到的衣服。要不然我找不到你、再說我怕我們浪費很多時間。\n",
      "我禮拜六晚上六點在台北車站一號出口附近等你。穿白襯衫，戴眼[MASK]的人就是我。你看過我的相片，所以一定會找到的。 我禮拜六晚上六點在台北車站一號出口附近等你。穿白襯衫，戴眼鏡的人就是我。你看過我的相片，所以一定會找到的。\n",
      "然後我們可以去咖啡座椅[MASK]談一談。如果你不要喝咖啡的話，我們可以去別的地方，隨便你，你要做什麼就做什麼。 然後我們可以去咖啡座椅坐談一談。如果你不要喝咖啡的話，我們可以去別的地方，隨便你，你要做什麼就做什麼。\n",
      "要是我的面試成功了的話，我們下禮拜我可以見面。我會告訴妳我的面試是什麼的，而且幫你一點忙推薦你面試的時候[MASK]麼辦。 要是我的面試成功了的話，我們下禮拜我可以見面。我會告訴妳我的面試是什麼的，而且幫你一點忙推薦你面試的時候怎麼辦。\n",
      "我們可以下禮拜六早上八點在大安森林公園見面，好不好？我打算我們先在公園靜靜地散[MASK]一下，然後去旁邊的咖啡館喝咖啡和聊天。 我們可以下禮拜六早上八點在大安森林公園見面，好不好？我打算我們先在公園靜靜地散步一下，然後去旁邊的咖啡館喝咖啡和聊天。\n",
      "為了讓你認得我，我會穿我黑皮夾克，戴藍色的帽子。我也會帶我的小狗去，你大概知道是哪一[MASK]，牠的眼睛藍藍的，圓圓的。 為了讓你認得我，我會穿我黑皮夾克，戴藍色的帽子。我也會帶我的小狗去，你大概知道是哪一隻，牠的眼睛藍藍的，圓圓的。\n",
      "我知道一個又好笑又怪怪的地方，可以介紹你。那家看起來不好看，我怕看得讓你覺得不舒服，[MASK]是很好玩，他們的菜還不錯，請嚐嚐！ 我知道一個又好笑又怪怪的地方，可以介紹你。那家看起來不好看，我怕看得讓你覺得不舒服，但是很好玩，他們的菜還不錯，請嚐嚐！\n",
      "明天我會[MASK]綠帽子，穿粉紅色襯衫，手上拿「王宜家」旗子，你應該不會有問題認出我。 明天我會戴綠帽子，穿粉紅色襯衫，手上拿「王宜家」旗子，你應該不會有問題認出我。\n",
      "我是李希平，知道我們[MASK]兩個禮拜就要見面，我真很高興又期待因為我們在網路聊天了快一年了。 我是李希平，知道我們再兩個禮拜就要見面，我真很高興又期待因為我們在網路聊天了快一年了。\n",
      "因為我們還沒見過面，所以我當天會穿粉紅色的上衣跟牛仔褲還有會[MASK]了一個帽子，帽子上有寫我的名字，這樣你應該認得我了。 因為我們還沒見過面，所以我當天會穿粉紅色的上衣跟牛仔褲還有會戴了一個帽子，帽子上有寫我的名字，這樣你應該認得我了。\n",
      "我希望你很期待我安排那天的行程。我也希望我們可以玩[MASK]很快樂。 我希望你很期待我安排那天的行程。我也希望我們可以玩得很快樂。\n",
      "雖然我爸賺錢也很辛苦，帶孩子也並不容易，尤其[MASK]我，因為我從小就不喜歡聽話。 雖然我爸賺錢也很辛苦，帶孩子也並不容易，尤其是我，因為我從小就不喜歡聽話。\n",
      "我哭的時候給我最喜歡喝的巧克力牛奶是我母親。我喝著飲料[MASK]她的話才了解我的錯。 我哭的時候給我最喜歡喝的巧克力牛奶是我母親。我喝著飲料聽她的話才了解我的錯。\n",
      "我現在上師大的中文課。第一天，我要去那邊申請。可是我不會寫漢[MASK]，所以申請表很難提。李建國跟我一起去、他提申請表提得很好。 我現在上師大的中文課。第一天，我要去那邊申請。可是我不會寫漢字，所以申請表很難提。李建國跟我一起去、他提申請表提得很好。\n",
      "我最感謝的人是我的家人。因為他們總是鼓勵還有支持我做的事情，他們[MASK]幫我很多事情。 我最感謝的人是我的家人。因為他們總是鼓勵還有支持我做的事情，他們會幫我很多事情。\n",
      "他們有時候，有一點嘮[MASK]，可是我知道他們擔心我。他們付很多錢，所以有時候我對他們很不好意思，可是我感謝的心情比較多。 他們有時候，有一點嘮叨，可是我知道他們擔心我。他們付很多錢，所以有時候我對他們很不好意思，可是我感謝的心情比較多。\n",
      "雖然這個世界每天都有人當媽媽，但真的將媽媽的角色做得好，不是每個人能成功做得到。「媽媽」不僅是個名字，也需要行動配合[MASK]。 雖然這個世界每天都有人當媽媽，但真的將媽媽的角色做得好，不是每個人能成功做得到。「媽媽」不僅是個名字，也需要行動配合她。\n",
      "所以，我覺得來表示我的感謝，我不一定做很誇張[MASK]表達，只跟著她的教訓就好。 所以，我覺得來表示我的感謝，我不一定做很誇張的表達，只跟著她的教訓就好。\n",
      "有些時候，媽媽會[MASK]怨我們對她很像一個僕人一樣。有可能是因為她做的事我們不曉得珍惜，而不說「謝謝」。 有些時候，媽媽會抱怨我們對她很像一個僕人一樣。有可能是因為她做的事我們不曉得珍惜，而不說「謝謝」。\n",
      "我媽媽養六個孩子，每天照顧我們打掃房子還有做[MASK]，我爸爸工作得很努力為了給我和我姐妹好的生活所以我都尊敬他們。 我媽媽養六個孩子，每天照顧我們打掃房子還有做菜，我爸爸工作得很努力為了給我和我姐妹好的生活所以我都尊敬他們。\n",
      "我最感謝的人是我的媽媽。因為，小時候都是媽媽在身邊，所以每一件[MASK]都需要她幫忙。 我最感謝的人是我的媽媽。因為，小時候都是媽媽在身邊，所以每一件事都需要她幫忙。\n",
      "我想跟她說她給我這麼多錢讓我去玩和付學費真的讓我感覺到[MASK]有多愛我。我知道她為了我才把她的錢拿來買我要的東。 我想跟她說她給我這麼多錢讓我去玩和付學費真的讓我感覺到她有多愛我。我知道她為了我才把她的錢拿來買我要的東。\n",
      "如果，沒有她這麼努力的一直幫我，今天我的[MASK]活就會改變了很多。 如果，沒有她這麼努力的一直幫我，今天我的生活就會改變了很多。\n",
      "張小春[MASK]上 張小春敬上\n",
      "只要你告訴我們做什麼我們就做了，不管多久[MASK]練習我們都不會放棄。請你告訴我們你可不可以幫我們然後我們準備做什，謝謝。 只要你告訴我們做什麼我們就做了，不管多久的練習我們都不會放棄。請你告訴我們你可不可以幫我們然後我們準備做什，謝謝。\n",
      "因為劍術人一下課，就開始鍛鍊，我們週末的時候應該練習表演。[MASK]末的時候老師有沒有空？ 因為劍術人一下課，就開始鍛鍊，我們週末的時候應該練習表演。週末的時候老師有沒有空？\n",
      "剛開始練習的時，我覺得一個禮拜最少要練習四次。新年快要[MASK]的時候應該多練習，可能要每天練習。 剛開始練習的時，我覺得一個禮拜最少要練習四次。新年快要到的時候應該多練習，可能要每天練習。\n",
      "雖然每天練習會很辛苦，可是這樣就可以變成一個很好的跳舞[MASK]，可以一起加油。 雖然每天練習會很辛苦，可是這樣就可以變成一個很好的跳舞隊，可以一起加油。\n",
      "而且雖然我們在台灣學中文、很少機會知道他們的生活，文化，歷史等等。同時，可以讓大家看原住民的傳統衣服，很有[MASK]！ 而且雖然我們在台灣學中文、很少機會知道他們的生活，文化，歷史等等。同時，可以讓大家看原住民的傳統衣服，很有趣！\n",
      "欸？！可是這個是不是芒果冰是芒果汁．．．怎麼辦？怎麼辦？我要吃是芒果冰[MASK]。 欸？！可是這個是不是芒果冰是芒果汁．．．怎麼辦？怎麼辦？我要吃是芒果冰啊。\n",
      "我換給菜你們，他們馬上[MASK]給芒果冰還有給我別的東西。 我換給菜你們，他們馬上換給芒果冰還有給我別的東西。\n",
      "對我來說，什麼東西都可以買，不光安事件還是有空的，也不必只在在百貨公[MASK]買的，在路邊的攤位也可以買。 對我來說，什麼東西都可以買，不光安事件還是有空的，也不必只在在百貨公司買的，在路邊的攤位也可以買。\n",
      "一天我跟日本朋友去台灣南部[MASK]行，第一次我去台灣沙灘，我在那邊好好兒地玩。 一天我跟日本朋友去台灣南部旅行，第一次我去台灣沙灘，我在那邊好好兒地玩。\n",
      "[MASK]品書店真是又大又豪華。不論有好幾樓的書，也有文具、雜誌、電影ＤＶＤ、音樂唱片，和很多的食物和點心。 誠品書店真是又大又豪華。不論有好幾樓的書，也有文具、雜誌、電影ＤＶＤ、音樂唱片，和很多的食物和點心。\n",
      "我去別的店逛街，找鞋子可是別的店的小姐也是很煩，她們都會跟在我的後面[MASK]膠水。我走去哪裡，他們就跟我走去哪裡。 我去別的店逛街，找鞋子可是別的店的小姐也是很煩，她們都會跟在我的後面像膠水。我走去哪裡，他們就跟我走去哪裡。\n",
      "我越來越學習第四本書的[MASK]時候，我常常想我不夠生詞，還有我中文的能力。 我越來越學習第四本書的課時候，我常常想我不夠生詞，還有我中文的能力。\n",
      "如果不過我這樣努力學習，我覺得不夠的話，那[MASK]時候，我要買第三本書努力學習。 如果不過我這樣努力學習，我覺得不夠的話，那個時候，我要買第三本書努力學習。\n",
      "我[MASK]期天到陽明山去玩。因為那天路很匆忙，所以我搭公車。 我星期天到陽明山去玩。因為那天路很匆忙，所以我搭公車。\n",
      "因為是週[MASK]，很多人會去陽明山去看風景。開到山上以後，看到人山人海的家庭。每個家庭都說這裡的風景真漂亮。 因為是週末，很多人會去陽明山去看風景。開到山上以後，看到人山人海的家庭。每個家庭都說這裡的風景真漂亮。\n",
      "我先去百貨公司買衣服。日本的衣服都很好看，[MASK]料也很好。一看就好想買。 我先去百貨公司買衣服。日本的衣服都很好看，質料也很好。一看就好想買。\n",
      "[MASK]者最近放假，我跟兩個同學從十月十號到十月十三號一起到台灣綠島去了。 著者最近放假，我跟兩個同學從十月十號到十月十三號一起到台灣綠島去了。\n",
      "所以第二天的天氣非常好。我們馬上去山[MASK]看風景。從頂上看風景非常漂亮而且會感動。然後會看到紅。 所以第二天的天氣非常好。我們馬上去山上看風景。從頂上看風景非常漂亮而且會感動。然後會看到紅。\n",
      "不但人很可，他們還喜歡幫助人。迷路的[MASK]，或是你需要幫忙，他們他們馬上來幫你的忙。 不但人很可，他們還喜歡幫助人。迷路的事，或是你需要幫忙，他們他們馬上來幫你的忙。\n",
      "雖然對我來說，我的英文還不錯，可是波士頓人的口音很難聽。我[MASK]很多的問題他們的口音。 雖然對我來說，我的英文還不錯，可是波士頓人的口音很難聽。我有很多的問題他們的口音。\n",
      "我的朋友跟我說別擔心，慢慢你會聽得懂波士頓的口音。一個禮拜以後我甚麼問題都沒有，[MASK]波士頓的口音我都聽得懂。 我的朋友跟我說別擔心，慢慢你會聽得懂波士頓的口音。一個禮拜以後我甚麼問題都沒有，連波士頓的口音我都聽得懂。\n",
      "我在法國已經一個禮拜多了，時[MASK]真過著很快．．．我這一次來法國碰到很多很有意思的事，我想跟妳說說。 我在法國已經一個禮拜多了，時間真過著很快．．．我這一次來法國碰到很多很有意思的事，我想跟妳說說。\n",
      "妳給我的台灣烏龍茶，我爸爸給我的高粱酒，我自己買的原[MASK]民音樂，我都給他了，他非常高興，我就跟妳說，禮多人不怪！ 妳給我的台灣烏龍茶，我爸爸給我的高粱酒，我自己買的原住民音樂，我都給他了，他非常高興，我就跟妳說，禮多人不怪！\n",
      "我今天早上去了「新店」區。台北交通非常方便，從我家走到捷運[MASK]站是十分鐘。從「台電大樓」站到「新店」站只要十三分鐘。 我今天早上去了「新店」區。台北交通非常方便，從我家走到捷運車站是十分鐘。從「台電大樓」站到「新店」站只要十三分鐘。\n",
      "來動物園參觀的人很多，除了大人[MASK]外，還有很多孩子，看來很熱鬧。 來動物園參觀的人很多，除了大人以外，還有很多孩子，看來很熱鬧。\n",
      "我覺得曼谷的風景跟台灣[MASK]一樣。 我覺得曼谷的風景跟台灣快一樣。\n",
      "妳好，妳還記得我[MASK]？我剛剛從澳洲回來的。 妳好，妳還記得我嗎？我剛剛從澳洲回來的。\n",
      "Ｐｅｒｔｈ是很多山還有樹的地方。人[MASK]也不多不過這個地方很安定。 Ｐｅｒｔｈ是很多山還有樹的地方。人是也不多不過這個地方很安定。\n",
      "我跟男朋友也希望妳可以跟我們一起去。真的很好玩。希望下個假我們可以一起去[MASK]行。 我跟男朋友也希望妳可以跟我們一起去。真的很好玩。希望下個假我們可以一起去旅行。\n",
      "除了這個事情以外，我在韓國玩的事情都很[MASK]意思了。還有我買了你的禮物。下次見面的時候我要給你！ 除了這個事情以外，我在韓國玩的事情都很有意思了。還有我買了你的禮物。下次見面的時候我要給你！\n",
      "我們從波蘭坐火車到烏克蘭，然後坐公車到車爾諾貝力去。我們花三天看這個事故的場地，車[MASK]諾貝力的核能電廠。 我們從波蘭坐火車到烏克蘭，然後坐公車到車爾諾貝力去。我們花三天看這個事故的場地，車爾諾貝力的核能電廠。\n",
      "因為自己也很喜愛畫畫和[MASK]油畫，所以覺得我有這個能力可以好好地教到學生一些有用的東西。 因為自己也很喜愛畫畫和塗油畫，所以覺得我有這個能力可以好好地教到學生一些有用的東西。\n",
      "從第一天開始，我會叫我的學生先選他們想要開的生[MASK]。 從第一天開始，我會叫我的學生先選他們想要開的生意。\n",
      "選完以後我會給他們第一部分的說明；怎麼開始他們的生意。生意開始以後，我每天會跟我的學生講他們做對和[MASK]的方法。 選完以後我會給他們第一部分的說明；怎麼開始他們的生意。生意開始以後，我每天會跟我的學生講他們做對和錯的方法。\n",
      "我要小[MASK]的班，有大概二十、二十五個學生。 我要小小的班，有大概二十、二十五個學生。\n",
      "所以我上課的時候，如果學生拿到了好成績，我會給他小禮物。可是如果學生拿到了[MASK]成績，我會給他補考。 所以我上課的時候，如果學生拿到了好成績，我會給他小禮物。可是如果學生拿到了壞成績，我會給他補考。\n",
      "從理論很快就要講到現實，[MASK]為經濟理論的重點是要來解釋現實，上課就要討論為什麼理論能或不能解釋現在或過去的經濟情況。 從理論很快就要講到現實，因為經濟理論的重點是要來解釋現實，上課就要討論為什麼理論能或不能解釋現在或過去的經濟情況。\n",
      "比方說，對最近的金融風暴，我希望每個學生都可以清楚地說明為什麼發生[MASK]，跟現在要怎麼因應，採取那些政策可以解決這個問題。 比方說，對最近的金融風暴，我希望每個學生都可以清楚地說明為什麼發生的，跟現在要怎麼因應，採取那些政策可以解決這個問題。\n",
      "我也教運動課。我覺得運動是很[MASK]要，孩子從小要動，不要到他們長大就不喜歡動。 我也教運動課。我覺得運動是很重要，孩子從小要動，不要到他們長大就不喜歡動。\n",
      "我會用的方法給教六歲的孩子是用遊[MASK]，用唱歌的方法教他們運動。我也會用這個方法教他們荷蘭文。 我會用的方法給教六歲的孩子是用遊戲，用唱歌的方法教他們運動。我也會用這個方法教他們荷蘭文。\n",
      "我最希望是他們會愛上語言，就可以學很多語言。去外國可以認識很多的人，就可以介[MASK]。 我最希望是他們會愛上語言，就可以學很多語言。去外國可以認識很多的人，就可以介紹。\n",
      "體[MASK]課不但有趣，而且要他們的成績高。 體育課不但有趣，而且要他們的成績高。\n",
      "我覺得說話的能力最重要。去外國的時候非說話不可。所以來我的[MASK]學生一定要說很多。他們每次進步很快。 我覺得說話的能力最重要。去外國的時候非說話不可。所以來我的課學生一定要說很多。他們每次進步很快。\n",
      "如果學生有不懂的題目，隨時可以問我。不方便在上課的時候問的話，可以下課以後到我的辦公[MASK]問。 如果學生有不懂的題目，隨時可以問我。不方便在上課的時候問的話，可以下課以後到我的辦公室問。\n",
      "如果這樣的話，學生可以學得到很深的語言，還有學生會對他們學[MASK]語言。 如果這樣的話，學生可以學得到很深的語言，還有學生會對他們學的語言。\n",
      "如[MASK]我當大學教授的話，我一定要教歷史。為什麼？因為我對歷史非常興趣。 如果我當大學教授的話，我一定要教歷史。為什麼？因為我對歷史非常興趣。\n",
      "學[MASK]的功課會是現代的政府跟以前的比較，然後報告，他們覺得有什麼東西一樣，有什麼不一樣，再說告訴我他們覺得那個比較好。 學生的功課會是現代的政府跟以前的比較，然後報告，他們覺得有什麼東西一樣，有什麼不一樣，再說告訴我他們覺得那個比較好。\n",
      "我對語言學有很大的興趣，對別的科學我不太了解。而且英文，除了波蘭文[MASK]外，是我最容易說的語言。 我對語言學有很大的興趣，對別的科學我不太了解。而且英文，除了波蘭文以外，是我最容易說的語言。\n",
      "為甚麼我不想教我的母語？因為雖然我當然說很好的波蘭文，可是一個關於教波[MASK]文的東西都不知道，我也沒有教的經驗。 為甚麼我不想教我的母語？因為雖然我當然說很好的波蘭文，可是一個關於教波蘭文的東西都不知道，我也沒有教的經驗。\n",
      "因為我覺得用學的語言跟同學說話不但讓語言學生學到不一樣的詞[MASK]說法他們的進步也會很快，所以我也要給他們很多機會跟同學合作。 因為我覺得用學的語言跟同學說話不但讓語言學生學到不一樣的詞跟說法他們的進步也會很快，所以我也要給他們很多機會跟同學合作。\n",
      "如果學生不能解決現實的問題，吃什麼，租房子，交朋友什麼的，他們不能適應住在外國，所以他們的生[MASK]會有很多困難。 如果學生不能解決現實的問題，吃什麼，租房子，交朋友什麼的，他們不能適應住在外國，所以他們的生活會有很多困難。\n",
      "我從現在起，上網要安排吃飯的地方、玩的地方什麼的，等你跟他聯絡，我[MASK]會確定的。 我從現在起，上網要安排吃飯的地方、玩的地方什麼的，等你跟他聯絡，我才會確定的。\n",
      "那天他穿著時尚的衣著，戴著棕色的眼[MASK]。 那天他穿著時尚的衣著，戴著棕色的眼鏡。\n",
      "他的身材又高又瘦！眼睛大大的、鼻子高高、嘴[MASK]也厚厚的，真的帥死了！ 他的身材又高又瘦！眼睛大大的、鼻子高高、嘴唇也厚厚的，真的帥死了！\n",
      "她那一天帶了綠色隱形眼鏡，穿著也滿流行，襯衫上有「你好貓咪」畫圖，我覺得她看起來[MASK]該差不多十五歲左右。 她那一天帶了綠色隱形眼鏡，穿著也滿流行，襯衫上有「你好貓咪」畫圖，我覺得她看起來應該差不多十五歲左右。\n",
      "請你第一時間幫我查出這個女生的資訊，然後馬上通知我。在這個包裹裡我也放了幾首我親自寫的詩。求求你請把它交給我[MASK]愛的女生。 請你第一時間幫我查出這個女生的資訊，然後馬上通知我。在這個包裹裡我也放了幾首我親自寫的詩。求求你請把它交給我親愛的女生。\n",
      "我覺得這個人長得不錯跟很想要跟[MASK]認識可是後來，沒有機會去跟他說話。 我覺得這個人長得不錯跟很想要跟他認識可是後來，沒有機會去跟他說話。\n",
      "最近我的生活變了很苦悶，我天天睡不著而食物也[MASK]不下，都是那位美女在你的生日會的錯！ 最近我的生活變了很苦悶，我天天睡不著而食物也吃不下，都是那位美女在你的生日會的錯！\n",
      "她就是我下過雨的天夏天，沒她就讓我覺得很期待，那時候我就決定了，她不是從[MASK]世界來的，就是對我來說她一定是天使。 她就是我下過雨的天夏天，沒她就讓我覺得很期待，那時候我就決定了，她不是從這世界來的，就是對我來說她一定是天使。\n",
      "上個禮拜六，在你最喜歡的夜店，我愛上了最漂亮的女生。我還不知道她的名字呢，可是我已經確定我想當[MASK]的白馬公主。 上個禮拜六，在你最喜歡的夜店，我愛上了最漂亮的女生。我還不知道她的名字呢，可是我已經確定我想當她的白馬公主。\n",
      "她的臉白白的、圓圓的，頭髮黑黑的，眼睛大大的。[MASK]穿的衣服也很適合她，看起來就算是一個女神不說，她的性格也算是非常溫柔。 她的臉白白的、圓圓的，頭髮黑黑的，眼睛大大的。她穿的衣服也很適合她，看起來就算是一個女神不說，她的性格也算是非常溫柔。\n",
      "那時候我就睡醒了，我覺得很可怕、不能[MASK]睡覺。 那時候我就睡醒了，我覺得很可怕、不能再睡覺。\n",
      "在那個夢我參加我好朋友的結婚會，在一個法國堡城。人很多，其實地方[MASK]死了，但是每一個人看起來好舒服，不注意位子夠不夠。 在那個夢我參加我好朋友的結婚會，在一個法國堡城。人很多，其實地方擠死了，但是每一個人看起來好舒服，不注意位子夠不夠。\n",
      "我知道，有時候我跟我的爸爸、媽媽說英文都快點[MASK]忘了我的中文，我的夢給我的感受很用功因為很難。 我知道，有時候我跟我的爸爸、媽媽說英文都快點敢忘了我的中文，我的夢給我的感受很用功因為很難。\n",
      "中文對我說是非常很難，不論你會會一千多個字，你不會說很寫[MASK]都沒有用。 中文對我說是非常很難，不論你會會一千多個字，你不會說很寫標都沒有用。\n",
      "我已經很期待我們的新年晚會！我們下個禮拜已經談談了我們要表演的是什麼。所以我跟您要說以下我[MASK]對這次新年晚會的想法。 我已經很期待我們的新年晚會！我們下個禮拜已經談談了我們要表演的是什麼。所以我跟您要說以下我們對這次新年晚會的想法。\n",
      "像您看，我們覺得這場戲劇會讓大家了解留學生對台灣[MASK]想法。而且我們希望鼓勵更多的留學生。 像您看，我們覺得這場戲劇會讓大家了解留學生對台灣的想法。而且我們希望鼓勵更多的留學生。\n",
      "每一隊應該表十五分鐘，然後第一名的隊要收到禮物。我希望國[MASK]中心可以幫我找三個老師給表演的人分。 每一隊應該表十五分鐘，然後第一名的隊要收到禮物。我希望國語中心可以幫我找三個老師給表演的人分。\n",
      "如果老師跟同學們有時間，每天下課以後可以一起到大學旁邊的公園去練習半個小時。需要的話[MASK]末也可以見面。 如果老師跟同學們有時間，每天下課以後可以一起到大學旁邊的公園去練習半個小時。需要的話週末也可以見面。\n",
      "而且我也想向國語中心請教有沒有可以給我們的教室，給我們下雨的時候機會乾[MASK]地練習唱歌。 而且我也想向國語中心請教有沒有可以給我們的教室，給我們下雨的時候機會乾乾地練習唱歌。\n",
      "表演節目：[MASK]里島民族舞蹈。 表演節目：峇里島民族舞蹈。\n",
      "練習[MASK]間、地點：每個禮拜１、３、５，下課後（大概１小時）。在３０１教室。 練習時間、地點：每個禮拜１、３、５，下課後（大概１小時）。在３０１教室。\n",
      "老師，我們班的同學們大概學好了以後，請看一下給我們建議。表演時，我們要請國語中心幫我們放音樂、[MASK]光。 老師，我們班的同學們大概學好了以後，請看一下給我們建議。表演時，我們要請國語中心幫我們放音樂、燈光。\n",
      "據我所知同學們很愛唱歌、跳舞，他們也很有演戲的天[MASK]所以表演音樂劇絕對是一個可以讓他們發揮所能的機會。 據我所知同學們很愛唱歌、跳舞，他們也很有演戲的天份所以表演音樂劇絕對是一個可以讓他們發揮所能的機會。\n",
      "因為我們在台灣的時間不是很長，所以我們不太了解要在那裡買我們所需要的道具，以及我們也沒有[MASK]置舞台的經驗。 因為我們在台灣的時間不是很長，所以我們不太了解要在那裡買我們所需要的道具，以及我們也沒有布置舞台的經驗。\n",
      "我們班有很多不同的國家來的學生，[MASK]通的語言只是中文，都要學把中文學好。 我們班有很多不同的國家來的學生，溝通的語言只是中文，都要學把中文學好。\n",
      "我的台灣朋友們都很會唱歌，而且唱[MASK]很好聽，上課的時候也有教我們中文歌。 我的台灣朋友們都很會唱歌，而且唱得很好聽，上課的時候也有教我們中文歌。\n",
      "很多人一起唱歌的時候最[MASK]要的是就是多一點練習，明天下課以後大家一起討論吧！ 很多人一起唱歌的時候最重要的是就是多一點練習，明天下課以後大家一起討論吧！\n",
      "操場不只大，可以讓跳舞時的走來去，跳來跳去；為了安全起見，放鞭炮[MASK]也比較安全因為是室外的。 操場不只大，可以讓跳舞時的走來去，跳來跳去；為了安全起見，放鞭炮時也比較安全因為是室外的。\n",
      "我也[MASK]老師幫我們找時間在操場，以免我們要用時，正好有其他人也想用。 我也想老師幫我們找時間在操場，以免我們要用時，正好有其他人也想用。\n",
      "每次看到買帽子的攤子都會十分興奮，但每次不敢買因為總有朋友在我背上潑了冷水地說，「你不適合[MASK]」的這一句話。 每次看到買帽子的攤子都會十分興奮，但每次不敢買因為總有朋友在我背上潑了冷水地說，「你不適合戴」的這一句話。\n",
      "一直到拿到成績單那時刻我嚇了一跳，我真的很高興自己[MASK]算進了前五名，我父母親也很高興。 一直到拿到成績單那時刻我嚇了一跳，我真的很高興自己總算進了前五名，我父母親也很高興。\n",
      "心[MASK]願望，改變我的人生，給我信心的ＩＰＯＤ買了，你知道那時候我多開心嗎？ 心裡願望，改變我的人生，給我信心的ＩＰＯＤ買了，你知道那時候我多開心嗎？\n",
      "我們選了一台電腦。可是老闆說，這台電腦看起來很漂亮。但是電腦的用量不[MASK]，也不好用。 我們選了一台電腦。可是老闆說，這台電腦看起來很漂亮。但是電腦的用量不夠，也不好用。\n",
      "我所有的錢都用光了買這台電腦。我[MASK]麼辦？不可以換。香港太遠。飛機票買不起。 我所有的錢都用光了買這台電腦。我怎麼辦？不可以換。香港太遠。飛機票買不起。\n",
      "我在和朋友們分開逛街買東西，然後我朋友拿一件看起來不錯的Ｔ恤，我也想要買一樣的Ｔ恤，然後再去賣那件Ｔ[MASK]的店。 我在和朋友們分開逛街買東西，然後我朋友拿一件看起來不錯的Ｔ恤，我也想要買一樣的Ｔ恤，然後再去賣那件Ｔ恤的店。\n",
      "還有是台灣小吃我最喜歡。他們小吃又好吃又有很多[MASK]類像是炒米粉，臭豆腐，烤魷魚，麵線．．．等等！ 還有是台灣小吃我最喜歡。他們小吃又好吃又有很多種類像是炒米粉，臭豆腐，烤魷魚，麵線．．．等等！\n",
      "如果老師把一個安靜的氣氛換成熱鬧活潑的，[MASK]個人的精神都很好，那堂課的效果會很不錯的。 如果老師把一個安靜的氣氛換成熱鬧活潑的，每個人的精神都很好，那堂課的效果會很不錯的。\n",
      "如果我是老師，我喜歡[MASK]中文老師，因為我學中文學了差不多五年了，在學習中文的過程當中我已經找到了我前所未有的興趣。 如果我是老師，我喜歡當中文老師，因為我學中文學了差不多五年了，在學習中文的過程當中我已經找到了我前所未有的興趣。\n",
      "我自己認為當老師除了有[MASK]範能力之外還要有敬業精神。另外，還要不斷吸收新知充實自己才能為學生做了好榜樣。 我自己認為當老師除了有示範能力之外還要有敬業精神。另外，還要不斷吸收新知充實自己才能為學生做了好榜樣。\n",
      "傳統的教學方式就是老師說什[MASK]，學生就知道什么，學生都認為老師說出來的話不會錯的，所以不會自己研究發揮一下本身的創新性。 傳統的教學方式就是老師說什麼，學生就知道什么，學生都認為老師說出來的話不會錯的，所以不會自己研究發揮一下本身的創新性。\n",
      "您[MASK]末過得怎麼樣？我今天在複習我們這個星期學習的內容。 您週末過得怎麼樣？我今天在複習我們這個星期學習的內容。\n",
      "因為如果雖然我真用功，可是我不知道很多[MASK]我們學習的部分，我不能趕上您的課。 因為如果雖然我真用功，可是我不知道很多的我們學習的部分，我不能趕上您的課。\n",
      "謝謝您願意擔任我們國華系的會話教師！也由衷[MASK]感恩您這開學以來的關懷與照顧！ 謝謝您願意擔任我們國華系的會話教師！也由衷地感恩您這開學以來的關懷與照顧！\n",
      "這兩個禮拜的時間有在老師的關照之下，對於台灣的生活我慢慢[MASK]覺得習慣 這兩個禮拜的時間有在老師的關照之下，對於台灣的生活我慢慢地覺得習慣\n",
      "僅可讓我能很快速地連起到名詞的意思、很快[MASK]了解到課程的內容。 僅可讓我能很快速地連起到名詞的意思、很快地了解到課程的內容。\n",
      "希望在未來學習的期間我會好好[MASK]努力，不會讓老師失望。 希望在未來學習的期間我會好好地努力，不會讓老師失望。\n",
      "開學已經過了兩個禮拜，幸運的[MASK]，我上到很好的老師的課。 開學已經過了兩個禮拜，幸運的是，我上到很好的老師的課。\n",
      "希望老師您可以體諒我的煩惱而改變上課的方式，也許是我不夠聰明，但我還是希望在上老師[MASK]可以學到一些東西。 希望老師您可以體諒我的煩惱而改變上課的方式，也許是我不夠聰明，但我還是希望在上老師課可以學到一些東西。\n",
      "希望老師您可以幫我解決這個問題，這門課是我最難也最重要的課，我當然希望可以拿到最好的分數而讓父母感[MASK]驕傲。 希望老師您可以幫我解決這個問題，這門課是我最難也最重要的課，我當然希望可以拿到最好的分數而讓父母感到驕傲。\n",
      "他們實在對我太好了，我不想辜負了他們的愛心，希望以後可以好好[MASK]孝順他們老人家。 他們實在對我太好了，我不想辜負了他們的愛心，希望以後可以好好地孝順他們老人家。\n",
      "最近天氣冷了，您的身體舒服？真的似乎來秋天[MASK]！ 最近天氣冷了，您的身體舒服？真的似乎來秋天啊！\n",
      "您的課聽得好，總是教得努[MASK]，我很感動。 您的課聽得好，總是教得努力，我很感動。\n",
      "您教台灣的情況、藝術、各地的消信、有意思的故事，我們都覺得您的[MASK]很有意思。 您教台灣的情況、藝術、各地的消信、有意思的故事，我們都覺得您的課很有意思。\n",
      "第一次上課的時候我很緊張，老師說的話完全聽不懂。我很生氣了對我自己，「為甚麼我一個人聽不懂[MASK]」。 第一次上課的時候我很緊張，老師說的話完全聽不懂。我很生氣了對我自己，「為甚麼我一個人聽不懂啊」。\n",
      "您記著[MASK]？我去找您以後，說「請幫助我」，那天以後，您每天幫助我。 您記著嗎？我去找您以後，說「請幫助我」，那天以後，您每天幫助我。\n",
      "從有一天越來越聽得[MASK]您的課，我很感謝您的幫助。 從有一天越來越聽得懂您的課，我很感謝您的幫助。\n",
      "學生王天華[MASK]上。 學生王天華敬上。\n",
      "把鮮蝦子[MASK]以後加鳳梨罐頭再加沙拉醬的。我們孩子也很喜歡吃鳳梨蝦球，所以去溫泉時我們常常點鳳梨蝦球，味道真好。 把鮮蝦子炸以後加鳳梨罐頭再加沙拉醬的。我們孩子也很喜歡吃鳳梨蝦球，所以去溫泉時我們常常點鳳梨蝦球，味道真好。\n",
      "父母很忙，[MASK]的時候吃快餐，我們平常吃很少菜，所以吃中國菜的時候會吃很多菜。 父母很忙，有的時候吃快餐，我們平常吃很少菜，所以吃中國菜的時候會吃很多菜。\n",
      "我在日本的時候最喜歡的中國菜是炸春捲，咕[MASK]肉，青椒肉絲。 我在日本的時候最喜歡的中國菜是炸春捲，咕咾肉，青椒肉絲。\n",
      "很多日本人受歡迎，咕[MASK]肉，青椒肉絲，麻婆豆腐。 很多日本人受歡迎，咕咾肉，青椒肉絲，麻婆豆腐。\n",
      "日本菜跟中國菜不一樣，日本菜比中國菜比較清[MASK]的。 日本菜跟中國菜不一樣，日本菜比中國菜比較清淡的。\n",
      "我剛來台灣的時候，吃含有蒜料理的話，我的胃部很[MASK]，沒辦法常吃，但是已經習慣了，我最近做料理的時候常用蒜，薑。 我剛來台灣的時候，吃含有蒜料理的話，我的胃部很痛，沒辦法常吃，但是已經習慣了，我最近做料理的時候常用蒜，薑。\n",
      "我覺得中國食物的特色是顯出當材料本來的味道，反而韓國食物的特色[MASK]除了材料的味道以外顯出其他放東西的味道。 我覺得中國食物的特色是顯出當材料本來的味道，反而韓國食物的特色是除了材料的味道以外顯出其他放東西的味道。\n",
      "因為我覺得中國菜蠻油膩，還有聞得起來很特別[MASK]味道，沒有日本菜的那麼口味好。 因為我覺得中國菜蠻油膩，還有聞得起來很特別的味道，沒有日本菜的那麼口味好。\n",
      "最不好的菜是臭豆腐，最不喜歡的是聞，我一聞就想吐，味道也不太好。這個菜是外國人的口味不一樣的[MASK]。 最不好的菜是臭豆腐，最不喜歡的是聞，我一聞就想吐，味道也不太好。這個菜是外國人的口味不一樣的菜。\n",
      "中國菜和韓國菜不一樣。中國菜是一點膩，可是韓國菜是大部分的[MASK]一點辣。 中國菜和韓國菜不一樣。中國菜是一點膩，可是韓國菜是大部分的菜一點辣。\n",
      "我常常跟朋友一起逛夜市，我台灣人的朋友們個個很親切[MASK]每次介紹我不同的台灣小吃。 我常常跟朋友一起逛夜市，我台灣人的朋友們個個很親切地每次介紹我不同的台灣小吃。\n",
      "我覺得去夜市讓你們感到台灣的氣氛，還有台灣很有名美味的食物，可能你想吃的東西[MASK]麼都有！ 我覺得去夜市讓你們感到台灣的氣氛，還有台灣很有名美味的食物，可能你想吃的東西什麼都有！\n",
      "差一點我忘記要請你幫我帶來一些東西。那些東西我可以請我家人給你。我家人已經知道了你要來台灣所以他們也風[MASK]心。 差一點我忘記要請你幫我帶來一些東西。那些東西我可以請我家人給你。我家人已經知道了你要來台灣所以他們也風放心。\n",
      "我們這樣認識了，可是她原來我們系的學姐。這麼巧！不過她因為剛轉中文系，而[MASK]也剛開始學中文。 我們這樣認識了，可是她原來我們系的學姐。這麼巧！不過她因為剛轉中文系，而她也剛開始學中文。\n",
      "房子裡[MASK]置的家具跟房間的安排也滿不錯，我還好滿足啦。 房子裡布置的家具跟房間的安排也滿不錯，我還好滿足啦。\n",
      "在日本也可能住很久的話，當然會來東西的壽命。可是好像台灣的壽命比日本短[MASK]多。 在日本也可能住很久的話，當然會來東西的壽命。可是好像台灣的壽命比日本短得多。\n",
      "這個地區上算是很划算的不動產。租費只要一萬塊，不過設備都全美無瑕。再加上[MASK]有一個在台灣難得的好處：木頭地板。 這個地區上算是很划算的不動產。租費只要一萬塊，不過設備都全美無瑕。再加上它有一個在台灣難得的好處：木頭地板。\n",
      "在我房子樓下有一個小商店，商店裡有三個兄弟[MASK]妹，我常常跟他們聊天。 在我房子樓下有一個小商店，商店裡有三個兄弟姊妹，我常常跟他們聊天。\n",
      "我最難忘的一個人是我的奶奶。現在他沒有了。[MASK]前他還有的時候我們常常跟他一塊兒去買東西，去玩兒。 我最難忘的一個人是我的奶奶。現在他沒有了。以前他還有的時候我們常常跟他一塊兒去買東西，去玩兒。\n",
      "他是從中國來的。所以他會說中文，他說[MASK]很好。他常常教我們說中文。他說我們要會說中文，所以我現在到台灣來學中文。 他是從中國來的。所以他會說中文，他說得很好。他常常教我們說中文。他說我們要會說中文，所以我現在到台灣來學中文。\n",
      "他很喜歡做飯，他做得很好吃。她每天做飯給我們吃。我們很喜歡他做的菜。他也很喜歡唱歌兒，他唱得不[MASK]。 他很喜歡做飯，他做得很好吃。她每天做飯給我們吃。我們很喜歡他做的菜。他也很喜歡唱歌兒，他唱得不錯。\n",
      "他[MASK]前是中文老師所以他很耐心。我們都很愛他。她是真好的奶奶。我很想他。 他以前是中文老師所以他很耐心。我們都很愛他。她是真好的奶奶。我很想他。\n",
      "雖然房租很貴，我不要搬家，我的房間很大，也有客廳，[MASK]加上房東對我很好。 雖然房租很貴，我不要搬家，我的房間很大，也有客廳，再加上房東對我很好。\n",
      "我剛到台灣的時候有運氣認[MASK]一個要搬家的外國人。他說他的房租太貴，所以他找便宜的房子。 我剛到台灣的時候有運氣認識一個要搬家的外國人。他說他的房租太貴，所以他找便宜的房子。\n",
      "在美國交通不方便，地鐵從我家很遠。美國的房子的環境很安[MASK]，我的房子很乾淨，可是房租比台灣的貴。 在美國交通不方便，地鐵從我家很遠。美國的房子的環境很安靜，我的房子很乾淨，可是房租比台灣的貴。\n",
      "我們有三間房間，[MASK]房間都有冷氣、床、桌書、椅子、架書、電腦。 我們有三間房間，每房間都有冷氣、床、桌書、椅子、架書、電腦。\n",
      "我搬家以前住在新店山上的國際學舍。因為山上，空氣很好，可以看到山下的城市的夜景，而且學舍有[MASK]理員而比較安全。 我搬家以前住在新店山上的國際學舍。因為山上，空氣很好，可以看到山下的城市的夜景，而且學舍有管理員而比較安全。\n",
      "因為台灣的氣候很濕的關係，浴室裡還有衣櫃裡有一點發[MASK]，洗不掉。 因為台灣的氣候很濕的關係，浴室裡還有衣櫃裡有一點發霉，洗不掉。\n",
      "住的地方應有安全感。房子和學校的距離[MASK]要太遠，走路就可以到學校的等等。 住的地方應有安全感。房子和學校的距離不要太遠，走路就可以到學校的等等。\n",
      "我覺得三重市的環境不太好。因為我住的地方是[MASK]區，所以還不發展。 我覺得三重市的環境不太好。因為我住的地方是郊區，所以還不發展。\n",
      "不動產公司幫房東找房客的話，房東不得不算手[MASK]費。 不動產公司幫房東找房客的話，房東不得不算手續費。\n",
      "如果那個房子不滿意的話，再從第一次[MASK]找。 如果那個房子不滿意的話，再從第一次的找。\n",
      "其實，除了房子大小跟房租[MASK]外，兩個房子裡面的東西都差不多。 其實，除了房子大小跟房租以外，兩個房子裡面的東西都差不多。\n",
      "我常常去我家附近的餐廳[MASK]包子和蛋花湯吃飯，那裡的店員已經認識我，所以每次我去跟他們聊天。 我常常去我家附近的餐廳買包子和蛋花湯吃飯，那裡的店員已經認識我，所以每次我去跟他們聊天。\n",
      "夜市裡有各式各種的地攤，小吃的，用的，穿[MASK]什麼都有。 夜市裡有各式各種的地攤，小吃的，用的，穿的什麼都有。\n",
      "今天早上我非常[MASK]高興，因為我今天收到了信從我的最好的朋友，她的名字叫美真，她和我一樣已經二十三歲。 今天早上我非常地高興，因為我今天收到了信從我的最好的朋友，她的名字叫美真，她和我一樣已經二十三歲。\n",
      "你[MASK]到台灣我會帶你去花蓮。 你一到台灣我會帶你去花蓮。\n",
      "我的好朋友跟我說他可以借給我汽車。所以很方便。我們在那裡可以[MASK]魚，還有在海邊玩。 我的好朋友跟我說他可以借給我汽車。所以很方便。我們在那裡可以釣魚，還有在海邊玩。\n",
      "臺灣的天[MASK]會容易變，還有常常會下雨。所以你一定要帶長袖，免得會感冒！ 臺灣的天氣會容易變，還有常常會下雨。所以你一定要帶長袖，免得會感冒！\n",
      "你來台灣以後我想跟妳一起去陽明山。在山上風景很美。我們可以一邊吃野餐一邊欣[MASK]風景。 你來台灣以後我想跟妳一起去陽明山。在山上風景很美。我們可以一邊吃野餐一邊欣賞風景。\n",
      "在台灣的交通很方便。我們可以坐公車去。這個時候這的天氣[MASK]化很大，所以衣服你應該多帶一點。 在台灣的交通很方便。我們可以坐公車去。這個時候這的天氣變化很大，所以衣服你應該多帶一點。\n",
      "你們先去台灣的政府在印[MASK]，帶你們的護照，如果他們給你簽發，你再可以去台灣。 你們先去台灣的政府在印尼，帶你們的護照，如果他們給你簽發，你再可以去台灣。\n",
      "現在在台北天氣不錯，你們不用帶很多冬天衣，現在台北的天氣跟印[MASK]一樣，我覺可以適應了。 現在在台北天氣不錯，你們不用帶很多冬天衣，現在台北的天氣跟印尼一樣，我覺可以適應了。\n",
      "我要帶你們去阿里山，在臺灣滿有名的地方。不但帶你們欣賞看山，我也打算帶你們去花[MASK]吃好吃的東西，看海邊。 我要帶你們去阿里山，在臺灣滿有名的地方。不但帶你們欣賞看山，我也打算帶你們去花蓮吃好吃的東西，看海邊。\n",
      "最後我要給你們看台北的夜市。我們一起逛街，買[MASK]些東西，試試看台灣的小吃，尤其是：臭豆腐。 最後我要給你們看台北的夜市。我們一起逛街，買一些東西，試試看台灣的小吃，尤其是：臭豆腐。\n",
      "媽媽來的時候，我的計畫是要帶您去臺中。那邊的風景好漂亮。我們可以爬山。臺灣有很多山。我們到臺灣[MASK]高鐵去。 媽媽來的時候，我的計畫是要帶您去臺中。那邊的風景好漂亮。我們可以爬山。臺灣有很多山。我們到臺灣坐高鐵去。\n",
      "現在是夏天，所以不要帶很多衣服。這[MASK]賣的衣服很便宜，等一下我們買幾件。 現在是夏天，所以不要帶很多衣服。這裡賣的衣服很便宜，等一下我們買幾件。\n",
      "從小到大，我媽媽育養我們很[MASK]苦，是因為我家裡說起來也比一般窮，而且家裡有五個小孩。 從小到大，我媽媽育養我們很辛苦，是因為我家裡說起來也比一般窮，而且家裡有五個小孩。\n",
      "我想告訴大家要好好[MASK]去照顧自己的媽媽，要懂得孝順父母。 我想告訴大家要好好地去照顧自己的媽媽，要懂得孝順父母。\n",
      "他是個既溫柔又幽[MASK]的人，在友圈裡沒有一個不喜歡他的人。 他是個既溫柔又幽默的人，在友圈裡沒有一個不喜歡他的人。\n",
      "還有一點讓我更欣[MASK]他的就是很孝順父母，也總是以樂觀的情形看待每件事情。 還有一點讓我更欣賞他的就是很孝順父母，也總是以樂觀的情形看待每件事情。\n",
      "當我心情很低落時，他會安慰我，使我有了更多的自信、也不斷地[MASK]勵我要做一個堅強的人，別輕易放棄。 當我心情很低落時，他會安慰我，使我有了更多的自信、也不斷地鼓勵我要做一個堅強的人，別輕易放棄。\n",
      "自從我認識他以來，我的人生也幾乎完全[MASK]改觀，再也不以悲觀的心情對待人生。 自從我認識他以來，我的人生也幾乎完全地改觀，再也不以悲觀的心情對待人生。\n",
      "我最欣賞的一個人是紐約洋基隊＂台灣之光＂王建民。他現在不但亞洲人最棒的投手，而且洋基隊的王[MASK]投手。 我最欣賞的一個人是紐約洋基隊＂台灣之光＂王建民。他現在不但亞洲人最棒的投手，而且洋基隊的王牌投手。\n",
      "長得高高瘦瘦的，臉有點像金[MASK]武的，他不是別人而是我親哥哥。 長得高高瘦瘦的，臉有點像金城武的，他不是別人而是我親哥哥。\n",
      "父親的朋友跟我說我爸爸之前[MASK]他過他的命，要不然他那時候就被壞人殺死了。 父親的朋友跟我說我爸爸之前救他過他的命，要不然他那時候就被壞人殺死了。\n",
      "我覺得我旁邊就有他，如果不在我身邊，但是也算是在我心裡，於是我應該會[MASK]好好兒的日子。 我覺得我旁邊就有他，如果不在我身邊，但是也算是在我心裡，於是我應該會過好好兒的日子。\n",
      "例如說從[MASK]觀的一個人變成一個很樂觀的人，短視的思考變成遠視的思考。 例如說從悲觀的一個人變成一個很樂觀的人，短視的思考變成遠視的思考。\n",
      "我很想帶你去看台灣的風景，還有帶你吃最好吃的台灣菜。台[MASK]菜又好吃又便宜。 我很想帶你去看台灣的風景，還有帶你吃最好吃的台灣菜。台灣菜又好吃又便宜。\n",
      "我可以安排很多的東西也可以帶他去玩一玩、參觀參[MASK]，她想做什麼我就帶她去做什麼。 我可以安排很多的東西也可以帶他去玩一玩、參觀參觀，她想做什麼我就帶她去做什麼。\n",
      "爸[MASK]，你們好？我是政桓。我來台灣十個多月了。我說中文說得越來越好。 爸媽，你們好？我是政桓。我來台灣十個多月了。我說中文說得越來越好。\n",
      "而且我知道你特別喜歡動物。那我們一起去動物園吧。聽說台北的動物[MASK]是亞洲最大的。 而且我知道你特別喜歡動物。那我們一起去動物園吧。聽說台北的動物園是亞洲最大的。\n",
      "你們來那一天舅舅會[MASK]我接你們，我們先把行李放好再去出吃飯，反正飛機上的東西不好吃，你們很餓吧！ 你們來那一天舅舅會陪我接你們，我們先把行李放好再去出吃飯，反正飛機上的東西不好吃，你們很餓吧！\n",
      "第二天氣好的話可以去淡水玩，那邊的風景還不錯，而且吃的東西多，我真[MASK]愛惜臺灣的傳統美食尤其是這邊的小吃棒極了。 第二天氣好的話可以去淡水玩，那邊的風景還不錯，而且吃的東西多，我真的愛惜臺灣的傳統美食尤其是這邊的小吃棒極了。\n",
      "除了吃東西以外，我們也可以參觀一零一，中正紀念堂什麼的，只怕媽媽會覺得無[MASK]。 除了吃東西以外，我們也可以參觀一零一，中正紀念堂什麼的，只怕媽媽會覺得無聊。\n",
      "按照我的計畫你們有什麼意見？請告訴我你們還想去哪裡，我才能[MASK]排好。 按照我的計畫你們有什麼意見？請告訴我你們還想去哪裡，我才能安排好。\n",
      "可惜你在台灣只能過三天兩夜，不過我已經好好兒[MASK]排好了帶你去故宮博物館你知道吧！來臺灣一定要去的！ 可惜你在台灣只能過三天兩夜，不過我已經好好兒安排好了帶你去故宮博物館你知道吧！來臺灣一定要去的！\n",
      "那下個月我等你們來。我[MASK]不得過日子因為我真想你們了。 那下個月我等你們來。我恨不得過日子因為我真想你們了。\n",
      "她來台灣沒有去上中文課，她買書回家自己學，很用功[MASK]學，三四年後，她的文強，現在她不是很會講而看報紙像臺灣人的速度。 她來台灣沒有去上中文課，她買書回家自己學，很用功地學，三四年後，她的文強，現在她不是很會講而看報紙像臺灣人的速度。\n",
      "她覺得中文可以跟台灣人溝通，她開始做生意。她在桃園開一家越南店，[MASK]越南菜，生意很好，客人大部分是越南外勞。 她覺得中文可以跟台灣人溝通，她開始做生意。她在桃園開一家越南店，賣越南菜，生意很好，客人大部分是越南外勞。\n",
      "為什麼他是我最難忘的人？因為[MASK]是我的初戀。他去美國以後，一次也沒看到他。可是我還是好想他。 為什麼他是我最難忘的人？因為他是我的初戀。他去美國以後，一次也沒看到他。可是我還是好想他。\n",
      "大約兩年前，我跟朋友去綠島旅行時，遇到[MASK]綠島的空軍軍人，他們就是最難忘的人。 大約兩年前，我跟朋友去綠島旅行時，遇到住綠島的空軍軍人，他們就是最難忘的人。\n",
      "我不會忘記那天的事情。我最愛的哥去世了，也跟我爸見個面。但我們並沒有打招呼。過了那天他也沒有出[MASK]在我們的見面了。 我不會忘記那天的事情。我最愛的哥去世了，也跟我爸見個面。但我們並沒有打招呼。過了那天他也沒有出現在我們的見面了。\n",
      "唱完歌[MASK]後他們還送了我一隻很大的無尾熊，我簡直太感動了，高興得無法控制我的眼淚流出來。 唱完歌之後他們還送了我一隻很大的無尾熊，我簡直太感動了，高興得無法控制我的眼淚流出來。\n",
      "過後我們還拍了不少照片，玩[MASK]很開心。我心裡真的很感謝他們幫我慶祝這麼難忘的生日。 過後我們還拍了不少照片，玩得很開心。我心裡真的很感謝他們幫我慶祝這麼難忘的生日。\n",
      "那一年可是我最難忘的生日，我也會好好地珍惜我們的友[MASK]。 那一年可是我最難忘的生日，我也會好好地珍惜我們的友誼。\n",
      "想到後馬上報了名旅行團。旅行團的行程雖然很緊湊，可是不會[MASK]苦。也可以吃中國的色香味俱有的菜和看各式各樣的中國的文物。 想到後馬上報了名旅行團。旅行團的行程雖然很緊湊，可是不會辛苦。也可以吃中國的色香味俱有的菜和看各式各樣的中國的文物。\n",
      "看完了後我才發現在[MASK]近有很多擺地攤。那些地方我可以看到名牌的。而且沒想到那麼便宜的。 看完了後我才發現在附近有很多擺地攤。那些地方我可以看到名牌的。而且沒想到那麼便宜的。\n",
      "我花了很多錢買名牌的包包。可是不到一個小時就破壞了。回家後我到百貨公司去才知道我買的[MASK]真的仿冒品！而且沒有保證書。 我花了很多錢買名牌的包包。可是不到一個小時就破壞了。回家後我到百貨公司去才知道我買的是真的仿冒品！而且沒有保證書。\n",
      "我痛下決心瘦身，我要使全世界刮目相看，不會[MASK]讓任何人罵我胖。 我痛下決心瘦身，我要使全世界刮目相看，不會再讓任何人罵我胖。\n",
      "所以，媽媽死了以後他們的家一點[MASK]沒有音樂。因為他們的爸爸想音樂的時候，一定想太太。 所以，媽媽死了以後他們的家一點都沒有音樂。因為他們的爸爸想音樂的時候，一定想太太。\n",
      "厄瓦是一個又可愛又[MASK]亮的機器女人（其實有雞蛋的樣子）。她也不會說話。所以，第一個鐘頭裡連一個句子都沒有。 厄瓦是一個又可愛又漂亮的機器女人（其實有雞蛋的樣子）。她也不會說話。所以，第一個鐘頭裡連一個句子都沒有。\n",
      "我最喜歡的電影[MASK]「龍貓」。它是日本宮崎駿的漫畫。 我最喜歡的電影是「龍貓」。它是日本宮崎駿的漫畫。\n",
      "時代[MASK]約５０年前日本。有兩個姐妹關於媽媽住的醫院的關係搬過來鄉下。父親是一個大學的教授。他們開始住在鄉下的很舊的家屋。 時代是約５０年前日本。有兩個姐妹關於媽媽住的醫院的關係搬過來鄉下。父親是一個大學的教授。他們開始住在鄉下的很舊的家屋。\n",
      "有一天兩個姐妹看奇妙的動物。然後遇到不可思議的體驗。看這個電影[MASK]我想出來小時候的鄉下的感覺。 有一天兩個姐妹看奇妙的動物。然後遇到不可思議的體驗。看這個電影讓我想出來小時候的鄉下的感覺。\n",
      "我外公住在一個小島的河[MASK]。國小的暑假跟寒假的時候我跟家人找他。 我外公住在一個小島的河上。國小的暑假跟寒假的時候我跟家人找他。\n",
      "２００９年１２月「葉問」就開場了。我看部電影在雅加達印尼，我第一次看到部電影的海報的時候，我以為[MASK]電影是超人故事。 ２００９年１２月「葉問」就開場了。我看部電影在雅加達印尼，我第一次看到部電影的海報的時候，我以為部電影是超人故事。\n",
      "因為海報上寫「ＩＰＭＡＮ」而且男主角還滿有名的明星，奇怪怎麼海報上圖畫看起來很古代的樣子，有一個人在[MASK]工夫的動作。 因為海報上寫「ＩＰＭＡＮ」而且男主角還滿有名的明星，奇怪怎麼海報上圖畫看起來很古代的樣子，有一個人在打工夫的動作。\n",
      "難道是什麼電影啊？結果我跟朋友們一起看部電影「[MASK]問」。 難道是什麼電影啊？結果我跟朋友們一起看部電影「葉問」。\n",
      "這部電影讓所有的華人要自信更加油站出來照顧華人的文化，不要被別人、別的國家欺[MASK]，華人也是人，有生活目的，有生命！ 這部電影讓所有的華人要自信更加油站出來照顧華人的文化，不要被別人、別的國家欺負，華人也是人，有生活目的，有生命！\n",
      "現在我們也可以看「葉問二」了。我已經看兩次了！跟葉問[MASK]一樣超好看！！！ 現在我們也可以看「葉問二」了。我已經看兩次了！跟葉問一一樣超好看！！！\n",
      "我最喜歡的電影是美國的卡通叫做「ＴＯＹＳＴＯＲＹ」。本來是為了學英文我要看這部電影。可是看[MASK]很多次也是沒有看膩。 我最喜歡的電影是美國的卡通叫做「ＴＯＹＳＴＯＲＹ」。本來是為了學英文我要看這部電影。可是看得很多次也是沒有看膩。\n",
      "我喜歡的是這個孩子跟這些玩具一起玩[MASK]很開心的樣子。 我喜歡的是這個孩子跟這些玩具一起玩得很開心的樣子。\n",
      "這家只有一個外婆，外婆已經九十歲了，可是還很健康。外婆的生活很[MASK]苦，沒有錢，每天外婆都去工作。 這家只有一個外婆，外婆已經九十歲了，可是還很健康。外婆的生活很辛苦，沒有錢，每天外婆都去工作。\n",
      "為什麼我喜歡這部電影呢？是因為這部電影有我[MASK]喜歡的歌名表演。她叫ＭａｎｄｙＭｏｏｒｅ。 為什麼我喜歡這部電影呢？是因為這部電影有我最喜歡的歌名表演。她叫ＭａｎｄｙＭｏｏｒｅ。\n",
      "那時候男生說當然他並不要愛這位女生。後來是因為每天都在一起，男生對女[MASK]很好。男生已經愛上這女生了。 那時候男生說當然他並不要愛這位女生。後來是因為每天都在一起，男生對女生很好。男生已經愛上這女生了。\n",
      "七天六夜的最後天比牠大的一隻公獅把小鹿給吃了。那隻母獅看起來很傷心地流淚。然後我也流淚。那天我最傷心的[MASK]天。 七天六夜的最後天比牠大的一隻公獅把小鹿給吃了。那隻母獅看起來很傷心地流淚。然後我也流淚。那天我最傷心的一天。\n",
      "我看的電影不多，因為我比較喜歡卡通片。我最喜歡看的卡通片是「[MASK]南」 我看的電影不多，因為我比較喜歡卡通片。我最喜歡看的卡通片是「柯南」\n",
      "「柯南」不像普通的卡通片，因為看「[MASK]南」會讓我們想一想到底發生什麼事情，讓我們動一動腦筋結果是誰殺人？ 「柯南」不像普通的卡通片，因為看「柯南」會讓我們想一想到底發生什麼事情，讓我們動一動腦筋結果是誰殺人？\n",
      "有[MASK]南的解釋我覺得我增加一個知識。 有柯南的解釋我覺得我增加一個知識。\n",
      "有一天一個公司來到鄉下。這個女生就[MASK]要試試看。後來因為太多人公司人員決定用大風吹的遊戲來找人。 有一天一個公司來到鄉下。這個女生就想要試試看。後來因為太多人公司人員決定用大風吹的遊戲來找人。\n",
      "她想學舞但是沒有錢後來[MASK]找一個方法就是在舞蹈的公司打工。然後她在那邊工作，一邊工作，一邊學舞。 她想學舞但是沒有錢後來她找一個方法就是在舞蹈的公司打工。然後她在那邊工作，一邊工作，一邊學舞。\n",
      "就是說她做了自己的挑戰。她想要看外面的環境。然後在她遇到失敗的時候她怎麼去跳出去從那個失[MASK]。 就是說她做了自己的挑戰。她想要看外面的環境。然後在她遇到失敗的時候她怎麼去跳出去從那個失落。\n",
      "「送達人」是在日本告別式的時候，來參加幫忙死人弄好。例如，幫他洗乾淨，換衣服，化[MASK]。 「送達人」是在日本告別式的時候，來參加幫忙死人弄好。例如，幫他洗乾淨，換衣服，化妝。\n",
      "我最喜歡的一部電影是叫做「二見[MASK]情」。這部電影是大概２０年前製作的電影。 我最喜歡的一部電影是叫做「二見鍾情」。這部電影是大概２０年前製作的電影。\n",
      "那個故事講我們一位女生的生活。那位女生的爸爸有新的老[MASK]。那位老婆的個性真的不好，老破不喜歡白雪公主。 那個故事講我們一位女生的生活。那位女生的爸爸有新的老婆。那位老婆的個性真的不好，老破不喜歡白雪公主。\n",
      "那個城市的名字還是西班牙文的，翻[MASK]的意思是「白色的房子」。 那個城市的名字還是西班牙文的，翻譯的意思是「白色的房子」。\n",
      "一天，那位美國人以前的女朋友就進去他的酒吧。兩年後再見面，可是現在她[MASK]先生陪她。 一天，那位美國人以前的女朋友就進去他的酒吧。兩年後再見面，可是現在她的先生陪她。\n",
      "對我來說，那個活動真[MASK]有意思、一方面可以暸解很多不同的活動、另外一方面可以認識那個球員。 對我來說，那個活動真的有意思、一方面可以暸解很多不同的活動、另外一方面可以認識那個球員。\n",
      "我聽說你跟你的女朋友訂婚了，恭喜你們！不[MASK]您說我有一點失望你都沒有邀請我參加訂婚禮，不過我可以去你們十月的婚禮吧？ 我聽說你跟你的女朋友訂婚了，恭喜你們！不瞞您說我有一點失望你都沒有邀請我參加訂婚禮，不過我可以去你們十月的婚禮吧？\n",
      "除了訂婚禮[MASK]外你暑假過得好嗎？我希望你找到了打工的機會，讓你有賺錢的方法。 除了訂婚禮以外你暑假過得好嗎？我希望你找到了打工的機會，讓你有賺錢的方法。\n",
      "校園會有十幾位有名的拉麵廚師擺地攤賣拉麵[MASK]各種各樣的肉、魚、蔬菜等等。 校園會有十幾位有名的拉麵廚師擺地攤賣拉麵配各種各樣的肉、魚、蔬菜等等。\n",
      "你應該知道楊丞琳是我的大學研究所畢業，她不只是一個很漂亮的明星歌手，她也非常[MASK]努力又聰明。 你應該知道楊丞琳是我的大學研究所畢業，她不只是一個很漂亮的明星歌手，她也非常地努力又聰明。\n",
      "我不知道她喜不喜歡拉麵，可是我一想到楊[MASK]琳吃拉麵就很興奮了。 我不知道她喜不喜歡拉麵，可是我一想到楊丞琳吃拉麵就很興奮了。\n",
      "這些活動一定會讓你有意思。你來[MASK]加這些活動，很開心得不得了。 這些活動一定會讓你有意思。你來參加這些活動，很開心得不得了。\n",
      "我跟你說喔，我參加了我[MASK]學校的一個活動，你猜他們找誰來。 我跟你說喔，我參加了我們學校的一個活動，你猜他們找誰來。\n",
      "我覺得這個活動真的很棒，不只是因為他們有找李大明來表演也是因為大家都玩[MASK]很開心。 我覺得這個活動真的很棒，不只是因為他們有找李大明來表演也是因為大家都玩得很開心。\n",
      "好久不見，我是李大明還記得我嗎，最近過[MASK]還好嗎？ 好久不見，我是李大明還記得我嗎，最近過得還好嗎？\n",
      "今天我參加了學校舉辦的一場活動，我很喜歡，所以我想寫[MASK]封信給你，想跟你談談我今天的感情。 今天我參加了學校舉辦的一場活動，我很喜歡，所以我想寫這封信給你，想跟你談談我今天的感情。\n",
      "他在這裡念書，念[MASK]很好，老師們都喜歡他。 他在這裡念書，念得很好，老師們都喜歡他。\n",
      "我們學[MASK]請她來是因為讓學生們可以從他身上學到一些東西。我很羨慕他，真想跟他一樣。 我們學校請她來是因為讓學生們可以從他身上學到一些東西。我很羨慕他，真想跟他一樣。\n",
      "我[MASK]你天天快樂！ 我祝你天天快樂！\n",
      "好！說到這[MASK]我就想要聽聽看他的歌了。 好！說到這裡我就想要聽聽看他的歌了。\n",
      "因為他很紅，所以參加的人很多，排隊線很長，[MASK]位很快就滿了，還有很多人站著參加。 因為他很紅，所以參加的人很多，排隊線很長，座位很快就滿了，還有很多人站著參加。\n",
      "每個人都好[MASK]看見他，聽他講話！ 每個人都好想看見他，聽他講話！\n",
      "希望你在英國一切也好，學習過[MASK]順利！ 希望你在英國一切也好，學習過得順利！\n",
      "不久以後，楊先生就[MASK]來了開始演講。 不久以後，楊先生就進來了開始演講。\n",
      "希望你在大學時也有[MASK]私的機會，如果有的話不要忘記寫信給我！ 希望你在大學時也有似私的機會，如果有的話不要忘記寫信給我！\n",
      "我姐姐也在這裡念書。她已經畢業了差不多了。她的同學都很高興，因為[MASK]們爸媽打算舉行晚會好好兒地慶祝慶祝。 我姐姐也在這裡念書。她已經畢業了差不多了。她的同學都很高興，因為他們爸媽打算舉行晚會好好兒地慶祝慶祝。\n",
      "我們都知道他愛看韓國的電影，還有喜歡看美國籃[MASK]比賽。 我們都知道他愛看韓國的電影，還有喜歡看美國籃球比賽。\n",
      "那場活動真熱鬧，為了看他的演[MASK]，很多人特別來了我們學校，來的人數應該超過了一千多人吧！ 那場活動真熱鬧，為了看他的演唱，很多人特別來了我們學校，來的人數應該超過了一千多人吧！\n",
      "那我[MASK]等你的連絡。最近天氣開始變熱了，小心不要中暑！ 那我再等你的連絡。最近天氣開始變熱了，小心不要中暑！\n",
      "真[MASK]很可惜妳生病不能來參加！可是不用擔心，我一定會去醫院看妳和給妳看照片，我拍很多照片了！ 真的很可惜妳生病不能來參加！可是不用擔心，我一定會去醫院看妳和給妳看照片，我拍很多照片了！\n",
      "好幾天了沒看到你，現在你怎麼樣，我希望你什麼都好。現在我也很好，沒有什麼[MASK]要關心。 好幾天了沒看到你，現在你怎麼樣，我希望你什麼都好。現在我也很好，沒有什麼事要關心。\n",
      "怎麼樣？我最近非常[MASK]開心，你知道為什麼嗎？因為我交了一個女朋友。 怎麼樣？我最近非常地開心，你知道為什麼嗎？因為我交了一個女朋友。\n",
      "妳跟妳的家人最近過得好[MASK]？有沒有什麼消息？ 妳跟妳的家人最近過得好嗎？有沒有什麼消息？\n",
      "有一天我跟朋友們一起去參加一個生日的派[MASK]。那邊我碰到一個非常棒的男生！他的名字是「大沙」。 有一天我跟朋友們一起去參加一個生日的派對。那邊我碰到一個非常棒的男生！他的名字是「大沙」。\n",
      "[MASK]愛的，從那一天我們一起出去幾次。所以我又比較多時間了解他是什麼樣子的。他真是一個聰明的人。 親愛的，從那一天我們一起出去幾次。所以我又比較多時間了解他是什麼樣子的。他真是一個聰明的人。\n",
      "[MASK]愛的李大明，我真不知道怎麼幫！但是我感覺是我越來越多愛他．．．妳有空的話來找我。我希望我的男朋友介紹給妳。 親愛的李大明，我真不知道怎麼幫！但是我感覺是我越來越多愛他．．．妳有空的話來找我。我希望我的男朋友介紹給妳。\n",
      "[MASK]的個性很溫柔，她對別人都很好，很有禮貌。對我來說誰都比不上她。 她的個性很溫柔，她對別人都很好，很有禮貌。對我來說誰都比不上她。\n",
      "我們在吃飯的時候，很像一個電影，那個餐廳的環境非[MASK]適合跟你喜歡的人去吃飯。 我們在吃飯的時候，很像一個電影，那個餐廳的環境非常適合跟你喜歡的人去吃飯。\n",
      "大明，改天來基隆[MASK]！我幫你認識我的男朋友。確定你會喜歡他！王先生是一個大好人！ 大明，改天來基隆玩！我幫你認識我的男朋友。確定你會喜歡他！王先生是一個大好人！\n",
      "今天我很興[MASK]想跟你分享，希望你給我意見我這樣交朋友對嗎？因為他會影響到我一輩子！我也很當心以後是如何沒人可料到。 今天我很興奮想跟你分享，希望你給我意見我這樣交朋友對嗎？因為他會影響到我一輩子！我也很當心以後是如何沒人可料到。\n",
      "我有好消息。就是因為我交了一個女朋友，所以我最近很開心。你是我的[MASK]好朋友，所以我想要把這個好消息告訴你。 我有好消息。就是因為我交了一個女朋友，所以我最近很開心。你是我的最好朋友，所以我想要把這個好消息告訴你。\n",
      "你知道嗎？我跟她在一起的時候，我覺得我們一定是命[MASK]的雙愛人。老天爺把她送給我，相信我不會錯了。 你知道嗎？我跟她在一起的時候，我覺得我們一定是命裡的雙愛人。老天爺把她送給我，相信我不會錯了。\n",
      "她是好甜的女人，我要告訴你。每個週末，她到我的地方來幫我作房事。然後就好好[MASK]做菜給我吃。 她是好甜的女人，我要告訴你。每個週末，她到我的地方來幫我作房事。然後就好好地做菜給我吃。\n",
      "我們都喜歡去夜市，看電影，買東，什麼的。她也喜歡東部的風[MASK]好漂亮。 我們都喜歡去夜市，看電影，買東，什麼的。她也喜歡東部的風景好漂亮。\n",
      "她常常照顧她姐姐的孩[MASK]因為她最喜歡孩子，覺得她要有孩。我希望你回答我的信。 她常常照顧她姐姐的孩子因為她最喜歡孩子，覺得她要有孩。我希望你回答我的信。\n",
      "我要把一本書介紹給你。我是從電視上知道的。簡單[MASK]說內容是怎麼過人生。 我要把一本書介紹給你。我是從電視上知道的。簡單地說內容是怎麼過人生。\n",
      "我很喜歡這本書，因為，那個人很聰明的人，而[MASK]那個時候有他才有公平。所以，我真的很喜他 我很喜歡這本書，因為，那個人很聰明的人，而且那個時候有他才有公平。所以，我真的很喜他\n",
      "美國進入第二世界戰爭的時候，蔣介石問美國幫打仗日本。他需要美國的軍糧，要把國[MASK]軍現代化。 美國進入第二世界戰爭的時候，蔣介石問美國幫打仗日本。他需要美國的軍糧，要把國民軍現代化。\n",
      "蔣介石當史迪威國民軍委員的總統，說史迪[MASK]可以領導中國的軍團。 蔣介石當史迪威國民軍委員的總統，說史迪威可以領導中國的軍團。\n",
      "我介紹幾米的「星空」。幾米是台灣人的繪本作家，我來台灣以後去他的展覽，那時候我才知道他，[MASK]畫的繪真的漂亮得我很感動。 我介紹幾米的「星空」。幾米是台灣人的繪本作家，我來台灣以後去他的展覽，那時候我才知道他，他畫的繪真的漂亮得我很感動。\n",
      "我想我喜歡這本書因為它很有意思，不太難，孩子看也懂。這本書還有很多教事：忠、孝、[MASK]、仁。 我想我喜歡這本書因為它很有意思，不太難，孩子看也懂。這本書還有很多教事：忠、孝、禮、仁。\n",
      "這時候[MASK]給我推薦了這本書再把一張紙寫下來那個書的名字。他還告訴我在哪裡可以買到這本書。 這時候她給我推薦了這本書再把一張紙寫下來那個書的名字。他還告訴我在哪裡可以買到這本書。\n",
      "現在如果我們信神說的話，馬上就他的靈進去我們的靈，開始住在我們的[MASK]面。 現在如果我們信神說的話，馬上就他的靈進去我們的靈，開始住在我們的裡面。\n",
      "我跟你住在一起[MASK]經一個月了，我覺得你有不好的生活習慣，你晚上都很晚睡，都在聽音樂！很大聲！ 我跟你住在一起已經一個月了，我覺得你有不好的生活習慣，你晚上都很晚睡，都在聽音樂！很大聲！\n",
      "我們大家希望你能小聲一點，或[MASK]是早一點睡！健康也好．．． 我們大家希望你能小聲一點，或者是早一點睡！健康也好．．．\n",
      "我希望你減少玩遊戲的時間，注重學習，還有晚上早一點睡，你這樣子做不但你的建康好，而[MASK]你的分數會有進步。 我希望你減少玩遊戲的時間，注重學習，還有晚上早一點睡，你這樣子做不但你的建康好，而且你的分數會有進步。\n",
      "十二點之後，麻煩你關燈睡覺，不要再上網跟朋友聊天了，好[MASK]？ 十二點之後，麻煩你關燈睡覺，不要再上網跟朋友聊天了，好嗎？\n",
      "我誠實[MASK]建議你得更注意這些方面，以免麻煩別人。 我誠實地建議你得更注意這些方面，以免麻煩別人。\n",
      "你要專心做家事，與其很快地，但馬馬虎虎做事，不如好好地幫助對[MASK]，對不對？ 你要專心做家事，與其很快地，但馬馬虎虎做事，不如好好地幫助對方，對不對？\n",
      "說實在的，目前我真感覺很悶，我不要因這個小小的事情而會被迫請師長[MASK]房間。跟你住在一起的日子我過得很愉快。 說實在的，目前我真感覺很悶，我不要因這個小小的事情而會被迫請師長換房間。跟你住在一起的日子我過得很愉快。\n",
      "幫助別人是一個好的行動，我曾經幫助很多人，不過我最印[MASK]的事是我幫助一個小學學生。 幫助別人是一個好的行動，我曾經幫助很多人，不過我最印象的事是我幫助一個小學學生。\n",
      "有一次我的朋友給我打電話，就說她有一個問題。她一邊哭一邊告訴我她的困難，她跟她的男朋友[MASK]架了。 有一次我的朋友給我打電話，就說她有一個問題。她一邊哭一邊告訴我她的困難，她跟她的男朋友吵架了。\n",
      "公寓在師大夜市旁邊，但是情況很安[MASK]。交通非常方便，不管要去哪裡，捷運、公車都有。 公寓在師大夜市旁邊，但是情況很安靜。交通非常方便，不管要去哪裡，捷運、公車都有。\n",
      "我住在大樓附近，除了附近有漂亮得沒話說的國立公園。我們跟我的朋友可以考幾個鴨。我祝你決定[MASK]來這棟大樓。 我住在大樓附近，除了附近有漂亮得沒話說的國立公園。我們跟我的朋友可以考幾個鴨。我祝你決定住來這棟大樓。\n",
      "我的房東是一個很好的[MASK]，我要甚麼幫助他一定幫你甚麼忙，我總是會喜歡那個房間。 我的房東是一個很好的人，我要甚麼幫助他一定幫你甚麼忙，我總是會喜歡那個房間。\n",
      "我們房東也是個好人好[MASK]爸爸一樣，他有一個女兒差不多我們一樣大，他也是我的好朋友，你一看到他就會喜歡的。 我們房東也是個好人好像爸爸一樣，他有一個女兒差不多我們一樣大，他也是我的好朋友，你一看到他就會喜歡的。\n",
      "大樓附近超市，便利店，夜市也有。所以，生活的環境也不錯。我覺得夜裡的房子很安[MASK]，交通跟旁邊的環境很好。 大樓附近超市，便利店，夜市也有。所以，生活的環境也不錯。我覺得夜裡的房子很安靜，交通跟旁邊的環境很好。\n",
      "我住的大樓有１個空[MASK]你想住嗎？ 我住的大樓有１個空房你想住嗎？\n",
      "要是有趣，回信告訴我，我就給你[MASK]詳細的資料。 要是有趣，回信告訴我，我就給你更詳細的資料。\n",
      "我的房東一個老人。他[MASK]切中點帶嚴格，他住跟我一樣的大樓，可是他常常去出國旅行，所以大部分不在家裡。 我的房東一個老人。他親切中點帶嚴格，他住跟我一樣的大樓，可是他常常去出國旅行，所以大部分不在家裡。\n",
      "而且交通很方便，有一個捷運站、公車站，[MASK]這裡到你的學校的公車也很多。 而且交通很方便，有一個捷運站、公車站，從這裡到你的學校的公車也很多。\n",
      "可是房東另外沒有什麼特別不好的地方所以住起來很不錯。所以啦，你可以考好好地想一想[MASK]。 可是房東另外沒有什麼特別不好的地方所以住起來很不錯。所以啦，你可以考好好地想一想吧。\n",
      "你要是錯過這麼好房間，沒有更好的房間喔！你應該早點決定吧！給我聯[MASK]，拜拜。 你要是錯過這麼好房間，沒有更好的房間喔！你應該早點決定吧！給我聯絡，拜拜。\n",
      "我覺得那個房間非常好，因為房間很大，很乾淨，[MASK]加上電費和水費都免費。房間裡面有一個床，櫃子，電視機，光碟，什麼的。 我覺得那個房間非常好，因為房間很大，很乾淨，再加上電費和水費都免費。房間裡面有一個床，櫃子，電視機，光碟，什麼的。\n",
      "那一天是我最快樂的一天，因為我跟家人、朋友們、男朋友都一起慶祝我的生日，[MASK]虧他們很愛我，要不然我就死定了。 那一天是我最快樂的一天，因為我跟家人、朋友們、男朋友都一起慶祝我的生日，幸虧他們很愛我，要不然我就死定了。\n",
      "我媽媽平常都忙著工作，連照顧自[MASK]也沒時間。這次我感覺到很溫暖的母子情。我更多愛她，很愛她。 我媽媽平常都忙著工作，連照顧自己也沒時間。這次我感覺到很溫暖的母子情。我更多愛她，很愛她。\n",
      "因為我這時候很高興，緊張，又感動、所以不太清楚我[MASK]麼樣求婚，但是結果我們決定結婚。 因為我這時候很高興，緊張，又感動、所以不太清楚我怎麼樣求婚，但是結果我們決定結婚。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"                \\nfor sent, cor in zip(sents, cors):\\n    if '0' in cor :continue\\n    for i in range(0, len(cor), 2):\\n        sent = list(sent)\\n        sent[int(cor[i])-1] = cor[i + 1]\\n        cor_sent = ''.join(list(sent))\\n    #print(cor_sent, cor)\\n    \\nfor sent, cor, inpt in zip(sents, cors, inp):\\n    print(sent, inpt, cor)\\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents, inps = [], []\n",
    "with open(\"1209\\\\CLP14_CSC_TestInput.txt\",'r',encoding='utf-8') as label:\n",
    "    while True:\n",
    "        line = label.readline().strip()        \n",
    "        if not line:break\n",
    "        #if len(line) > 60:continue\n",
    "        sents.append(line.split()[1])\n",
    "        #print(sent)\n",
    "        \n",
    "cors = []        \n",
    "with open(\"1209\\\\CLP14_CSC_TestTruth.txt\",'r',encoding='utf-8') as label:\n",
    "    while True:\n",
    "        line = label.readline().strip()\n",
    "        if not line:break\n",
    "        #if '0' in line.split(', '):continue\n",
    "        cors.append(line.split(', ')[1:])\n",
    "print(len(cors), len(sents))\n",
    "\n",
    "cor_sents = []\n",
    "labels, n = [], -1\n",
    "for sent, cor in zip(sents, cors):\n",
    "    if '0' in cor or len(sent) > 60:continue\n",
    "    label = [0 for i in range(110)]\n",
    "    \n",
    "    for i in range(0, len(cor), 2):\n",
    "        label[int(cor[i])-1] = 1\n",
    "        sent = list(sent)\n",
    "        mask_inp = list(sent)\n",
    "        mask_inp[int(cor[i])-1] = '[MASK]'\n",
    "        inp = ''.join(mask_inp)\n",
    "        sent[int(cor[i])-1] = cor[i + 1]\n",
    "        cor_sent = ''.join(list(sent))\n",
    "        \n",
    "    cor_sents.append(cor_sent)   \n",
    "    labels.append(label)\n",
    "    inps.append(inp)\n",
    "\n",
    "    \n",
    "for inpt, cor in zip(inps, cor_sents):\n",
    "    print(inpt, cor)\n",
    "'''        \n",
    "labels, n = [], -1\n",
    "with open(\"1209\\\\test_answer_14.txt\",'r',encoding='utf-8') as label:\n",
    "    while True:\n",
    "        line = label.readline().strip()  \n",
    "        \n",
    "        #print(n)\n",
    "        #print(line, len(sents[n]), len(line))\n",
    "        if not line or n > len(sents):break \n",
    "        n += 1\n",
    "        if not '1' in line:continue          \n",
    "        if len(sents[n]) > 60:continue\n",
    "        labels.append([int(i) for i in line]+[0 for _ in range(110-len(line))])\n",
    "        \n",
    "        tmp = list(sents[n])\n",
    "        for i in range(len(tmp)):            \n",
    "            if line[i] == '1':      \n",
    "                tmp[i] = '[MASK]'\n",
    "                e = ''.join(tmp)\n",
    "        inp.append(e)\n",
    "for cor, inpt in zip(cors, inp):\n",
    "    print(inpt, cor) \n",
    "'''\n",
    "'''                \n",
    "for sent, cor in zip(sents, cors):\n",
    "    if '0' in cor :continue\n",
    "    for i in range(0, len(cor), 2):\n",
    "        sent = list(sent)\n",
    "        sent[int(cor[i])-1] = cor[i + 1]\n",
    "        cor_sent = ''.join(list(sent))\n",
    "    #print(cor_sent, cor)\n",
    "    \n",
    "for sent, cor, inpt in zip(sents, cors, inp):\n",
    "    print(sent, inpt, cor)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'       \\nwith open(\"1209\\\\testCorpus.txt\",\\'r\\',encoding=\\'utf-8\\') as test:\\n    while True:\\n        line = test.readline().strip()\\n        if not line:break\\n        texts.append(line)\\n        labels.append([0 for j in range(MAX_LEN)])\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels, inp, cor = [], [], []\n",
    "err_n = 0\n",
    "with open(\"1209\\\\test_input_text_answer_15.txt\",'r',encoding='utf-8') as label:\n",
    "    while True:\n",
    "        line = label.readline().strip()\n",
    "        if not line:break\n",
    "        e, c, l = line.split('-***-')\n",
    "        cor.append(c)        \n",
    "        labels.append([int(i) for i in l]+[0 for _ in range(110-len(l))])\n",
    "        tmp = list(e)\n",
    "        for i in range(len(tmp)):            \n",
    "            if l[i] == '1':      \n",
    "                err_n += 1\n",
    "                tmp[i] = '[MASK]'\n",
    "                e = ''.join(tmp)\n",
    "        inp.append(e)\n",
    "print(err_n)\n",
    "#print(inp, cor, labels)\n",
    "'''       \n",
    "with open(\"1209\\\\testCorpus.txt\",'r',encoding='utf-8') as test:\n",
    "    while True:\n",
    "        line = test.readline().strip()\n",
    "        if not line:break\n",
    "        texts.append(line)\n",
    "        labels.append([0 for j in range(MAX_LEN)])\n",
    "''' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\islab\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers-4.1.1-py3.8.egg\\transformers\\tokenization_utils_base.py:2179: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([390, 110]) torch.Size([390, 110])\n",
      "536\n",
      "tensor(340)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>真實標記</th>\n",
       "      <th>預測標記</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>相對的、每位產齡婦女的生育嬰兒個數卻 [持] 續下滑。這表示全球出現適合年齡生育的婦女不想生...</td>\n",
       "      <td>相對的、每位產齡婦女的生育嬰兒個數卻 [持] 續下滑。這表示全球出現適合年齡生育的婦女不想生...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>問 [題] 就在於，從１９５０－１９６０年雖然產齡婦女率低，但平均每位產齡婦女生育的嬰兒率高。</td>\n",
       "      <td>問 [題] 就在於，從１９５０－１９６０年雖然產齡婦女率低，但平均每位產齡婦女生育的嬰兒率高。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>以前戰爭時代沒什麼醫 [療] 設備像現在那麼普遍，但一口家庭生孩子的比率增多，相反地死亡的數...</td>\n",
       "      <td>以前戰爭時代沒什麼醫 [療] 設備像現在那麼普遍，但一口家庭生孩子的比率增多，相反地死亡的數...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>在 [此] ，我想最重要的還是能於培養出一群傑出的資源，能於為國家爭榮、願意貢獻一份力量的人才。</td>\n",
       "      <td>在 [德] ，我想最重要的還是能於培養出一群傑出的資源，能於為國家爭榮、願意貢獻一份力量的人才。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>後來因為人民生得越多，人口大幅 [地] 增加，產生了不少社會問題</td>\n",
       "      <td>後來因為人民生得越多，人口大幅 [度] 增加，產生了不少社會問題</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>你要是錯過這麼好房間，沒有更好的房間喔！你應該早點決定吧！給我 [聯]  [絡] ，拜拜。</td>\n",
       "      <td>你要是錯過這麼好房間，沒有更好的房間喔！你應該早點決定吧！給我 [聯]  [絡] ，拜拜。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>我覺得那個房間非常好，因為房間很大，很乾淨， [再] 加上電費和水費都免費。房間裡面有一個床...</td>\n",
       "      <td>我覺得那個房間非常好，因為房間很大，很乾淨， [再] 加上電費和水費都免費。房間裡面有一個床...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>那一天是我最快樂的一天，因為我跟家人、朋友們、男朋友都一起慶祝我的生日， [幸] 虧他們很愛...</td>\n",
       "      <td>那一天是我最快樂的一天，因為我跟家人、朋友們、男朋友都一起慶祝我的生日， [因] 虧他們很愛...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>我媽媽平常都忙著工作，連照顧自 [己] 也沒時間。這次我感覺到很溫暖的母子情。我更多愛她，很愛她。</td>\n",
       "      <td>我媽媽平常都忙著工作，連照顧自 [己] 也沒時間。這次我感覺到很溫暖的母子情。我更多愛她，很愛她。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>因為我這時候很高興，緊張，又感動、所以不太清楚我 [怎] 麼樣求婚，但是結果我們決定結婚。</td>\n",
       "      <td>因為我這時候很高興，緊張，又感動、所以不太清楚我 [什] 麼樣求婚，但是結果我們決定結婚。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>380 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  真實標記  \\\n",
       "0    相對的、每位產齡婦女的生育嬰兒個數卻 [持] 續下滑。這表示全球出現適合年齡生育的婦女不想生...   \n",
       "1      問 [題] 就在於，從１９５０－１９６０年雖然產齡婦女率低，但平均每位產齡婦女生育的嬰兒率高。   \n",
       "2    以前戰爭時代沒什麼醫 [療] 設備像現在那麼普遍，但一口家庭生孩子的比率增多，相反地死亡的數...   \n",
       "3     在 [此] ，我想最重要的還是能於培養出一群傑出的資源，能於為國家爭榮、願意貢獻一份力量的人才。   \n",
       "4                     後來因為人民生得越多，人口大幅 [地] 增加，產生了不少社會問題   \n",
       "..                                                 ...   \n",
       "375      你要是錯過這麼好房間，沒有更好的房間喔！你應該早點決定吧！給我 [聯]  [絡] ，拜拜。   \n",
       "376  我覺得那個房間非常好，因為房間很大，很乾淨， [再] 加上電費和水費都免費。房間裡面有一個床...   \n",
       "377  那一天是我最快樂的一天，因為我跟家人、朋友們、男朋友都一起慶祝我的生日， [幸] 虧他們很愛...   \n",
       "378  我媽媽平常都忙著工作，連照顧自 [己] 也沒時間。這次我感覺到很溫暖的母子情。我更多愛她，很愛她。   \n",
       "379      因為我這時候很高興，緊張，又感動、所以不太清楚我 [怎] 麼樣求婚，但是結果我們決定結婚。   \n",
       "\n",
       "                                                  預測標記  \n",
       "0    相對的、每位產齡婦女的生育嬰兒個數卻 [持] 續下滑。這表示全球出現適合年齡生育的婦女不想生...  \n",
       "1      問 [題] 就在於，從１９５０－１９６０年雖然產齡婦女率低，但平均每位產齡婦女生育的嬰兒率高。  \n",
       "2    以前戰爭時代沒什麼醫 [療] 設備像現在那麼普遍，但一口家庭生孩子的比率增多，相反地死亡的數...  \n",
       "3     在 [德] ，我想最重要的還是能於培養出一群傑出的資源，能於為國家爭榮、願意貢獻一份力量的人才。  \n",
       "4                     後來因為人民生得越多，人口大幅 [度] 增加，產生了不少社會問題  \n",
       "..                                                 ...  \n",
       "375      你要是錯過這麼好房間，沒有更好的房間喔！你應該早點決定吧！給我 [聯]  [絡] ，拜拜。  \n",
       "376  我覺得那個房間非常好，因為房間很大，很乾淨， [再] 加上電費和水費都免費。房間裡面有一個床...  \n",
       "377  那一天是我最快樂的一天，因為我跟家人、朋友們、男朋友都一起慶祝我的生日， [因] 虧他們很愛...  \n",
       "378  我媽媽平常都忙著工作，連照顧自 [己] 也沒時間。這次我感覺到很溫暖的母子情。我更多愛她，很愛她。  \n",
       "379      因為我這時候很高興，緊張，又感動、所以不太清楚我 [什] 麼樣求婚，但是結果我們決定結婚。  \n",
       "\n",
       "[380 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs, test_masks_i = preprocessing_for_bert(inps)\n",
    "test_corrects, test_masks_c = preprocessing_for_bert(cor_sents)\n",
    "test_labels = torch.tensor(labels)\n",
    "print(test_labels.size(), test_corrects.size())\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_labels, test_corrects, test_masks_i)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "#cor_count, df, y_real_s, y_pred_s = get_test_result(test_dataloader)\n",
    "num, df = get_test_result(test_dataloader)\n",
    "\n",
    "#df.to_csv('test/Bert20WLSTMSIGHAN2014.csv'.format(epochs,mode))\n",
    "print(num)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "maskedLM_model = BertForMaskedLM.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "maskedLM_model = maskedLM_model.to(device)\n",
    "def get_test_result(dataloader):\n",
    "    bert.eval()\n",
    "    y_real_s = []\n",
    "    y_pred_s = []\n",
    "    cor_count = 0\n",
    "    \n",
    "    data = {\n",
    "    \"真實標記\":[],\n",
    "    \"預測標記\":[]\n",
    "    }\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    for batch in dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_labels, b_target_ids, b_masks = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        for b_input_id, b_label, b_target_id, b_mask in zip(b_input_ids, b_labels, b_target_ids, b_masks):\n",
    "            \n",
    "            masked_index = torch.nonzero(b_label).cpu().numpy()[:,0]\n",
    "            token = [int(i) for i in b_input_id.cpu().numpy() if i != 0] \n",
    "            sentence = tokenizer.decode(b_target_id[:len(token)]) #轉成中文句\n",
    "            realList = sentence.split()\n",
    "            corList = sentence.split()\n",
    "                \n",
    "            if len(masked_index) == 0 or '[UNK]' in realList or 'ｓａｍｇｅｔａｎｇ' in realList:continue\n",
    "            \n",
    "            # Compute logits\n",
    "            with torch.no_grad():\n",
    "                logits = maskedLM_model(input_ids = torch.unsqueeze(b_input_id, 0), attention_mask = torch.unsqueeze(b_mask, 0), labels=torch.unsqueeze(b_target_id, 0) )   \n",
    "            # Compute loss\n",
    "            #losses = logits[0]\n",
    "            m_logits = logits[1].cpu().numpy()\n",
    "            full = torch.max(torch.tensor(m_logits[0]), 1)[1].data\n",
    "            print(tokenizer.decode(full))\n",
    "            mask_logits = torch.tensor([m_logits[0][i] for i in masked_index])\n",
    "            real_ids = torch.tensor([b_target_id.cpu().numpy()[i] for i in masked_index])\n",
    "            pred_ids = torch.max(mask_logits, 1)[1].data\n",
    "        \n",
    "            text = tokenizer.decode(pred_ids)\n",
    "            text = text.split()\n",
    "            if len(text) != len(masked_index):continue\n",
    "            count = -1\n",
    "            #print(''.join(realList), masked_index, text)\n",
    "            for i in masked_index:\n",
    "                count+=1\n",
    "                corList[i] = ' [' + text[count] + '] '\n",
    "                realList[i] = ' [' + realList[i] + '] '\n",
    "                \n",
    "            data[\"真實標記\"].append(''.join(realList))\n",
    "            data[\"預測標記\"].append(''.join(corList))\n",
    "            cor_count += sum(real_ids == pred_ids)\n",
    "            '''\n",
    "            ## print the result\n",
    "            print(torch.max(mask_logits, 1)[1].data, real_ids)\n",
    "            print(loss(mask_logits, real_ids), losses)\n",
    "            #loss(mask_logits, real_ids)\n",
    "            text = tokenizer.decode(b_input_id)\n",
    "            tokens = tokenizer.tokenize(text)\n",
    "\n",
    "            print(\"輸入 tokens ：\", tokens[:])\n",
    "            print('-' * 50)\n",
    "            for mask_id in masked_index:\n",
    "                probs, indices = torch.topk(torch.softmax(logits[1][0][mask_id], -1), 1)\n",
    "                predicted_tokens = tokenizer.convert_ids_to_tokens(indices.tolist())\n",
    "                for i, (t, p) in enumerate(zip(predicted_tokens, probs), 1):\n",
    "                    tokens[mask_id] = t\n",
    "            print(\"{}\".format(tokens[:]))\n",
    "            '''\n",
    "        # Get the predictions    \n",
    "        #pred = torch.max(logits[1], 1)[1].data\n",
    "        #print(loss, pred)\n",
    "        #y_pred_s += list(pred.cpu().numpy())          \n",
    "        #y_real_s += list(b_labels.cpu().numpy())\n",
    "\n",
    "        #sentence = tokenizer.decode(b_input_ids)\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv('test/RoBertaCorrTestSIGHAN15origin.csv')\n",
    "    return cor_count, df\n",
    "\n",
    "# Compute the average accuracy and loss over the validation set.\n",
    "\n",
    "#y_real_s, y_pred_s = get_test_result(val_dataloader)\n",
    "#num, df = get_test_result(val_dataloader)\n",
    "#df.to_csv('test/Bert{}EpochErr_{}.csv'.format(epochs,mode))\n",
    "#df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "嬰 對 的 、 每 位 年 齡 婦 女 的 生 育 嬰 兒 個 數 卻 持 續 下 滑 。 這 表 示 全 球 出 現 適 合 年 齡 生 育 的 婦 女 不 想 生 育 的 現 象 。 對 對 對 對 相 相 相 相 相 、 相 的 相 ， 相 、 每 、 每 位 婦 的 的 生 生 嬰 的 個 持 卻 持 持 持 續 每 、 的 的 的 生 不 的 不 不 適 不 、 生 的 婦 、 不 不 不 再 生 的 的 的 相 相 對 ，\n",
      "嬰 這 就 在 於 ， 從 １９５０ － １９６０ 年 雖 然 產 齡 婦 女 率 低 ， 但 平 均 每 位 產 齡 婦 女 生 育 的 嬰 兒 率 高 。 這 這 這 這 這 ， 而 這 這 ， 這 ， 這 原 這 這 原 也 題 ， 這 ， 從 從 從５ 年 年 年 到 到 年 ， ， ， ， 均 產 均 在 年 年 年 年 到 到 ， 年 位 的 的 的 的 的 的 的 較 高 ， ， 到 而 ， ， ， 這 ，\n",
      "的 前 戰 爭 時 代 沒 什 麼 醫 療 設 備 像 現 在 那 麼 普 遍 ， 但 一 口 家 庭 生 孩 子 的 比 率 增 多 ， 相 反 地 死 亡 的 數 量 也 更 多 。 因 因 而 在 死 ， 療 療 療 療 療 ， ， 在 ， 的 ， ， ， 醫 療 醫 療 療 療 像 會 像 那 的 ， ， ， ， ， 醫 療 醫 療 療 的 的 的 也 也 更 更 ， 而 死 人 ， 死 人 的 數 也 也 更 在 在 在 的\n",
      "， 但 ， 我 想 最 重 要 的 還 是 能 於 培 養 出 一 群 傑 出 的 資 源 ， 能 於 為 國 家 爭 榮 、 願 意 貢 獻 一 份 力 量 的 人 才 。 但 但 但 但 但 而 但 所 但 但 但 ， 而 但 而 但 但 但 我 ， ， 最 ， 的 是 是 能 是 培 夠 培 能 能 有 有 有 於 、 ， 的 是 是 能 能 繁 繁 、 、 願 、 、 、 一 的 的 人 人 的 人 而 所 」 所 但 ， 我\n",
      "， 也 因 為 人 民 生 得 越 多 ， 人 口 大 幅 度 增 加 ， 產 生 了 不 少 社 會 問 題 ， ， ， ， ， ， 也 也 不 不 的 社 ， ， ， ， ， ， ， 也 也 也 ， 不 ， ， ， ， ， 也 也 度 度 ， ， ， 生 生 得 越 多 ， 而 就 也 度 度 度 度 ， ， 也 也 變 得 越 ， ， 的 也 也 度 度 度 增 ， ， 也 也 了 不 的 的 。 ， ， ， ， ， ， ， ，\n",
      "， 人 市 場 已 經 飽 和 ， 現 在 「 少 子 化 」 商 機 才 是 引 人 注 目 。 能 讓 企 業 們 轉 移 注 目 ， 「 少 子 化 」 可 不 是 小 問 題 喔 ！ 而 ！ 「 ！ 可 可 可 就 是 個 的 個 的 ， 人 已 是 飽 了 ， ， ， 的 的 的 化 的 商 的 才 更 最 最 最 的 的 ， ， 能 讓 企 更 轉 的 注 呢 ， 「 這 化 也 可 可 可 不 個 題 喔 喔 喔 ！ ！\n",
      "。 業 主 突 然 發 現 現 在 的 青 少 年 很 會 花 錢 。 他 們 去 購 物 時 下 手 闊 绰 而 不 會 像 成 人 愛 討 價 還 價 。 他 們 業 主 們 主 主 主 主 們 們 。 業 們 業 們 他 主 們 主 主 主 們 們 們 們 ， ， 的 青 的 們 也 也 很 錢 ， 們 在 在 們 們 ， ， 的 青 很 快 ， ， ， 們 們 們 。 。 。 。 。 。 快 绰 快 ， 快 。 主 們 們 們\n",
      "， 三 ， 現 在 青 少 年 有 著 多 元 化 學 習 的 機 會 ， 出 國 旅 行 對 他 們 來 說 也 不 成 大 問 題 了 ， 所 以 青 少 年 所 領 取 的 知 識 比 父 母 上 一 代 豐 富 極 了 ！ 四 四 四 四 ， ， 青 在 年 有 有 多 多 的 的 的 的 的 ， ， ， ， 出 ， 也 也 青 青 青 的 不 不 不 是 大 ， 青 青 青 青 青 青 青 所 所 所 所 的 ， 要 比 比\n",
      "生 的 意 識 方 面 上 ， 為 了 減 少 急 劇 的 的 生 孩 子 率 ， 需 要 呼 籲 適 當 的 生 育 政 策 。 在 在 在 在 在 在 在 在 在 性 在 性 。 在 在 在 在 在 在 在 在 在 在 在 急 劇 高 在 在 在 ， ， 上 ， 要 了 減 急 急 劇 劇 劇 生 再 的 率 ， ， ， ， 要 要 急 急 急 劇 劇 的 的 生 。 。 。 。 ， 在 更 急 急 急 劇 性 。 。 。 。 的\n",
      "的 這 樣 的 情 況 到 底 該 稱 之 為 「 社 會 危 機 」 ， 還 是 它 能 為 全 人 類 帶 來 什 麼 樣 的 好 處 嗎 ？ 或 它 它 又 是 又 ？ 又 ？ ？ ？ ？ ？ 它 它 它 它 它 能 它 是 這 的 情 的 情 情 情 情 應 該 該 之 它 是 是 大 的 機 呢 ， ？ ？ ？ 是 它 該 該 它 能 有 是 的 的 的 呢 呢 ？ 又 ？ ？ ？ ？ 能 ？ ？ ？ 有 ？ ？ 的 情 情\n",
      "， 國 人 自 古 至 今 有 一 個 刻 板 觀 念 ， 認 為 生 育 男 性 才 有 未 來 ， 女 孩 嫁 出 去 就 是 「 潑 出 去 的 水 」 ， 沒 有 作 用 ， 更 無 法 傳 宗 接 代 。 因 這 這 這 此 中 也 都 自 一 都 都 都 刻 板 刻 板 刻 板 的 薄 即 要 為 要 不 不 接 的 傳 板 刻 板 男 嫁 嫁 ， 是 潑 潑 潑 潑 潑 水 水 水 並 更 沒 有 ， ， 無 更 接 法\n",
      "人 １９９０ 愛 莎 妮 亞 人 口 超 過 １５０ 萬 人 ， 但 是 現 在 我 們 的 人 口 不 到 １３５ 萬 人 。 而 我 在 我 們 們 在 在 我 在 在 在 在 在 在 在 在 我 們 國 曾 們 有 有 在 在 在 年 年 年 年 年 的 亞 的 們 已 有 約 過 １４ ， 。 ， 在 我 年 年 的 國 的 們 已 有 有 過 。 而 。 而 而 們 ， 們 們 們 們 們 們 們 。 。 在 在 ， 年\n",
      "幣 們 獨 立 之 後 的 經 濟 情 況 很 差 ， 那 時 候 我 們 還 繼 續 使 用 俄 羅 斯 的 貨 幣 ， 必 須 面 對 通 貨 膨 脹 的 問 題 。 我 我 我 我 我 我 我 ， ， 和 的 也 的 ， 我 在 我 在 在 在 在 的 的 的 的 也 況 很 差 ， ， 候 候 候 候 候 候 還 在 的 的 也 也 也 ， ， ， 而 還 要 要 對 通 對 和 的 的 的 。 [UNK] 而 我 而 候 候 之 候\n",
      "人 口 越 少 ， 管 理 整 個 國 家 越 複 雜 ， 哪 裡 都 缺 人 才 ， 甚 至 於 我 們 政 府 得 出 移 民 法 律 ， 讓 外 籍 人 移 民 到 愛 沙 尼 亞 。 口 而 口 而 而 以 ， 到 人 口 人 人 人 口 人 ， ， 得 整 理 的 就 就 就 越 ， ， 裡 都 缺 缺 缺 ， ， ， ， ， 就 ， 不 還 不 有 有 出 的 的 ， ， 讓 讓 不 民 來 民 到 愛 亞 。 而 而 而\n",
      "空 「 子 化 」 也 有 一 些 好 處 ， 比 方 說 ， 我 們 就 有 空 間 的 問 題 了 ， 房 子 、 院 子 都 越 來 越 大 ， 人 民 不 會 碰 到 因 空 間 不 夠 而 造 成 衝 突 這 種 問 題 。 「 「 「 「 「 「 「 是 有 ， 處 ， 比 ， 說 「 現 是 不 有 不 有 有 有 的 的 題 了 」 現 現 說 會 就 都 都 是 ， ， 就 就 會 不 會 碰 到 因 不 大 而 ，\n",
      "， 回 到 早 期 的 社 會 ， 直 到 現 今 ， 人 類 將 隨 著 各 方 面 的 進 步 而 不 斷 產 生 「 少 子 化 」 的 現 象 衍 生 。 直 直 直 直 直 直 直 直 而 的 這 產 直 直 直 直 回 回 回 回 溯 回 最 的 人 會 ， ， 到 到 再 ， ， 都 將 將 將 各 著 各 的 的 而 ， ， ， 直 而 直 ， 、 的 的 的 的 的 產 而 產 而 而 而 而 直 。 將 再 更 人\n",
      "家 許 多 已 開 發 國 家 將 工 廠 移 轉 到 人 口 較 多 的 國 家 ， 給 開 發 中 國 家 提 供 就 業 的 來 源 。 這 在 已 在 在 也 在 這 在 在 在 和 在 在 已 已 已 在 的 也 也 也 供 已 已 已 已 開 中 國 也 的 已 廠 都 轉 到 人 其 較 較 較 的 的 ， 的 的 的 的 業 也 也 提 提 新 新 更 的 的 。 在 在 在 在 也 也 提 提 的 供 和 。 些 已\n",
      "的 使 再 多 的 人 口 ， 但 如 果 懵 懂 無 知 的 話 不 但 對 國 家 也 毫 無 助 益 ， 甚 至 在 一 個 極 端 派 的 煽 動 之 下 很 容 易 成 為 反 社 會 主 義 者 。 即 即 即 即 懵 懵 即 即 用 有 用 有 也 ， 卻 懵 懵 懵 懵 懵 懂 的 話 ， 話 對 也 的 也 也 也 懵 懵 懵 懵 懵 懵 ， 在 一 個 的 派 的 的 ， ， ， 就 就 就 一 反 一 反 反 的\n",
      "的 少 子 化 」 正 面 的 影 響 可 以 說 是 比 較 少 ， 但 是 最 主 要 就 是 可 以 減 少 許 多 的 經 費 及 資 源 ， 也 可 改 善 環 境 的 汙 染 。 「 「 「 「 它 「 「 可 「 「 「 比 「 「 化 的 的 的 的 的 也 可 可 是 是 比 比 少 少 少 它 它 其 它 也 也 也 也 是 是 能 有 人 人 的 的 及 ， ， 也 可 可 可 以 少 及 質 質 質 少 「 化\n",
      "的 負 面 的 影 響 最 主 要 的 就 是 人 口 數 的 減 少 會 使 人 們 的 工 作 量 增 加 、 所 要 背 負 的 負 擔 加 重 及 人 口 老 年 化 等 的 問 題 。 等 等 而 而 而 如 而 的 的 的 的 的 它 的 ， ， 最 ， 就 的 就 是 ， 口 數 量 數 會 會 會 會 口 會 是 是 是 是 是 的 的 的 所 所 所 的 所 會 增 、 、 及 的 的 的 等 化 等 等 等 而 的\n",
      "， 代 的 改 變 ， 慢 慢 將 人 類 的 觀 念 更 新 以 及 改 革 ， 人 們 早 已 不 像 從 前 那 樣 ， 不 懂 得 如 何 避 孕 ， 然 後 意 外 的 懷 了 寶 寶 。 而 而 而 而 就 的 時 時 更 更 新 時 時 ， ， 時 也 將 人 類 的 觀 更 更 更 更 新 更 改 時 ， ， 時 也 的 時 的 更 更 更 ， ， 會 ， ， 會 去 而 ， ， 就 就 就 的 的 的 懷 上 上 時\n",
      "生 國 政 府 認 為 生 太 多 子 女 會 嚴 重 地 危 害 國 家 的 經 濟 成 長 ， 且 降 低 國 民 的 生 活 品 質 ， 因 此 提 倡 「 少 生 孩 子 」 。 美 而 美 而 而 而 而 要 要 「 少 、 中 而 國 並 因 為 生 太 多 子 將 將 會 的 地 地 地 地 於 會 會 的 和 會 子 會 會 要 少 要 少 的 的 ， ， 並 並 並 也 要 要 要 要 少 生 子 。 美 而 美 也\n",
      "， 此 ， 若 韓 國 政 府 未 準 備 好 能 夠 因 應 「 少 子 化 」 現 象 的 政 策 ， 將 會 嚴 重 地 危 害 韓 國 政 府 的 財 政 支 出 ， 甚 至 導 致 國 家 破 產 。 因 因 因 因 因 ， ， ， 如 若 如 國 未 未 未 有 備 好 好 好 適 夠 應 少 其 少 子 化 的 未 未 未 有 好 能 好 會 地 地 地 的 到 國 政 的 的 的 支 ， ， ， ， 會 會 會 此 於\n",
      "的 口 口 成 長 確 實 會 造 成 不 少 困 難 ， 經 濟 衰 退 是 其 嚴 重 的 結 果 。 口 口 口 口 口 口 口 口 也 是 這 口 口 口 口 口 口 口 口 的 口 的 也 也 也 ， 口 口 口 口 口 口 口 口 口 的 口 也 實 會 造 了 不 不 口 的 ， 而 濟 衰 的 衰 退 是 最 也 非 了 不 不 口 ， 而 口 衰 口 衰 退 是 最 最 的 。 經 。 。 。 而 口 口 的 口 的\n",
      "的 反 觀 想 ， 人 口 減 少 或 許 也 代 表 這 個 國 家 不 會 如 人 口 過 多 的 國 家 混 亂 ， 國 家 政 府 更 不 用 為 那 麼 多 人 操 心 。 而 而 而 而 而 而 而 ， ， 這 ， ， ， ， ， ， 想 ， ， ， 減 少 ， 許 就 就 著 這 著 國 並 不 不 不 像 像 在 少 ， ， 就 混 混 混 ， ， 和 府 也 也 不 再 為 為 那 人 多 而 。 而 而 而 一 看\n",
      "的 於 開 發 中 國 家 的 人 ， 政 府 可 以 試 試 教 育 他 們 人 口 太 多 的 風 險 及 後 果 ， 但 我 覺 得 針 對 這 個 問 題 ， 要 以 政 策 來 解 決 較 於 適 當 及 簡 單 。 對 對 對 於 對 發 中 於 中 的 人 ， 我 也 也 要 要 試 教 去 ， 有 太 們 太 有 太 的 我 我 我 要 要 我 我 我 我 ， ， ， 個 個 個 ， ， ， 要 要 來 來 會 較 較\n",
      "的 如 ： 每 個 家 庭 只 限 有 兩 個 小 孩 、 或 超 過 三 個 孩 子 要 繳 付 更 多 稅 等 等 。 這 樣 的 法 律 才 能 直 接 並 且 有 效 地 將 這 嚴 重 的 問 題 舒 緩 。 並 而 而 而 ， ， ： ， ： 的 只 只 能 擁 一 有 一 、 、 、 ， ： 兩 的 的 的 要 要 要 要 有 的 稅 稅 ， ， ， ， ， ， 才 才 能 能 地 能 地 能 地 地 地 將 這 的 ，\n",
      "增 合 國 報 告 表 示 全 球 人 口 一 直 不 斷 的 增 加 ， 但 是 大 量 的 新 增 人 口 來 自 於 開 發 中 國 家 。 而 而 在 在 聯 這 聯 在 聯 的 和 的 和 而 在 在 聯 在 在 一 斷 不 斷 不 發 的 國 表 示 全 球 一 在 在 在 在 斷 在 斷 的 的 ， ， ， 且 在 球 的 在 在 在 在 斷 於 展 國 國 國 。 。 而 在 在 。 。 在 不 斷 。 。 中 國 國\n",
      "， 來 ， 在 電 視 新 聞 或 報 導 我 們 常 常 看 到 很 多 婚 姻 問 題 。 比 如 說 ， 老 公 打 老 婆 、 離 婚 ， 等 等 。 等 ， 而 ， 而 ， ， ， ， ， ， ， 等 。 ， 導 ， 導 ， ， ， ， 在 ， 、 或 紙 導 報 導 導 導 中 導 會 會 ， ， ， ， 的 或 報 導 導 導 導 導 ， ， ， ， ， ， ， ， ， 等 。 而 。 。 。 ， ， ， 。 ， ， 在\n",
      "也 如 說 ， 發 生 戰 爭 如 果 很 少 人 也 無 法 打 贏 吧 ， 因 為 武 器 多 ， 進 也 好 也 要 有 人 去 控 制 。 再 再 再 再 再 再 再 再 比 。 再 再 比 再 比 比 ， ， 人 ， ， ， ， ， ， ， 假 ， ， ， ， ， 很 打 人 人 ， ， 打 ， ， ， 比 ， ， 進 進 進 很 進 人 ， ， ， ， 進 。 。 。 。 。 。 。 ， 。 。 。 。 。 。 。 比 ，\n",
      "的 面 的 話 就 是 人 人 都 有 工 作 不 用 怕 沒 錢 賺 ， 這 樣 難 民 就 會 少 許 多 。 反 後 面 對 面 ， 移 貧 民 也 也 好 。 再 後 後 對 面 對 面 ， 移 也 也 也 是 。 ， ， ， 對 面 的 話 ， 是 ， 人 都 有 ， ， ， 不 怕 怕 錢 窮 的 難 災 難 難 難 都 都 ， ， ， ， 怕 怕 ， 錢 ， ， 難 窮 難 窮 難 窮 。 的 。 。 。 的 面 的 ，\n",
      "， 果 市 場 做 適 當 處 理 ， 把 產 品 減 少 ， 這 樣 不 會 發 生 有 工 作 可 是 沒 人 做 的 事 情 。 如 如 如 如 如 如 如 如 如 如 如 如 如 如 如 如 如 如 如 如 如 如 如 如 做 如 如 如 要 做 要 要 ， 再 就 把 把 就 也 減 減 如 就 就 就 就 會 要 ， ， ， 讓 把 再 做 人 做 如 如 如 如 如 如 。 。 。 。 。 也 做 做 。 如 。 要 要\n",
      "， 先 是 工 作 態 度 要 表 現 到 最 佳 狀 態 ， 以 好 受 到 上 司 的 肯 定 及 信 任 。 其 其 再 其 ， ， 要 要 及 及 及 及 。 再 。 其 其 再 首 ， ， 的 要 要 要 要 表 ， ， 。 ， 再 ， ， 是 及 要 要 要 要 到 到 最 的 ， ， 以 便 以 到 得 及 及 要 要 要 到 到 好 的 。 再 。 以 便 得 到 及 的 的 及 及 讚 及 。 。 。 最 再 是 ，\n",
      "你 還 記 得 你 當 時 說 你 父 親 生 病 了 ， 他 身 體 有 沒 有 好 一 點 ， 請 你 跟 他 代 替 我 跟 他 說 「 早 點 好 ！ 」 。 」 我 我 我 我 當 當 當 現 ， 好 ， 」 我 我 我 我 我 我 當 當 有 當 當 當 當 當 親 說 說 說 說 問 說 說 說 說 說 當 當 當 說 說 親 說 代 代 代 代 替 替 ， 說 你 要 好 ！ 好 。 」 。 我 我 。 說 跟 你 當\n",
      "你 的 話 ， 很 好 因 為 我 那 個 時 候 還 沒 有 放 假 ， 我 可 以 好 好 的 帶 你 去 台 北 很 多 好 玩 跟 有 名 的 地 方 參 觀 參 觀 。 很 很 很 很 很 很 很 ， 的 ， 我 ， 我 ， ， ， 很 ， 很 很 ， ， ， ， 的 那 還 還 還 還 很 ， 很 我 我 我 的 的 我 我 的 我 我 ， 我 很 跟 很 很 跟 很 跟 有 的 跟 跟 跟 跟 。 」 」 對 很 很 ，\n",
      "。 話 不 說 ， 下 個 月 見 。 再 ） ） [UNK] 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 ， ， ， ， ， ， 再 。 。 」 」 。 。 。 。 。 。 ， ， ， ， ， 再 。 。 。 [UNK] 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 ， ， ，\n",
      "吃 們 一 邊 吃 一 邊 說 話 聊 天 。 文 平 和 小 王 從 十 二 點 吃 飯 和 說 話 說 到 三 點 以 後 都 還 沒 說 完 。 他 他 他 他 他 最 最 最 最 都 都 都 他 他 他 他 他 他 他 他 他 他 就 吃 吃 吃 飯 ， 在 在 。 他 而 小 王 和 和 從 從 從 從 要 之 在 在 ， 。 他 。 之 之 之 之 之 最 都 都 之 ， 。 。 小 ， 到 到 ， 之 之 之 也 就 都\n",
      "你 們 在 網 上 認 識 已 經 很 久 了 ， 雖 然 我 們 從 來 沒 見 過 面 但 對 我 來 說 你 是 我 很 熟 的 朋 友 。 你 你 你 你 你 你 你 你 你 你 你 你 你 你 我 你 我 你 我 我 我 我 我 網 網 網 網 網 也 網 也 很 久 了 ， 但 你 你 也 也 也 都 有 只 ， ， ， 很 ， 了 你 ， 你 是 很 我 很 很 。 ， 你 。 你 ， ， ， ， 你 是 是 網 網 網\n",
      "。 果 你 答 應 我 的 計 畫 還 是 有 什 麼 問 題 請 你 告 訴 我 。 我 們 可 以 改 時 間 。 如 但 如 。 。 。 。 。 你 不 什 改 。 如 。 如 。 如 。 。 。 如 。 如 。 什 什 什 這 如 如 如 你 了 。 這 。 。 。 是 這 什 什 什 。 。 。 。 。 。 。 。 。 。 。 什 這 什 什 什 。 。 。 。 。 。 。 。 。 。 。 。 什 。 。 。 如 。 你 我\n",
      "。 會 穿 紅 色 的 衣 服 ， 牛 仔 褲 跟 黃 色 的 帽 子 。 我 也 戴 黑 色 的 眼 鏡 。 我 我 我 我 我 。 。 我 有 黑 鏡 鏡 。 鏡 我 我 我 我 。 我 我 會 。 我 。 。 鏡 鏡 我 我 我 我 我 穿 紅 色 的 ， ， ， ， 仔 ， 跟 跟 跟 ， ， 。 。 。 我 ， ， ， ， ， 跟 跟 跟 跟 跟 。 。 。 。 。 。 。 。 。 。 鏡 。 。 。 。 。 。 。 穿 紅\n",
      "我 們 要 見 面 的 那 天 快 就 要 到 了 ， 所 以 我 這 一 次 寫 信 給 你 是 想 跟 妳 安 排 一 下 關 於 我 們 要 見 面 的 事 。 」 我 我 我 於 關 於 我 於 我 ， 的 事 我 我 我 我 我 我 我 ， 我 ， 也 也 快 快 快 到 ， ， 我 我 我 我 我 我 寫 要 寫 ， ， ， 也 。 ， 妳 關 ， 關 關 於 於 於 我 們 的 的 的 。 。 我 我 我 我 我 我 我\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "。 天 我 們 兩 個 決 定 要 見 面 的 一 天 ， 我 們 那 時 候 每 天 都 在 想 這 件 事 。 我 們 在 們 那 那 那 我 我 我 。 那 那 那 那 們 們 們 們 是 那 是 我 是 。 那 那 的 那 那 那 是 是 是 是 們 決 們 要 要 的 的 的 的 ， 們 們 們 們 的 我 我 就 要 要 的 的 的 的 那 們 們 們 們 們 們 我 們 。 。 。 。 。 。 。 們 。 們 們 那 們\n",
      "！ 知 道 一 家 又 好 吃 又 便 宜 的 泰 國 餐 廳 ， 我 們 就 去 這 裡 吃 飯 吧 ！ 這 ~ ~ ~ 這 就 這 那 那 那 裡 ！ ！ ！ ！ ！ ！ ！ ！ ！ 就 就 去 那 裡 ！ ！ ！ ！ ！ 的 的 ， ， 有 有 是 又 有 又 又 又 的 的 的 的 的 的 ， 那 那 這 去 這 這 這 又 的 的 的 的 的 的 ， ！ ， 那 去 那 那 那 裡 ！ ！ ！ ！ ！ ！ ！ ！ ！ 到 這\n",
      "的 家 餐 廳 就 在 中 山 捷 運 站 附 近 ， 所 以 我 們 就 約 在 捷 運 站 五 號 出 口 好 了 。 就 我 ▼ 就 ▼ 在 這 我 的 。 好 。 好 這 這 這 因 就 是 是 是 是 的 很 附 附 附 ， 這 這 因 也 就 在 離 離 的 附 的 附 附 附 好 ， 大 就 就 約 約 約 在 在 的 附 附 附 附 附 。 。 。 就 就 在 約 在 在 的 的 的 附 好 。 。 。 這 ▼ 就 就\n",
      "吃 然 我 們 離 士 林 很 近 ， 我 想 我 們 吃 完 飯 以 後 再 去 那 裡 逛 一 逛 ， 因 為 那 裡 的 小 吃 不 但 非 常 多 ， 又 好 吃 ！ 那 那 那 那 也 也 是 又 又 好 又 而 而 雖 那 雖 ， 那 家 家 都 也 也 也 ， 我 我 我 想 想 在 再 再 再 再 再 再 再 也 再 ， 我 那 那 那 那 那 那 那 ， ， 是 是 多 好 又 好 又 好 而 那 」 雖 在 在 家\n",
      "色 覺 得 我 們 那 一 天 都 穿 著 紅 色 的 衣 服 ， 跟 白 色 的 褲 子 ， 好 了 頭 上 戴 一 頂 藍 色 的 帽 子 ， 這 樣 我 想 一 定 不 會 認 錯 人 了 ！ 我 我 我 我 我 我 我 著 ， ， ， 們 我 們 那 們 都 都 都 著 著 著 的 的 的 ， ， ， 跟 ， ， 那 好 ， ， 著 著 著 有 戴 戴 戴 人 ！ ！ 好 ， ， 我 我 我 就 我 就 再 再 人 了 ！ 我\n",
      "的 果 我 家 的 狗 還 沒 把 我 的 天 藍 色 襯 衫 咬 破 的 話 ， 我 會 穿 著 白 色 的 褲 子 在 商 品 外 等 你 抵 達 。 如 如 如 如 如 如 如 如 如 如 如 我 我 如 如 如 如 如 如 如 如 如 如 你 的 狗 還 還 穿 穿 的 天 的 天 衣 的 咬 ， 如 如 ， 狗 ， 會 穿 穿 的 天 的 白 的 ， ， ， 。 。 。 。 。 。 。 如 。 。 。 。 。 如 ， 我 你\n",
      "票 為 您 還 沒 有 看 過 １０１ ， 我 會 先 幫 我 們 買 去 樓 頂 的 票 ， 看 看 風 景 。 再 因 因 因 您 ， 因 ， ， 。 因 再 因 因 因 因 因 因 您 您 您 ， １０ ， ， ， 因 因 因 因 因 您 您 還 您 去 看 過 １０ ， ， 我 會 先 幫 先 先 先 去 到 到 ， ， 到 １０ ， ， 我 先 先 先 先 您 先 去 到 的 ， ， ， ， ， ， 先 先 。 。 。 。 您 先\n",
      "。 現 在 打 算 說 我 們 這 個 禮 拜 六 早 上 十 點 鐘 在 台 北 市 政 府 的 捷 運 站 三 號 出 口 見 面 。 我 說 我 說 說 說 我 。 。 。 。 。 ， ， ， 說 我 說 是 在 是 是 是 是 是 是 我 說 是 說 說 在 是 在 下 在 在 ， 的 的 上 ， ， 在 在 在 在 在 是 在 下 在 在 ， ， ， ， ， ， 在 在 在 府 府 府 府 府 的 的 的 。 。 ， ， 說\n",
      "的 那 裡 ， 我 們 可 以 走 到 那 附 近 的 新 光 三 寶 百 貨 公 司 逛 一 逛 ， 那 裡 有 很 多 非 常 漂 亮 的 衣 服 和 作 品 。 在 在 在 在 在 還 在 的 的 的 店 ， 在 在 在 在 在 的 的 三 ， 還 和 再 再 走 到 到 到 到 的 的 的 三 寶 三 寶 百 寶 去 去 去 去 到 到 新 的 的 的 多 三 的 的 的 的 的 和 。 。 在 在 在 在 在 三 寶 三 ，\n",
      "！ 吧 ！ 就 這 樣 ！ 我 等 不 住 了 ， 希 望 禮 拜 六 快 一 點 來 ， 就 那 會 兒 見 吧 ！ 再 ！ 再 再 ！ ！ ！ ， 到 見 見 見 ！ 再 ！ 我 我 我 ！ ！ ！ ！ ！ 到 到 兒 見 到 ！ [UNK] ！ 好 ！ 好 ！ ！ 我 住 久 住 住 住 我 ， 我 我 再 再 再 再 見 ！ ！ 住 到 到 到 到 ， ！ 我 ！ 能 能 能 點 點 ， 見 到 到 再 兒 ！ ！ ！ ！ ！ 是 就\n",
      "郵 收 到 我 送 給 妳 的 禮 物 嗎 ？ 我 希 望 沒 被 郵 局 掉 了 。 我 地 址 當 然 寫 對 了 ， 可 是 我 不 知 道 如 果 我 郵 票 放 夠 了 。 我 我 我 想 我 我 把 我 我 我 放 好 。 我 送 送 寄 送 送 寄 寄 的 。 ？ 。 不 不 夠 我 有 我 被 給 寄 寄 我 的 的 。 。 不 是 寫 對 。 是 我 我 我 我 我 ， 我 把 我 放 也 放 夠 。 。 我 寄 寄\n",
      "我 很 高 興 聽 到 你 要 這 個 週 末 要 來 看 我 ， 我 安 排 很 多 很 好 玩 的 活 動 。 我 我 我 我 我 你 你 你 你 週 週 ， 週 我 我 [UNK] [UNK] 我 我 我 你 ， 你 在 週 週 週 末 週 ， ， ， ， 我 我 說 我 說 週 週 週 末 週 末 來 看 ， 給 給 有 了 你 你 在 週 週 週 週 週 末 來 ， 。 給 給 有 有 了 有 週 週 週 ， ， 。 。 [UNK] [UNK] 我 我 ，\n",
      "。 很 期 待 跟 你 一 起 出 去 玩 。 期 待 得 我 不 睡 覺 。 當 然 我 會 一 定 開 心 。 覺 我 很 。 。 。 。 。 不 很 很 很 很 覺 我 待 很 。 。 。 。 。 。 。 。 很 。 待 我 待 待 待 你 待 得 你 跟 你 。 。 玩 。 期 待 待 待 得 能 能 。 。 。 。 。 。 。 。 待 很 待 待 待 。 能 。 。 。 。 。 。 待 。 待 待 待 。 待 。 。 。 待\n",
      "。 在 八 月 十 三 號 下 午 三 點 在 名 古 屋 站 一 號 出 口 見 面 。 在 在 在 在 在 在 在 1 的 1 。 。 。 。 。 。 。 。 。 在 。 。 1 。 1 。 。 。 。 。 。 。 於 於 於 的 一 的 在 於 的 的 的 的 於 於 三 於 名 名 一 一 一 一 一 一 一 的 的 於 於 於 於 於 jr jr 一 一 一 一 一 一 的 。 。 。 。 。 。 。 。 。 。 。 。 於 於\n",
      "的 我 來 說 ， 能 找 到 那 麼 好 的 朋 友 ， 特 別 是 韓 國 ， 是 一 件 很 幸 福 的 事 。 對 對 對 我 這 我 ， 這 我 很 的 的 的 對 對 對 對 對 ， ， 我 我 我 很 好 好 的 人 ， 對 我 ， ， ， 能 ， 能 能 我 好 好 好 朋 ， ， ， ， ， ， ， ， ， 會 我 很 好 的 的 的 ， 這 這 這 ， ， ， ， ， ， 會 很 的 的 的 。 [UNK] 對 我 ， 我\n",
      "， 那 時 的 生 活 過 的 很 差 ， 真 的 運 氣 不 好 。 我 一 月 出 了 車 禍 （ 被 父 母 罵 ） ， 二 月 的 成 績 退 步 了 ， 還 有 三 月 跟 愛 人 分 手 了 。 我 我 我 我 在 的 是 的 我 我 我 我 我 也 也 很 ， 的 是 的 是 的 是 的 。 我 在 的 的 了 出 了 車 的 是 被 的 被 ， ， 三 的 的 的 就 也 也 了 ， ， 還 在 我 就 跟 的 也 。\n",
      "。 是 我 要 感 謝 我 哥 一 直 都 沒 有 放 棄 ， 一 直 在 我 的 身 邊 。 他 他 他 也 但 。 但 。 我 。 。 。 。 我 。 我 。 我 。 。 但 但 。 。 。 。 。 。 。 。 ， ， ， ， 。 。 ， 我 要 ， ， 我 我 ， ， 都 都 的 的 的 ， 他 他 陪 我 身 ， 我 ， ， ， 都 都 的 的 ， 。 。 他 陪 在 身 身 。 。 。 。 。 。 。 。 。 但 。 ， 要\n",
      "人 日 常 生 活 裡 人 們 會 不 會 受 到 別 人 的 幫 助 。 在 在 在 也 不 也 也 會 在 在 在 在 在 在 在 在 在 也 不 不 常 在 在 。 。 。 。 。 。 在 。 在 在 也 常 不 常 在 在 在 在 在 在 中 ， ， ， 不 不 不 會 常 常 到 到 在 的 。 在 在 在 ， 中 不 不 不 常 常 到 到 到 的 的 ？ 。 。 。 。 。 。 在 。 。 。 。 。 。 。 在 在 中\n",
      "噴 有 我 打 噴 嚏 的 時 候 媽 媽 會 勸 導 我 不 要 大 聲 打 噴 嚏 因 為 會 讓 別 人 覺 得 你 很 不 禮 貌 。 從 媽 媽 的 教 養 以 後 我 下 一 次 打 噴 嚏 的 聲 音 會 小 聲 一 點 。 而 我 我 我 嚏 嚏 嚏 ， ， 也 也 教 教 說 我 要 再 再 的 噴 嚏 嚏 嚏 我 會 再 再 一 說 說 我 再 再 的 噴 嚏 。 我 的 這 的 以 導 以 ， 我 再 打 嚏 嚏\n",
      "。 要 對 媽 媽 很 尊 敬 。 我 會 說 「 媽 媽 我 愛 你 」 。 我 永 遠 不 會 忘 記 你 所 帶 給 我 的 恩 。 我 遠 遠 遠 遠 。 想 。 ， 愛 的 。 。 我 。 我 遠 我 遠 要 。 。 要 要 。 我 的 。 。 。 。 。 。 我 會 會 ： ， ， ， 愛 愛 。 。 。 。 。 。 遠 。 會 會 ， ， ， ， 。 愛 。 。 。 遠 。 遠 。 遠 。 。 。 。 。 。 。 。 。\n",
      "的 果 沒 有 母 親 哪 裡 會 有 我 ， 她 辛 辛 苦 苦 的 生 下 我 ， 所 以 母 親 的 恩 是 我 一 生 都 回 不 完 的 。 如 如 如 如 如 如 我 如 我 回 。 的 我 如 若 如 如 如 如 如 我 如 果 ， ， ， ， ， ， 會 有 我 呢 她 是 是 是 的 的 的 的 我 ， ， ， ， ， 我 恩 的 恩 一 生 子 子 都 都 的 啊 」 如 。 如 。 。 。 。 。 。 我 如 ，\n",
      "的 母 親 是 一 個 很 嚴 格 的 人 ， 有 時 候 讓 我 很 看 不 懂 她 的 想 法 ， 總 是 逼 我 去 做 我 不 想 做 的 事 ， 長 大 了 才 了 解 她 所 做 的 一 切 是 為 我 好 的 。 我 我 我 而 她 是 很 是 很 她 很 的 她 她 她 她 會 會 看 很 看 楚 看 她 懂 她 我 好 我 她 她 讓 她 她 她 她 她 她 她 她 ， 我 我 我 才 我 才 她 她 她 所 她 都\n",
      "母 月 十 二 號 是 泰 國 的 母 親 節 ， 今 年 沒 有 你 慶 祝 但 在 這 裡 您 身 體 健 康 要 多 保 重 。 另 另 在 在 年 ， 在 在 在 你 。 意 。 。 在 在 在 今 是 的 的 的 。 。 。 八 ， 。 日 也 是 是 是 的 的 親 節 ， 我 我 我 有 有 有 ， ， ， ， 年 的 的 。 。 。 ， 我 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。\n",
      "我 且 我 的 中 文 沒 有 那 麼 流 利 ， 每 次 跟 別 人 說 都 有 錯 誤 。 而 而 而 而 而 而 而 。 都 都 都 都 都 。 而 而 。 而 而 而 而 而 。 我 都 我 。 而 。 而 而 而 。 而 而 而 且 而 的 中 也 也 還 那 很 好 好 ， 我 我 都 都 都 都 都 都 都 也 也 很 很 好 好 。 。 而 我 我 都 都 都 都 都 。 。 。 。 。 。 。 。 。 。 。 我 的\n",
      "。 我 的 生 活 中 ， 有 很 多 人 有 幫 助 我 。 但 是 除 了 我 的 父 母 以 外 我 最 感 謝 的 人 是 我 的 五 年 級 老 師 。 她 她 她 她 她 我 我 我 級 級 級 級 級 她 她 她 我 我 我 。 。 我 ， 我 ， 有 我 人 人 在 人 我 我 我 ， ， ， ， 我 我 我 我 ， 。 我 ， 我 ， 。 。 。 我 五 趙 級 級 級 級 老 她 。 我 。 。 。 。 。 我 我\n",
      "的 為 我 要 離 開 我 全 部 的 朋 友 ， 我 也 有 一 定 怕 ， 因 為 我 要 去 一 個 不 認 識 的 地 方 ， 真 的 是 人 生 地 不 熟 。 因 因 因 因 因 我 我 我 我 我 熟 熟 熟 熟 熟 因 因 我 因 要 要 我 我 我 我 的 的 ， ， 我 也 也 一 點 點 害 ， ， 我 我 我 的 我 我 認 識 認 識 的 ， 我 真 真 真 是 地 不 熟 熟 熟 因 因 因 因 。 我 要\n",
      "她 的 作 文 進 步 了 很 多 因 為 她 幫 助 我 。 她 也 給 我 介 紹 更 多 的 書 ， 所 以 我 讀 書 的 能 力 進 步 很 多 了 。 我 我 我 我 我 我 我 也 也 也 也 。 。 我 我 我 我 我 我 我 我 我 我 我 ， 。 。 。 為 她 能 在 ， ， 她 也 也 我 了 了 更 了 的 。 為 她 也 ， 我 我 的 的 也 也 也 也 了 了 。 我 。 我 。 我 。 我 我 ， 我\n",
      "我 了 我 ， 我 媽 媽 花 很 多 時 間 在 我 身 上 ， 沒 有 真 的 讓 自 己 休 息 。 除 除 除 除 除 我 我 我 我 我 我 我 。 除 除 除 除 除 除 除 我 我 我 我 我 。 除 除 除 除 為 身 身 ， ， ， 我 我 是 也 很 了 時 陪 在 我 身 身 身 也 有 沒 我 我 是 也 了 了 時 陪 陪 陪 身 身 身 ， 也 有 我 的 我 我 我 。 。 。 我 身 。 。 。 我 ，\n",
      "的 然 我 還 是 大 學 生 ， 我 一 定 會 很 用 功 的 拿 好 成 績 ， 所 以 我 畢 業 的 時 候 可 以 找 到 好 的 工 作 。 當 當 當 當 當 當 當 當 雖 當 當 當 當 當 當 雖 雖 雖 雖 我 ， ， 雖 我 我 是 好 ， ， 但 一 會 會 很 努 力 的 的 的 考 去 好 如 當 當 我 我 會 會 很 的 的 的 的 去 我 。 如 」 如 [UNK] 如 [UNK] 我 。 。 。 雖 而 我 我\n",
      "， 到 好 的 工 作 我 就 可 以 賺 很 多 錢 ， 於 是 我 可 以 幫 我 的 媽 媽 買 好 的 東 西 ， 像 衣 服 ， 車 ， 房 屋 等 等 。 再 再 像 我 像 像 像 ， ， ， ， ， ， 。 有 有 找 找 到 做 了 ， 我 我 就 賺 賺 賺 賺 賺 賺 賺 我 是 我 我 我 幫 幫 幫 我 買 買 賺 賺 買 買 ， ， ， ， ， ， 錢 ， ， ， ， ， ， 。 我 ， 我 我 我 我 我\n",
      "。 最 感 謝 的 人 是 我 這 位 鄰 居 。 我 我 我 。 。 。 。 。 。 。 的 這 。 。 。 。 。 。 。 。 。 。 。 一 的 的 這 。 。 。 。 。 。 。 。 。 。 。 。 。 一 的 一 。 。 。 。 。 一 。 。 。 。 的 這 這 一 的 。 。 。 。 。 。 。 。 。 。 。 的 這 這 的 的 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。\n",
      "， 把 小 貓 抱 起 來 ， 趕 快 跑 出 去 到 馬 路 邊 求 救 ， 可 是 有 很 少 的 車 ， 有 的 都 不 會 停 下 來 幫 我 。 「 [UNK] 我 「 我 我 我 ， 想 ， 幫 幫 幫 救 我 我 跑 跑 跑 跑 跑 跑 跑 跑 跑 就 就 就 跑 跑 跑 跑 跑 跑 跑 到 上 去 去 ， 跑 ， 等 我 跑 跑 跑 跑 跑 跑 跑 車 車 來 ， 幫 幫 幫 救 [UNK] [UNK] 走 。 。 。 等 。 我 和 跑 貓\n",
      "貓 被 一 個 小 貓 看 不 起 是 什 麼 ， 可 是 這 個 小 貓 是 我 最 大 的 貓 ， 它 從 小 陪 我 ， 所 以 我 把 小 貓 當 孩 子 一 樣 。 對 對 對 對 對 小 我 小 我 ， ， ， 對 雖 對 雖 被 小 被 小 小 被 被 我 我 我 ， 。 雖 雖 對 有 被 小 是 小 被 我 我 我 我 我 ， 我 它 我 我 我 我 小 我 ， ， ， ， 。 看 。 看 我 可 對 然 對 小 被\n",
      "。 她 做 每 件 事 不 管 怎 麼 的 辛 苦 都 沒 有 抱 怨 。 她 默 默 的 做 到 完 成 。 媽 媽 媽 媽 。 。 的 的 的 的 的 的 她 她 媽 媽 媽 媽 。 媽 媽 的 的 的 的 的 的 的 。 。 。 。 。 。 每 做 事 都 都 都 的 的 的 的 都 都 。 她 。 。 她 能 的 都 都 的 的 的 的 。 。 。 。 。 。 。 。 她 的 的 的 的 的 。 。 。 。 。 。 。 做 每\n",
      "。 一 次 ， 我 被 車 子 撞 了 。 那 時 候 我 剛 剛 好 在 附 近 ， 所 以 她 很 快 就 趕 過 來 看 我 。 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 她 她 她 。 。 我 我 我 我 我 我 我 我 我 我 我 我 她 她 。 。 我 我 我 我 我 我 我 我 我 。 。 。 。 。 我 。 我 我 我 我 我 。 我 我\n",
      "她 們 真 的 不 太 像 普 通 的 表 姊 妹 ， 可 能 是 從 小 一 起 長 到 大 的 關 係 。 我 都 知 道 她 喜 歡 什 麼 ， 所 以 她 喜 歡 的 東 西 我 都 會 買 給 她 。 我 我 我 她 我 我 我 也 她 的 是 是 很 是 表 的 表 表 表 ， ， ， 是 是 是 是 是 生 玩 的 表 表 表 表 ， ， 她 她 她 她 她 她 她 她 她 她 她 她 她 喜 的 的 我 我 都 會 她\n",
      "你 戴 黃 色 的 帽 子 。 你 那 天 穿 什 麼 衣 服 ？ 拜 託 你 穿 容 易 找 到 的 衣 服 。 要 不 然 我 找 不 到 你 、 再 說 我 怕 我 們 浪 費 很 多 時 間 。 再 再 再 再 說 你 怕 穿 你 。 我 。 我 到 你 。 你 想 想 想 穿 穿 穿 ？ ？ 。 。 我 。 。 要 。 你 你 你 要 穿 ？ 。 。 我 就 到 到 到 到 到 、 再 說 你 怕 要 會 會 你 。 。 你\n",
      "我 禮 拜 六 晚 上 六 點 在 台 北 車 站 一 號 出 口 附 近 等 你 。 穿 白 襯 衫 ， 戴 眼 鏡 的 人 就 是 我 。 你 看 過 我 的 相 片 ， 所 以 一 定 會 找 到 的 。 我 是 我 在 在 ， ， ， ， ， ， ， ， ， ， ， 北 一 一 一 一 一 號 ， ， ， ， 。 我 在 在 在 在 一 一 大 眼 鏡 ， 。 。 。 。 我 是 是 。 。 。 。 。 。 我 我 。 。 。\n",
      "咖 「 我 們 可 以 去 咖 啡 座 椅 上 談 一 談 。 如 果 你 不 要 喝 咖 啡 的 話 ， 我 們 可 以 去 別 的 地 方 ， 隨 便 你 ， 你 要 做 什 麼 就 做 什 麼 。 然 」 然 然 我 上 談 ， 談 ， 然 ， 可 再 在 去 椅 上 椅 上 上 上 談 ， 。 ， 你 你 你 想 在 在 椅 上 上 上 談 上 你 去 去 其 別 玩 。 ， ， 隨 ， 你 ， 你 想 你 ， 做 做 。 。\n",
      "試 是 我 的 面 試 成 功 了 的 話 ， 我 們 下 禮 拜 我 可 以 見 面 。 我 會 告 訴 妳 我 的 面 試 是 什 麼 的 ， 而 且 幫 你 一 點 忙 推 薦 你 面 試 的 時 候 怎 麼 辦 。 我 我 我 如 如 我 怎 我 怎 怎 怎 ， ， ， ， 下 下 下 下 拜 就 拜 就 怎 怎 了 怎 ， ， 我 妳 妳 妳 我 我 是 是 是 怎 的 ， 我 會 會 幫 幫 一 幫 ， ， ， 我 的 怎\n",
      "， 們 可 以 下 禮 拜 六 早 上 八 點 在 大 安 森 林 公 園 見 面 ， 好 不 好 ？ 我 打 算 我 們 先 在 公 園 靜 靜 地 散 步 一 下 ， 然 後 去 旁 邊 的 咖 啡 館 喝 咖 啡 和 聊 天 。 我 我 我 我 在 在 在 在 ， 在 一 再 ， 我 在 大 安 公 的 見 ， ， ， ， 好 ？ ？ ？ 我 ， 讓 先 先 在 先 去 裡 散 散 步 散 步 步 步 ， 再 先 在 在 在 在\n",
      "我 了 讓 你 認 得 我 ， 我 會 穿 我 黑 皮 夾 克 ， 戴 藍 色 的 帽 子 。 我 也 會 帶 我 的 小 狗 去 ， 你 大 概 知 道 是 哪 一 隻 ， 牠 的 眼 睛 藍 藍 的 ， 圓 圓 的 。 我 我 我 ， 我 ， 我 ， ， 我 ， 我 我 穿 著 的 色 的 的 ， ， ， 我 我 ， ， ， 我 我 帶 帶 帶 帶 ， 去 去 ， ， ， ， 牠 會 牠 隻 隻 隻 隻 隻 ， 牠 的 會 會 我\n",
      "， 知 道 一 個 又 好 笑 又 怪 怪 的 地 方 ， 可 以 介 紹 你 。 那 家 看 起 來 不 好 看 ， 我 怕 看 得 讓 你 覺 得 不 舒 服 ， 但 是 很 好 玩 ， 他 們 的 菜 還 不 錯 ， 請 嚐 嚐 ！ 哦 ， ， ， ， 有 ， ， 又 怪 的 ， 請 來 我 以 來 ， ， ， ， ， 店 店 店 ， 的 來 ， ， 我 我 ， 看 看 我 覺 我 很 。 ， 但 但 但 但 很 。 ， ， 的\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "」 」 我 會 戴 綠 帽 子 ， 穿 粉 紅 色 襯 衫 ， 手 上 拿 「 王 宜 家 」 旗 子 ， 你 應 該 不 會 有 問 題 認 出 我 。 」 」 」 天 今 天 今 天 戴 戴 戴 來 」 」 」 」 今 天 戴 ， 今 戴 戴 戴 戴 戴 戴 色 戴 戴 粉 粉 紅 的 的 ， ， ， 拿 拿 王 戴 王 子 戴 戴 粉 粉 紅 的 ， 也 會 會 拿 不 來 王 」 」 」 」 」 ， 。 ， ， 我 戴 戴 戴\n",
      "我 是 李 希 平 ， 知 道 我 們 有 兩 個 禮 拜 就 要 見 面 ， 我 真 很 高 興 又 期 待 因 為 我 們 在 網 路 聊 天 了 快 一 年 了 。 我 ， 我 是 也 都 都 都 ， ， ， ， ， 有 有 有 有 是 是 的 平 ， 我 有 有 有 有 有 有 一 有 月 就 就 要 了 ， 我 我 真 有 有 有 有 有 一 有 ， 都 都 都 都 上 了 快 快 快 ， 。 了 。 。 。 是 是 。 。\n",
      "我 為 我 們 還 沒 見 過 面 ， 所 以 我 當 天 會 穿 粉 紅 色 的 上 衣 跟 牛 仔 褲 還 有 會 戴 了 一 個 帽 子 ， 帽 子 上 有 寫 我 的 名 字 ， 這 樣 你 應 該 認 得 我 了 。 因 因 因 因 因 因 因 我 我 我 ， ， 我 我 我 我 我 我 穿 穿 著 著 的 的 的 ， ， ， 我 我 我 我 戴 戴 戴 戴 戴 戴 戴 ， ， 我 會 會 有 寫 我 的 ， 因 我 你 你\n",
      "。 希 望 你 很 期 待 我 安 排 那 天 的 行 程 。 我 也 希 望 我 們 可 以 玩 的 很 快 樂 。 我 。 我 。 。 。 。 。 的 的 的 很 。 。 。 我 。 。 。 。 。 能 。 。 。 。 。 的 。 。 。 。 你 能 你 。 我 我 我 我 我 的 。 的 。 我 。 我 。 你 你 能 。 我 到 我 的 的 。 。 。 。 。 。 。 。 你 。 能 的 的 。 。 。 。 。 。 。 。 你\n",
      "我 然 我 爸 賺 錢 也 很 辛 苦 ， 帶 孩 子 也 並 不 容 易 ， 尤 其 是 我 ， 因 為 我 從 小 就 不 喜 歡 聽 話 。 而 而 雖 而 而 而 而 我 我 我 我 我 雖 而 而 雖 雖 雖 而 但 雖 我 雖 而 雖 爸 的 爸 ， 也 是 但 ， 但 是 子 也 也 很 很 不 ， 是 是 是 是 是 但 但 但 而 我 我 很 很 很 。 雖 。 而 但 。 。 。 。 。 。 。 。 雖 我 我 爸\n",
      "。 哭 的 時 候 給 我 最 喜 歡 喝 的 巧 克 力 牛 奶 是 我 母 親 。 我 喝 著 飲 料 聽 她 的 話 才 了 解 我 的 錯 。 我 我 聽 聽 聽 聽 是 才 是 我 是 我 我 我 我 我 我 。 。 。 。 。 。 。 。 我 。 我 。 我 。 的 的 。 。 。 。 。 是 是 是 我 。 。 。 我 。 。 。 。 。 。 。 。 是 是 我 。 。 。 。 。 。 。 。 。 。 。 。 。 的 。\n",
      "申 現 在 上 師 大 的 中 文 課 。 第 一 天 ， 我 要 去 那 邊 申 請 。 可 是 我 不 會 寫 漢 字 ， 所 以 申 請 表 很 難 提 。 李 建 國 跟 我 一 起 去 、 他 提 申 請 表 提 得 很 好 。 他 他 說 ， ， ， 的 ， ， 。 我 我 ， ， ， 我 我 我 去 ， 提 的 ， ， ， ， 我 我 寫 寫 我 我 我 我 我 我 ， 很 很 。 李 跟 跟 跟 是 跟 ， ， ， ，\n",
      "我 最 感 謝 的 人 是 我 的 家 人 。 因 為 他 們 總 是 鼓 勵 還 有 支 持 我 做 的 事 情 ， 他 們 也 幫 我 很 多 事 情 。 所 也 我 我 也 也 也 也 也 也 做 做 。 。 我 我 我 我 我 我 我 也 我 我 是 是 ， 的 和 人 ， 因 為 他 們 會 是 在 我 我 我 我 ， 的 和 人 。 。 。 也 也 也 也 也 能 做 做 做 。 。 。 。 。 。 。 。 也 也 。 我\n",
      "我 們 有 時 候 ， 有 一 點 嘮 我 ， 可 是 我 知 道 他 們 擔 心 我 。 他 們 付 很 多 錢 ， 所 以 有 時 候 我 對 他 們 很 不 好 意 思 ， 可 是 我 感 謝 的 心 情 比 較 多 。 我 我 我 我 們 ， ， ， 會 會 小 擔 我 我 我 ， 我 我 我 ， ， ， ， ， 我 ， 很 擔 我 付 我 ， ， ， ， ， ， ， ， ， 會 會 會 很 多 的 ， 可 ， 我 感 我 我\n",
      "媽 然 這 個 世 界 每 天 都 有 人 當 媽 媽 ， 但 真 的 將 媽 媽 的 角 色 做 得 好 ， 不 是 每 個 人 能 成 功 做 得 到 。 「 媽 媽 」 不 僅 是 個 名 字 ， 也 需 要 行 動 配 合 著 雖 做 雖 ， 雖 ， 上 都 都 有 人 當 當 當 雖 要 要 要 要 將 將 媽 的 做 做 要 做 ， 好 ， 要 要 要 要 要 將 她 做 ， 做 ， 做 「 媽 媽 ， 不 僅 是 個 ，\n",
      "她 以 ， 我 覺 得 來 表 示 我 的 感 謝 ， 我 不 一 定 做 很 誇 張 的 表 達 ， 只 跟 著 她 的 教 訓 就 好 。 所 我 我 我 ， ， 所 ， ， ， ， ， ， 所 我 我 我 來 來 來 ， ， ， ， 所 ， 我 我 我 來 來 來 來 的 的 ， ， 我 我 不 要 要 要 很 的 的 的 來 來 的 的 ， ， ， 我 會 會 要 很 的 的 。 。 。 的 。 。 。 我 。 。 。 我 我\n",
      "。 些 時 候 ， 媽 媽 會 抱 怨 我 們 對 她 很 像 一 個 僕 人 一 樣 。 有 可 能 是 因 為 她 做 的 事 我 們 不 懂 得 珍 惜 ， 而 不 說 「 謝 謝 」 。 有 有 有 抱 抱 抱 抱 她 有 抱 有 有 有 ， ， 抱 抱 抱 抱 抱 抱 ， 抱 她 ， 她 愛 愛 僕 僕 抱 也 抱 抱 抱 抱 抱 ， ， 她 她 做 事 事 都 不 不 不 不 得 去 而 而 而 不 她 她 謝 。 有 有\n",
      "我 媽 媽 養 六 個 孩 子 ， 每 天 照 顧 我 們 打 掃 房 子 還 有 做 飯 ， 我 爸 爸 工 作 得 很 努 力 為 了 給 我 和 我 姐 妹 好 的 生 活 所 以 我 都 尊 敬 他 們 。 所 我 我 我 我 我 和 爸 有 我 和 我 家 飯 還 家 會 家 我 ， ， 飯 ， 飯 還 有 做 我 我 我 我 還 還 還 ， 家 ， 飯 飯 還 ， 給 我 我 和 我 更 更 好 ， ， ， 我 也 很 很\n",
      "。 最 感 謝 的 人 是 我 的 媽 媽 。 因 為 ， 小 時 候 都 是 媽 媽 在 身 邊 ， 所 以 每 一 件 事 都 需 要 她 幫 忙 。 而 而 而 而 我 事 事 事 事 事 她 我 去 。 我 我 我 我 我 我 ， 我 ， ， ， ， 是 ， 的 媽 媽 。 而 ， 我 我 我 我 ， ， ， ， 和 ， 的 。 。 。 我 我 ， 事 事 事 事 都 她 去 去 。 。 。 。 ， ， ， ， ， ， ， ，\n",
      "她 想 跟 她 說 她 給 我 這 麼 多 錢 讓 我 去 玩 和 付 學 費 真 的 讓 我 感 覺 到 她 有 多 愛 我 。 我 知 道 她 為 了 我 才 把 她 的 錢 拿 來 買 我 要 的 東 西 我 西 她 她 給 她 和 她 。 她 。 她 給 她 花 她 多 她 讓 我 去 我 和 她 。 。 。 。 。 她 給 她 的 她 她 她 她 她 。 。 。 說 是 是 是 是 她 她 才 拿 錢 拿 去 買 她 西 。\n",
      "的 果 ， 沒 有 她 這 麼 努 力 的 一 直 幫 我 ， 今 天 我 的 生 活 就 會 改 變 了 很 多 。 如 如 如 如 如 如 如 如 。 如 如 如 如 如 如 如 如 如 如 生 如 如 ， 如 如 。 如 如 幫 如 如 如 如 如 她 一 能 的 的 的 的 的 幫 幫 幫 那 那 生 的 生 生 生 能 的 的 的 的 的 幫 。 如 如 如 。 生 的 生 也 也 。 。 。 。 。 如 如 如 如 如 如\n",
      "上 小 春 晚 上 图 ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ 。 。 。 。 。 。 。 。 。 。 ！ ！ 床 床 。 。 。 。 。 。 。 。 。 。 。 。 ！ ！ ！ 。 。 。 。 。 。 。 。 。 。 。 ！ ！ ！ 床 ！ 。 。 。 。 。 。 。 。 。 。 。 ！ ！ ！ ！ 。 。 。 。 。 。 。 。 。 。 。 ！ ！ ！ ！ 。 。 。 。 。 。 。 。 。 。 。 ！ ！ 床 上 晚\n",
      "你 要 你 告 訴 我 們 做 什 麼 我 們 就 做 了 ， 不 管 多 久 的 練 習 我 們 都 不 會 放 棄 。 請 你 告 訴 我 們 你 可 不 可 以 幫 我 們 然 後 我 們 準 備 做 什 ， 謝 謝 。 請 請 只 不 你 你 你 你 的 你 ， ， 我 我 就 會 你 ， 不 要 的 的 的 的 的 ， 。 。 你 你 你 你 你 你 你 你 你 你 我 你 你 們 能 以 幫 以 幫 做 ， 我 你 準 要\n",
      "。 為 劍 術 人 一 下 課 ， 就 開 始 鍛 鍊 ， 我 們 週 末 的 時 候 應 該 練 習 表 演 。 週 末 的 時 候 老 師 有 沒 有 空 ？ 周 週 週 週 週 週 你 ， 你 說 說 ？ 呢 ？ 因 因 因 因 因 。 因 ， 人 一 們 課 ， 就 開 要 鍛 練 。 而 們 周 在 的 也 的 就 ， 就 要 練 要 。 。 週 週 週 ， 週 ， ， 說 ， 說 ？ 我 。 因 。 因 而 因 說 人 人\n",
      "習 開 始 練 習 的 時 ， 我 覺 得 一 個 禮 拜 最 少 要 練 習 四 次 。 新 年 快 要 來 的 時 候 應 該 多 練 習 ， 可 能 要 每 天 練 習 。 再 再 再 再 再 我 我 我 要 要 練 再 再 我 剛 剛 剛 我 ， 的 ， ， 我 我 候 我 我 每 我 我 我 要 要 要 練 。 的 。 我 我 在 來 來 來 ， ， ， 要 。 。 。 。 。 我 。 。 來 多 來 。 。 。 。 。 。\n",
      "的 然 每 天 練 習 會 很 辛 苦 ， 可 是 這 樣 就 可 以 變 成 一 個 很 好 的 跳 舞 者 ， 可 以 一 起 加 油 。 雖 雖 雖 雖 雖 雖 雖 雖 雖 雖 雖 雖 雖 雖 雖 雖 雖 雖 雖 雖 雖 雖 雖 雖 雖 雖 要 雖 會 會 會 會 ， ， 雖 是 雖 是 雖 你 就 能 以 一 會 會 很 很 ， ， 雖 雖 ， 雖 。 可 可 。 。 。 雖 。 雖 。 。 。 。 。 雖 。 雖 ， 雖 要\n",
      "， 且 雖 然 我 們 在 台 灣 學 中 文 、 很 少 機 會 知 道 他 們 的 生 活 ， 文 化 ， 歷 史 等 等 。 同 時 ， 可 以 讓 大 家 看 原 住 民 的 傳 統 衣 服 ， 很 有 趣 ！ 哦 ！ ！ ！ 而 ， ， ， ， ， 在 在 、 、 、 、 、 卻 、 有 會 能 到 他 到 他 的 的 ， ， 、 、 、 、 ， ， 。 ， ， 也 也 也 也 以 看 看 看 穿 原 穿 的 的 的 ， ，\n",
      "芒 ？ ！ 可 是 這 個 是 不 是 芒 果 冰 是 芒 果 汁 ． ． ． 怎 麼 辦 ？ 怎 麼 辦 ？ 我 要 吃 是 芒 果 冰 了 。 。 。 我 我 我 啊 。 啊 。 啊 啊 啊 啊 啊 。 。 。 。 啊 啊 啊 啊 。 啊 。 啊 。 啊 ？ 啊 啊 啊 芒 啊 不 啊 啊 啊 啊 啊 。 啊 。 啊 。 。 。 。 。 啊 啊 啊 。 啊 。 啊 啊 啊 啊 。 啊 。 。 。 。 。 。 。 。 。 。 。 啊\n",
      "我 換 給 了 你 們 ， 他 們 馬 上 就 給 芒 果 冰 還 有 給 我 別 的 東 西 。 換 我 換 我 換 ， 給 我 給 給 ， ， 給 我 就 ， 我 給 。 。 ， 給 給 給 給 給 ， ， 我 我 就 說 就 就 我 我 我 了 你 給 ， 說 就 就 上 就 就 我 我 我 ， ， ， 我 的 給 的 說 上 就 就 就 就 我 我 ， ， ， ， 我 我 的 。 。 。 。 。 。 。 。 。 。 了 你 了\n",
      "， 我 來 說 ， 什 麼 東 西 都 可 以 買 ， 不 光 是 事 件 還 是 有 空 的 ， 也 不 必 只 在 在 百 貨 公 司 買 的 ， 在 路 邊 的 攤 位 也 可 以 買 。 對 對 對 對 對 對 在 ， 的 對 對 對 安 我 我 我 是 我 我 都 可 都 買 ， 在 安 是 是 是 我 安 ， 我 我 我 我 我 ， 安 是 在 在 司 司 司 司 司 司 ， 在 在 的 的 的 小 也 也 買 。 我 我\n",
      "， 天 我 跟 日 本 朋 友 去 台 灣 南 部 旅 行 ， 第 一 次 我 去 台 灣 沙 灘 ， 我 在 那 邊 好 好 閒 地 玩 。 天 天 今 今 今 好 好 的 好 閒 的 的 玩 天 今 今 今 今 。 今 旅 旅 旅 一 玩 的 跟 跟 跟 跟 旅 去 去 灣 旅 的 旅 旅 旅 我 是 的 去 的 去 去 。 去 ， 灣 旅 好 好 閒 閒 閒 閒 閒 玩 。 今 今 。 。 。 。 。 。 。 。 閒 閒 的 。\n",
      "， 去 別 的 店 逛 街 ， 找 鞋 子 可 是 別 的 店 的 小 姐 也 是 很 煩 ， 她 們 都 會 跟 在 我 的 後 面 潑 膠 水 。 我 走 去 哪 裡 ， 他 們 就 跟 我 走 去 哪 裡 。 我 我 我 我 ， 跟 跟 找 找 找 找 找 ， 找 找 子 。 。 ， 是 店 的 的 的 也 是 很 很 煩 找 我 。 。 ， 跟 跟 我 潑 潑 潑 潑 潑 潑 膠 想 想 想 去 哪 ， 也 也 就 我 走 去\n",
      "我 越 來 越 學 習 第 四 本 書 的 的 時 候 ， 我 常 常 想 我 不 夠 生 詞 ， 還 有 我 中 文 的 能 力 。 在 在 在 在 當 在 在 當 在 ， ， 我 我 我 我 在 在 當 在 的 的 的 的 的 的 ， 我 ， 越 在 到 第 第 的 的 的 的 的 的 ， ， 就 會 不 我 不 夠 不 會 的 的 的 的 的 的 ， ， 我 會 想 說 我 不 ， ， 。 ， 。 ， ， 有 ， 想 在 在 在\n",
      "習 果 不 過 我 這 樣 努 力 學 習 ， 我 覺 得 不 夠 的 話 ， 那 個 時 候 ， 我 要 買 第 三 本 書 努 力 學 習 。 如 如 如 如 如 如 如 如 如 如 如 如 如 ── 如 如 如 如 如 如 如 如 如 ， 如 ， ， ， 我 我 樣 ， 習 ， 我 我 我 還 ， 的 話 ， ， 麼 這 我 我 ， ， ， 我 我 ， ， 。 ， 。 如 。 如 如 如 [UNK] 。 。 。 。 。 。 如 。 如 ，\n",
      "。 星 期 天 到 陽 明 山 去 玩 。 因 為 那 天 路 很 匆 忙 ， 所 以 我 搭 公 車 。 我 我 我 那 我 我 是 和 到 到 去 。 。 。 。 我 。 我 。 。 。 我 我 我 我 在 。 。 。 我 我 星 星 星 那 我 我 我 那 ， 去 。 。 。 是 那 天 那 上 路 很 ， 要 要 到 ， 去 。 。 。 。 那 天 的 上 很 。 。 。 。 。 。 。 。 。 。 。 。 。 。 有 那 我\n",
      "。 為 是 週 末 ， 很 多 人 會 去 陽 明 山 去 看 風 景 。 到 到 山 上 以 後 ， 看 到 人 山 人 海 的 家 庭 。 每 個 家 庭 都 說 這 裡 的 風 景 真 漂 亮 。 還 還 也 都 都 ， 說 ， 因 因 了 周 末 假 也 有 就 都 到 到 到 到 ， 。 真 真 真 這 。 來 到 到 到 ， 到 到 ， ， 了 ， 的 的 。 。 。 而 ， 的 都 說 都 說 ， ， 的 真 真 真 這\n",
      "。 先 去 百 貨 公 司 買 衣 服 。 日 本 的 衣 服 都 很 好 看 ， 面 料 也 很 好 。 一 看 就 好 想 買 。 再 再 再 再 。 。 。 。 。 。 。 。 。 再 。 再 。 。 。 。 。 。 。 。 。 。 。 去 去 去 的 ， 的 。 。 。 。 的 的 的 的 都 都 都 。 。 的 。 。 。 。 。 。 的 的 的 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 去 去\n",
      "我 「 最 近 放 假 ， 我 跟 兩 個 同 學 從 十 月 十 號 到 十 月 十 三 號 一 起 到 台 灣 綠 島 去 了 。 因 因 因 因 我 我 因 「 ， ， ， 。 。 因 因 因 因 因 因 因 因 我 因 我 。 因 。 因 因 因 因 ， 我 跟 我 個 跟 跟 就 從 從 ， ， ， ， 因 因 因 我 跟 我 跟 跟 從 ， 從 ， 。 。 」 我 」 我 。 。 我 跟 ， 就 到 ， ， 。 。 。 我\n",
      "。 以 第 二 天 的 天 氣 非 常 好 。 我 們 馬 上 去 山 頂 看 風 景 。 從 頂 上 看 風 景 非 常 漂 亮 而 且 會 感 動 。 然 後 會 看 到 紅 。 藍 。 再 後 會 會 會 會 會 會 會 會 藍 藍 所 後 。 後 的 會 會 非 。 好 。 所 會 會 會 到 頂 頂 頂 頂 看 看 。 會 。 。 。 會 會 會 會 。 頂 。 。 會 會 。 。 會 會 會 會 藍 到 藍 。 所 。 。\n",
      "他 但 人 很 可 ， 他 們 還 喜 歡 幫 助 人 。 迷 路 的 人 ， 或 是 你 需 要 幫 忙 ， 他 們 他 們 馬 上 來 幫 你 的 忙 。 人 人 人 人 人 人 ， 會 會 來 幫 ， 。 人 人 人 人 人 人 人 ， 人 很 好 ， ， 們 還 會 歡 來 人 人 人 有 人 人 人 人 人 人 你 ， 不 人 人 人 人 但 ， 會 會 會 會 來 幫 你 。 。 。 。 。 。 。 。 。 。 。 好 。\n",
      "我 然 對 我 來 說 ， 我 的 英 文 還 不 錯 ， 可 是 波 士 頓 人 的 口 音 很 難 聽 。 我 有 很 多 的 問 題 他 們 的 口 音 。 我 我 我 有 有 有 我 ， ， 和 我 的 我 我 雖 我 雖 ， 雖 ， 而 ， ， ， ， 我 的 我 文 還 還 好 。 。 是 我 是 的 人 的 的 我 我 我 我 還 。 有 有 有 有 問 問 是 是 和 和 的 。 我 。 我 我 我 。 雖 。 雖 我\n",
      "。 的 朋 友 跟 我 說 別 擔 心 ， 慢 慢 你 會 聽 得 懂 波 士 頓 的 口 音 。 一 個 禮 拜 以 後 我 甚 麼 問 題 都 沒 有 ， 連 波 士 頓 的 口 音 我 都 聽 得 懂 。 我 而 而 我 後 你 你 你 我 我 ， ， ， ， ， ， ， 你 慢 你 你 你 得 到 到 到 頓 的 頓 的 我 我 ， 一 一 我 我 ， 就 就 就 就 也 也 沒 了 連 為 連 波 的 的 的 我 我 我 都\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "法 在 法 國 已 經 一 個 禮 拜 多 了 ， 時 間 真 過 著 很 快 ． ． ． 我 這 一 次 來 法 國 碰 到 很 多 很 有 意 思 的 事 ， 我 想 跟 妳 說 說 。 我 我 我 我 我 ， 我 ， 我 我 間 間 間 真 ， 已 有 一 一 月 多 多 ， 間 我 間 間 間 間 真 我 ， ， 一 一 多 ， ， ， 也 在 ， ， ， 很 很 很 多 很 的 事 我 我 我 我 妳 妳 妳 說 我 我 我\n",
      "他 給 我 的 台 灣 烏 龍 茶 ， 我 爸 爸 給 我 的 高 粱 酒 ， 我 自 己 買 的 原 住 民 音 樂 ， 我 都 給 他 了 ， 他 非 常 高 興 ， 我 就 跟 妳 說 ， 很 多 人 不 怪 ！ 很 這 很 這 很 很 很 很 很 很 很 ， ， ， 我 我 來 給 爸 給 我 我 很 很 很 很 很 都 怪 這 我 我 住 的 樂 我 很 很 很 很 ， 很 很 很 很 很 ， 他 他 他 妳 這 這 很 很\n",
      "。 今 天 早 上 去 了 「 新 店 」 區 。 台 北 交 通 非 常 方 便 ， 從 我 家 走 到 捷 運 北 站 是 十 分 鐘 。 從 「 台 電 大 樓 」 站 到 「 新 店 」 站 只 要 十 三 分 鐘 。 而 而 而 而 我 走 ， 走 了 ， 新 區 」 區 。 在 的 的 的 交 通 的 的 ， 從 從 新 的 站 車 從 車 車 車 是 十 一 十 ， 而 「 北 大 電 」 站 」 站 到 電 」 站 也\n",
      "人 動 物 園 參 觀 的 人 很 多 ， 除 了 大 人 之 外 ， 還 有 很 多 孩 子 ， 看 來 很 熱 鬧 。 在 在 在 在 ， ， 之 也 也 也 也 在 很 之 在 在 在 在 ， ， 之 也 ， ， 也 在 在 在 到 動 裡 園 裡 裡 的 人 很 多 ， 人 了 有 了 之 之 之 還 ， 還 之 人 之 之 ， ， ， 了 老 之 之 之 ， ， 還 有 很 人 ， ， ， 也 很 之 。 之 在 在 裡 裡\n",
      "。 覺 得 曼 谷 的 風 景 跟 台 灣 不 一 樣 。 我 我 在 得 。 。 。 。 。 。 不 灣 。 。 。 。 在 覺 。 。 。 。 。 灣 。 。 的 。 。 。 。 。 。 。 。 。 。 。 。 不 不 不 的 。 。 。 得 在 灣 的 的 。 跟 不 灣 不 不 不 。 。 。 在 得 在 的 的 的 跟 跟 不 灣 不 不 不 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 在 得 在\n",
      "。 好 ， 妳 還 記 得 我 嗎 ？ 我 剛 剛 從 澳 洲 回 來 的 。 我 我 我 我 我 是 我 是 是 在 。 。 我 我 我 我 呢 。 我 我 。 我 。 。 。 。 。 。 我 我 。 嗎 嗎 嗎 。 。 。 我 。 。 。 。 ， 還 還 ， 我 嗎 嗎 我 是 是 是 是 從 ， 。 我 。 還 還 嗎 嗎 嗎 嗎 是 是 是 是 是 ， ， 。 。 。 。 我 。 。 。 。 。 是 在 。 。 。 我 還 還\n",
      "。 跟 男 朋 友 也 希 望 妳 可 以 跟 我 們 一 起 去 。 真 的 很 好 玩 。 希 望 下 個 假 我 們 可 以 一 起 去 旅 行 。 旅 想 也 想 。 。 。 。 。 。 旅 旅 旅 。 我 。 妳 妳 妳 。 。 。 。 。 說 。 。 妳 妳 妳 妳 妳 妳 們 去 去 玩 。 。 。 。 。 旅 。 妳 妳 妳 。 。 。 。 。 。 。 。 旅 旅 旅 旅 。 。 。 。 。 。 。 。 。 。 。 。\n",
      "你 了 這 個 事 情 以 外 ， 我 在 韓 國 玩 的 事 情 都 很 有 意 思 了 。 還 有 我 買 了 你 的 禮 物 。 下 次 見 面 的 時 候 我 要 給 你 ！ 哦 我 我 我 我 ！ 的 我 我 我 我 ！ 你 ！ ！ 除 。 除 有 以 。 ， ， 和 和 的 玩 的 的 也 很 很 有 有 有 意 啊 。 。 ， ， 我 給 你 的 。 。 。 我 我 你 的 的 的 我 我 我 送 ！ 你 ！ ！ ！ 多\n",
      "， 們 從 波 蘭 坐 火 車 到 烏 克 蘭 ， 然 後 坐 公 車 到 車 爾 諾 貝 力 去 。 我 們 花 三 天 看 這 個 事 故 的 場 地 ， 車 爾 諾 貝 力 的 核 能 電 廠 。 車 在 在 爾 爾 爾 爾 再 我 先 們 從 們 ， ， ， 到 到 爾 蘭 爾 再 爾 再 公 到 到 到 爾 爾 到 到 到 到 爾 爾 再 花 花 去 看 看 看 ， 的 ， ， ， 爾 爾 爾 爾 爾 爾 的 的 的 ， 。\n",
      "畫 我 自 己 也 很 喜 愛 畫 畫 和 畫 油 畫 ， 所 以 覺 得 我 有 這 個 能 力 可 以 好 好 地 教 到 學 生 一 些 有 用 的 東 西 。 因 因 因 因 因 因 因 因 很 很 ， ， ， 我 我 因 我 因 我 因 我 也 很 喜 歡 畫 畫 和 畫 畫 畫 ， 我 以 我 我 我 有 也 有 很 和 ， 和 畫 去 去 去 去 給 學 的 一 很 的 的 的 的 。 」 因 因 因 因 因 我 我 也\n",
      "學 第 一 天 開 始 ， 我 會 叫 我 的 學 生 先 選 他 們 想 要 開 的 生 意 。 我 我 在 我 我 在 在 我 我 ， 我 我 我 我 我 在 我 在 在 在 我 在 ， 我 ， 我 我 我 我 我 我 在 我 在 在 我 ， ， ， 我 ， 我 我 我 我 我 我 去 先 去 擇 開 我 先 開 我 ， 我 我 我 我 我 我 去 先 先 擇 我 我 開 要 生 意 。 。 我 。 。 我 。 。 。 ， ， ，\n",
      "生 完 以 後 我 會 給 他 們 第 一 部 分 的 說 明 ； 怎 麼 開 始 他 們 的 生 意 。 生 意 開 始 以 後 ， 我 每 天 會 跟 我 的 學 生 講 他 們 做 對 和 意 的 方 法 。 我 我 我 我 我 ， 我 我 ， 我 我 講 我 們 做 一 的 分 的 ； ； ； 我 我 怎 要 做 做 做 的 做 的 的 ， 在 的 完 了 ， ， 我 我 我 會 跟 跟 我 的 學 講 做 做 對 和 事 意\n",
      "。 要 小 學 的 班 ， 有 大 概 二 十 、 二 十 五 個 學 生 。 一 而 要 在 是 是 是 。 是 要 要 學 的 的 要 。 。 。 有 。 。 。 。 。 。 要 最 的 小 要 。 是 是 有 有 有 有 是 十 一 學 學 學 的 小 ， 有 有 有 有 有 、 、 、 、 個 個 個 小 學 的 有 有 有 有 有 有 、 、 、 個 個 個 。 。 。 。 。 。 。 。 。 。 。 。 。 小 學 小 學\n",
      "， 以 我 上 課 的 時 候 ， 如 果 學 生 拿 到 了 好 成 績 ， 我 會 給 他 小 禮 物 。 可 是 如 果 學 生 拿 到 了 差 成 績 ， 我 會 給 他 補 考 。 所 所 所 所 我 ， 所 ， ， 我 我 我 在 我 上 我 的 ， ， ， ， 是 學 是 拿 他 不 壞 壞 壞 ， 我 我 我 小 小 他 壞 學 他 ， ， ， 學 壞 不 不 差 差 差 差 差 我 我 我 會 補 他 補 在 我 在\n",
      "濟 理 論 很 快 就 要 講 到 現 實 ， 因 為 經 濟 理 論 的 重 點 是 要 來 解 釋 現 實 ， 上 課 就 要 討 論 為 什 麼 理 論 能 或 不 能 解 釋 現 在 或 過 去 的 經 濟 情 況 。 因 因 因 因 但 很 也 就 要 講 到 到 實 ， 因 因 因 為 學 的 的 的 的 是 是 講 的 的 因 因 因 因 而 也 就 就 要 要 ， ， ， 該 能 夠 能 或 夠 夠 能 來 或 來 的\n",
      "， 他 說 ， 對 最 近 的 金 融 風 暴 ， 我 希 望 每 個 學 生 都 可 以 清 楚 地 說 明 為 什 麼 發 生 了 ， 跟 現 在 要 怎 麼 因 應 ， 採 取 那 些 政 策 可 以 解 決 這 個 問 題 。 他 他 他 再 而 針 對 ， ， 的 來 ， ， 我 我 我 我 讓 的 的 都 都 要 要 ， 這 ， 來 我 是 我 是 了 了 了 了 跟 跟 跟 要 要 對 去 對 ， 要 要 些 我 些 ，\n",
      "。 也 教 運 動 課 。 我 覺 得 運 動 是 很 重 要 ， 孩 子 從 小 要 動 ， 不 要 到 他 們 長 大 就 不 喜 歡 動 。 我 我 我 我 我 我 我 我 教 要 要 要 要 我 我 我 我 我 我 我 很 很 重 教 教 教 要 ， ， 我 我 ， ， 不 是 很 重 重 的 ， 讓 要 要 。 我 我 我 ， ， 不 也 很 重 重 不 不 不 不 。 我 我 。 。 ， 。 是 ， 重 。 教 教 教 做\n",
      "的 會 用 的 方 法 給 教 六 歲 的 孩 子 是 用 遊 戲 ， 用 唱 歌 的 方 法 教 他 們 運 動 。 我 也 會 用 這 個 方 法 教 他 們 荷 蘭 文 。 我 我 我 用 我 用 ， 教 用 們 用 的 的 。 我 我 我 我 。 教 給 教 六 。 的 。 的 。 。 玩 戲 戲 戲 ， 用 用 ， 。 。 。 。 。 的 。 用 我 我 會 用 我 的 的 ， 給 們 說 的 的 的 用 。 我 我 我 我\n",
      "。 最 希 望 是 他 們 會 愛 上 語 言 ， 就 可 以 學 很 多 語 言 。 去 外 國 可 以 認 識 很 多 的 人 ， 就 可 以 介 紹 。 等 等 最 最 最 最 最 最 也 學 介 介 學 紹 。 。 最 。 就 ， 最 是 的 是 ， 們 會 愛 上 語 人 ， 就 就 以 學 學 學 。 。 。 。 能 去 。 。 ， 以 ， 有 很 人 賺 學 學 介 介 介 外 紹 。 。 。 。 去 。 。 。 是 ，\n",
      "的 上 課 不 但 有 趣 ， 而 且 要 他 們 的 成 績 高 。 上 課 上 上 這 上 這 的 要 的 的 的 這 上 這 的 這 課 的 課 ， 上 的 的 的 的 的 這 上 這 堂 的 的 ， 的 ， 的 讓 的 的 的 的 的 這 程 要 的 要 ， ， 要 要 要 的 的 的 的 要 上 上 上 要 要 要 ， ， 要 要 要 的 的 的 的 要 這 上 上 的 這 的 ， 的 要 的 的 的 的 的 這 堂 程 程\n",
      "。 覺 得 說 話 的 能 力 最 重 要 。 去 外 國 的 時 候 非 說 話 不 可 。 所 以 來 我 的 留 學 生 一 定 要 說 很 多 。 他 們 每 次 進 步 很 快 。 我 我 而 。 。 。 說 說 。 說 在 我 我 。 我 要 是 最 最 最 。 。 。 去 去 去 的 的 我 要 要 要 要 要 。 。 。 來 來 來 留 留 留 ， 一 都 要 要 很 。 。 說 每 的 每 都 都 都 。 我 我 我\n",
      "課 果 學 生 有 不 懂 的 題 目 ， 隨 時 可 以 問 我 。 不 方 便 在 上 課 的 時 候 問 的 話 ， 可 以 下 課 以 後 到 我 的 辦 公 室 問 。 如 如 如 如 如 如 如 我 如 室 室 室 ， 就 如 有 如 有 有 有 有 的 ， ， ， 也 也 都 以 來 我 如 如 你 你 想 要 在 ， ， ， ， 再 再 我 如 在 在 在 在 再 去 室 的 室 室 室 室 室 室 如 如 如 如 我\n",
      "學 果 這 樣 的 話 ， 學 生 可 以 學 得 到 很 深 的 語 言 ， 還 有 學 生 會 對 他 們 學 習 語 言 。 如 如 如 如 如 如 如 如 如 習 習 習 如 [UNK] 如 如 如 ， 如 ， 如 會 如 會 如 習 如 ， 如 ， ， ， ， ， 就 就 以 會 到 到 很 到 的 的 ， ， ， ， ， ， ， 會 會 會 得 到 很 習 。 。 。 。 。 如 。 。 。 會 。 會 習 習 。 如 。 的 話\n",
      "教 當 我 當 大 學 教 授 的 話 ， 我 一 定 要 教 歷 史 。 為 什 麼 ？ 因 為 我 對 歷 史 非 常 興 趣 。 而 而 而 而 當 為 我 我 。 我 。 。 。 當 當 當 當 如 。 ， ， ， ， ， ， 如 ， 如 是 我 當 的 話 的 話 ， ， 我 就 要 要 我 它 。 。 那 ， ， 的 ， ， ， ， 我 我 我 。 。 。 。 而 [UNK] 。 。 。 。 。 。 。 。 。 。 如 。 要 當\n",
      "， 我 的 功 課 會 是 現 代 的 政 府 跟 以 前 的 比 較 ， 然 後 報 告 ， 他 們 覺 得 有 什 麼 東 西 一 樣 ， 有 什 麼 不 一 樣 ， 再 說 告 訴 我 他 們 覺 得 那 個 比 較 好 。 我 我 我 的 做 課 就 是 跟 跟 跟 政 ， ， 去 去 我 的 我 ， 我 ， 我 是 跟 我 是 ， ， 你 我 有 不 麼 不 不 不 ， ， 麼 不 麼 樣 的 樣 ， 再 說 我 ， 我 我\n",
      "語 對 語 言 學 有 很 大 的 興 趣 ， 對 別 的 科 學 我 不 太 了 解 。 而 且 英 文 ， 除 了 波 蘭 文 之 外 ， 是 我 最 容 易 說 的 語 言 。 英 英 我 英 我 我 我 最 最 說 ， 說 但 但 ， ， 之 也 也 很 很 的 ， ， ， ， 於 其 的 的 也 也 也 也 太 ， ， ， ， ， ， ， ， 波 和 和 之 之 之 之 ， 我 最 最 最 會 說 的 的 是 是 ， ， 之\n",
      "我 甚 麼 我 不 想 教 我 的 母 語 ？ 因 為 雖 然 我 當 然 說 很 好 的 波 蘭 文 ， 可 是 一 個 關 於 教 波 蘭 文 的 東 西 都 不 知 道 ， 我 也 沒 有 教 的 經 驗 。 而 我 我 為 為 為 為 麼 我 我 蘭 我 我 的 母 語 ？ ， ， 我 我 我 我 有 會 會 會 很 教 的 呢 呢 ， ， 我 我 我 於 我 蘭 蘭 蘭 蘭 蘭 的 我 都 都 不 不 我 我 也 我 我 我\n",
      "語 為 我 覺 得 用 學 的 語 言 跟 同 學 說 話 不 但 讓 語 言 學 生 學 到 不 一 樣 的 詞 彙 說 法 他 們 的 進 步 也 會 很 快 ， 所 以 我 也 要 給 他 們 很 多 機 會 跟 同 學 合 作 因 因 因 因 我 我 我 的 我 語 跟 跟 跟 來 ， ， 能 能 能 跟 學 的 能 能 能 不 跟 的 語 彙 彙 彙 ， ， ， 的 的 也 也 會 會 很 ， ， 我 以 我 也 要 有 多\n",
      "， 果 學 生 不 能 解 決 現 實 的 問 題 ， 吃 什 麼 ， 租 房 子 ， 交 朋 友 什 麼 的 ， 他 們 不 能 適 應 住 在 外 國 ， 所 以 他 們 的 生 活 會 有 很 多 困 難 。 如 如 如 如 如 如 留 如 們 不 去 去 去 做 做 的 ， 如 ， 住 如 麼 ， ， ， ， ， ， ， 住 ， 的 ， 如 如 如 就 就 就 會 要 住 在 ， ， ， ， ， 以 的 生 活 就 活 會 活\n",
      "、 從 現 在 起 ， 上 網 要 安 排 吃 飯 的 地 方 、 玩 的 地 方 什 麼 的 ， 等 你 跟 他 聯 絡 ， 我 就 會 確 定 的 。 我 我 我 我 我 們 我 們 們 們 會 不 你 你 你 你 要 你 你 的 、 ， 你 不 你 你 你 ， 要 要 他 、 、 的 的 、 、 、 、 的 、 ， ， ， 要 要 他 他 他 的 他 、 就 就 就 就 就 會 ， 。 。 等 。 。 、 。 。 。 ， ， ，\n",
      "。 」 他 穿 著 時 尚 的 衣 著 ， 戴 著 棕 色 的 眼 鏡 。 [UNK] [UNK] [UNK] [UNK] 。 ， [UNK] 他 ， ， ， 。 。 [UNK] [UNK] 在 。 。 。 。 ， 。 ， 。 著 。 鏡 。 [UNK] 。 。 。 。 。 。 ， ， ， 著 ， ， 。 ， ， ， ， 著 的 髮 ， ， ， 著 深 著 的 眼 鏡 鏡 。 。 。 。 的 ， ， ， ， 著 著 黑 鏡 鏡 眼 。 。 。 。 。 。 。 。 。 著 。 。 。 。 。 他 ，\n",
      "！ 的 身 材 又 高 又 瘦 ！ 眼 睛 大 大 的 、 鼻 子 高 高 、 嘴 唇 也 厚 厚 的 ， 真 的 美 死 了 ！ 美 看 看 瘦 ！ 瘦 ！ ！ 是 美 是 美 美 ！ ！ ！ ！ ！ ！ ！ ！ ！ ， 瘦 瘦 美 美 ！ ！ ！ 又 ， 又 的 ！ ， 巴 高 高 的 的 、 巴 高 、 ！ ， 瘦 ， ， ！ ， 巴 高 高 的 美 美 ！ ！ ！ ！ 巴 嘴 巴 也 ！ 也 、 ！ ！ 美 ！ ！ ！ ， ，\n",
      "， 那 一 天 帶 了 綠 色 隱 形 眼 鏡 ， 穿 著 也 很 流 行 ， 襯 衫 上 有 「 你 好 貓 咪 」 的 圖 ， 我 覺 得 她 看 起 來 應 該 差 不 多 十 五 歲 左 右 。 她 她 她 應 她 應 她 她 很 很 很 她 戴 她 藍 黑 黑 的 的 ， ， 應 很 也 很 很 很 很 ， 她 襯 她 的 的 的 的 的 的 的 的 ， ， ， 她 應 應 應 應 應 也 應 該 是 在 是 十 的 吧 她\n",
      "。 你 第 一 時 間 幫 我 查 出 這 個 女 生 的 資 訊 ， 然 後 馬 上 通 知 我 。 在 這 個 包 裹 裡 我 也 放 了 幾 首 我 親 自 寫 的 詩 。 求 求 你 請 把 它 交 給 我 最 愛 的 女 生 。 我 我 我 我 你 幫 我 查 你 這 你 你 生 的 的 ， ， 再 再 再 後 告 你 你 我 你 我 你 包 包 包 我 我 還 還 了 放 了 給 我 我 的 寫 的 詩 。 你 ， ， ，\n",
      "。 覺 得 這 個 人 長 得 不 錯 ， 很 想 要 跟 他 認 識 可 是 後 來 ， 沒 有 機 會 去 跟 他 說 話 。 我 我 我 我 ， 我 ， ， 他 他 他 他 了 。 我 我 我 我 ， ， ， ， ， ， ， 他 他 他 ， 他 ， 人 ， ， ， ， ， ， ， 他 去 他 認 他 認 。 ， ， ， ， ， ， ， ， ， 他 他 他 他 認 。 。 ， ， ， ， ， ， 我 他 他 他 。 。 ， 得 這\n",
      "的 近 我 的 生 活 變 了 很 苦 悶 ， 我 天 天 睡 不 著 而 食 物 也 吃 不 下 ， 都 是 那 位 美 女 在 你 的 生 日 上 的 錯 ！ 唉 是 是 是 是 你 你 你 我 犯 犯 犯 錯 啊 唉 是 最 是 最 ， ， ， 我 變 變 變 變 ， 啊 ， 我 我 我 都 覺 著 ， ， 吃 吃 吃 吃 吃 ！ 啊 ！ 是 是 你 在 在 在 你 的 你 犯 犯 做 的 啊 ！ [UNK] 是 是 是 ！ 在 我 的\n",
      "她 就 是 我 下 過 雨 的 天 夏 天 ， 有 她 就 讓 我 覺 得 很 期 待 ， 那 時 候 我 就 決 定 了 ， 她 不 是 從 這 世 界 來 的 ， 就 是 對 我 來 說 她 一 定 是 天 使 。 她 她 而 有 有 在 在 在 在 過 雨 的 天 夏 天 有 有 有 她 她 我 我 我 我 我 她 她 的 天 ， 有 有 有 有 她 ， 她 一 是 我 這 這 這 來 來 來 ， ， 但 是 對 說 ， 一 就\n",
      "你 個 禮 拜 六 ， 在 你 最 喜 歡 的 夜 店 ， 我 愛 上 了 最 漂 亮 的 女 生 。 我 還 不 知 道 她 的 名 字 呢 ， 可 是 我 已 經 確 定 我 想 當 你 的 白 馬 公 主 。 我 我 在 那 一 那 ， ， ， 你 ， 你 你 你 你 的 的 夜 裡 ， ， 我 我 你 你 你 最 的 你 你 你 的 的 ， 我 我 她 她 她 的 是 呢 ， ， 我 我 我 她 經 是 她 她 你 你 你 你\n",
      "。 的 臉 白 白 的 、 圓 圓 的 ， 頭 髮 黑 黑 的 ， 眼 睛 大 大 的 。 她 穿 的 衣 服 也 很 適 合 她 ， 看 起 來 就 算 是 一 個 女 神 不 說 ， 她 的 性 格 也 算 是 非 常 溫 柔 。 她 她 她 的 她 的 、 、 ， ， ， ， ， 也 也 大 ， ， ， 也 也 黑 、 ， ， 她 她 她 ， 也 很 很 很 ， ， ， 她 她 她 是 是 是 小 小 不 也 不 她 的 的 的\n",
      "、 時 候 我 就 睡 醒 了 ， 我 覺 得 很 可 怕 、 不 能 再 睡 覺 。 」 我 我 我 我 我 我 我 我 再 再 再 再 我 」 我 我 我 我 我 我 我 我 我 我 再 我 我 我 」 。 我 我 我 我 我 我 這 我 我 我 就 就 了 ， 我 我 覺 就 很 很 很 、 不 再 再 再 再 不 。 。 我 我 我 我 我 很 很 、 再 再 再 再 再 。 。 [UNK] 。 我 。 。 。 。 。 那 。 我 我\n",
      "， 那 個 夢 我 參 加 我 好 朋 友 的 結 婚 會 ， 在 一 個 法 國 小 城 。 人 很 多 ， 其 實 地 方 都 死 了 ， 但 是 每 一 個 人 看 起 來 好 舒 服 ， 不 注 意 位 子 夠 不 夠 。 我 我 是 是 是 ， 是 ， ， 我 我 的 的 的 ， ， ， 是 是 是 小 小 小 的 ， ， ， ， ， ， 我 很 都 都 都 都 都 ， ， ， 是 人 人 都 都 都 都 好 很 很 我 是\n",
      "我 知 道 ， 有 時 候 我 跟 我 的 爸 爸 、 媽 媽 說 英 文 都 快 點 忘 忘 了 我 的 中 文 ， 我 的 夢 給 我 的 感 受 很 難 ， 因 為 很 難 。 我 夢 夢 夢 ， ， ， ， ， ， ， ， 我 我 我 我 ， 我 ， ， 我 ， 我 跟 、 、 、 、 、 說 說 ， ， 會 會 忘 忘 忘 ， 跟 、 說 ， ， 做 夢 夢 夢 夢 ， ， 很 ， ， ， ， 因 ， 很 我 我 我 我 ，\n",
      "， 文 對 我 說 是 非 常 很 難 ， 不 論 你 會 會 一 千 多 個 字 ， 你 不 會 說 很 寫 ， 都 沒 有 用 。 文 文 文 文 文 文 文 ， 字 ， ， ， 文 文 文 文 文 文 文 字 ， ， 文 ， 文 字 文 字 我 文 字 這 ， ， 難 ， ， 你 你 你 會 一 說 一 個 字 ， 這 ， ， 難 ， ， ， ， ， ， ， 寫 。 文 英 文 。 我 說 很 很 ， ， ， ， ， 。 文 文\n",
      "我 已 經 很 期 待 我 們 的 新 年 晚 會 ！ 我 們 下 個 禮 拜 已 經 談 談 了 我 們 要 表 演 的 是 什 麼 。 所 以 我 跟 您 要 說 以 下 我 們 對 這 次 新 年 晚 會 的 想 法 。 我 我 我 我 們 我 們 ， 我 的 我 的 的 ！ ！ ！ 在 在 在 在 這 也 就 就 就 要 要 ， 。 ！ 我 要 在 我 在 我 也 我 我 我 我 您 您 您 要 說 我 們 們 們 們 們 們\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "。 您 看 ， 我 們 覺 得 這 場 戲 劇 會 讓 大 家 了 解 留 學 生 對 台 灣 的 想 法 。 而 且 我 們 希 望 鼓 勵 更 多 的 留 學 生 。 我 您 您 我 也 我 鼓 有 更 的 的 來 。 。 ， ， 我 我 我 ， 我 ， 我 ， 這 個 戲 也 ， ， 會 讓 了 去 我 ， ， ， 的 ， 這 的 的 也 也 也 會 我 也 也 能 鼓 有 更 的 的 的 留 。 來 。 我 您 我 您 我 ，\n",
      "演 一 隊 應 該 表 演 五 分 鐘 ， 然 後 第 一 名 的 隊 要 收 到 禮 物 。 我 希 望 國 家 中 心 可 以 幫 我 找 三 個 老 師 給 表 演 的 人 分 。 我 我 我 我 我 有 我 我 我 我 演 每 每 的 要 表 演 表 演 演 演 ， ， ， 後 第 後 的 的 隊 要 要 要 表 演 演 演 。 ， ， 家 家 家 的 家 要 要 要 找 我 的 的 給 我 我 我 演 的 部 我 我 要 表\n",
      "。 果 老 師 跟 同 學 們 有 時 間 ， 每 天 下 課 以 後 可 以 一 起 到 大 學 旁 邊 的 公 園 去 練 習 半 個 小 時 。 需 要 的 話 周 末 也 可 以 見 面 。 週 周 週 周 周 周 周 周 ， 如 跟 如 如 跟 跟 如 們 有 的 話 ， 話 那 下 在 以 ， 以 就 可 在 在 在 在 在 話 ， 在 周 晚 去 去 ， ， 。 。 周 周 有 周 周 周 周 周 週 時 周 。 。 如\n",
      "我 且 我 也 想 向 國 語 中 心 請 教 有 沒 有 可 以 給 我 們 的 教 室 ， 給 我 們 下 雨 的 時 候 機 會 乾 淨 地 練 習 唱 歌 。 」 而 而 而 而 好 我 好 靜 淨 去 ， ， ， 而 而 且 而 且 ， ， 想 靜 她 的 的 ， ， ， ， ， ， ， 能 能 供 有 安 好 的 ， ， ， ， ， ， ， 安 好 安 好 安 安 淨 好 淨 地 和 。 。 。 。 。 。 。 。 能 能\n",
      "： 演 節 目 ： 千 里 島 民 族 舞 蹈 。 演 。 表 表 ： ： 千 ： 千 。 的 舞 。 。 。 。 表 表 。 ： ： 千 ： 。 。 舞 舞 。 。 。 。 。 。 。 。 ： ： 千 ： 。 。 。 。 。 。 。 表 。 。 ： ： 千 里 峇 民 的 舞 舞 。 。 。 。 。 。 ： 千 ： 千 峇 峇 民 的 舞 舞 。 。 表 。 。 。 。 。 千 。 島 。 。 。 。 。 。 表 。 表 。 ： 千\n",
      "。 習 時 間 、 地 點 ： 每 個 禮 拜 １ 、 ３ 、 ５ ， 下 課 後 （ 大 概 １ 小 時 ） 。 在 ３０１ 教 室 。 教 教 教 教 教 ： 教 在 ： 教 教 教 教 教 教 學 學 習 學 學 。 。 學 學 練 習 時 間 、 時 間 ： 習 在 禮 的 １ 的 ２ 、 ４ ， ４ 或 上 習 習 。 習 在 禮 的 ２ ２ 。 在 習 。 。 上 。 。 。 。 。 。 。 ， 。 。 。 練 。 時 間\n",
      "， 師 ， 我 們 班 的 同 學 們 大 概 學 好 了 以 後 ， 請 看 一 下 給 我 們 建 議 。 表 演 時 ， 我 們 要 請 國 語 中 心 幫 我 們 放 音 樂 、 燈 光 。 燈 ， 師 老 ， ， 都 學 好 了 ， 老 你 老 把 的 ， 大 ， 大 都 學 習 了 好 以 ， 請 您 請 看 ， ， 大 ， ， 都 學 ， 表 時 時 表 要 要 請 國 在 的 的 忙 幫 我 們 放 燈 燈 燈 。 請\n",
      "演 我 所 知 同 學 們 很 愛 唱 歌 、 跳 舞 ， 他 們 也 很 有 演 戲 的 天 ， 所 以 表 演 音 樂 劇 絕 對 是 一 個 可 以 讓 他 們 發 揮 所 能 的 機 會 。 」 我 我 我 我 我 我 我 我 我 我 我 的 的 的 都 很 很 很 歌 玩 、 也 ， ， 們 也 也 很 有 有 天 很 很 愛 歌 、 ， ， ， ， ， 也 是 是 一 能 能 能 能 能 發 們 發 能 才 機 機 我 我\n",
      "的 為 我 們 在 台 灣 的 時 間 不 是 很 長 ， 所 以 我 們 不 太 了 解 要 在 那 裡 買 我 們 所 需 要 的 道 具 ， 以 及 我 們 也 沒 有 布 置 舞 台 的 經 驗 。 因 因 因 因 因 因 在 因 ， 在 在 在 在 的 的 也 並 不 還 不 長 ， ， 以 ， 們 也 也 也 太 也 也 ， 不 ， 購 買 購 所 所 的 的 道 ， ， 而 我 也 也 也 也 布 置 布 置 的 的 的\n",
      "中 們 班 有 很 多 不 同 的 國 家 來 的 學 生 ， 溝 通 的 語 言 只 是 中 文 ， 都 要 學 把 中 文 學 好 。 我 我 我 我 我 我 我 我 我 要 要 要 要 我 我 我 我 在 班 來 來 的 的 ， 我 都 都 有 都 有 從 從 國 的 出 來 的 的 溝 溝 溝 溝 溝 的 不 不 是 的 國 的 出 來 的 的 要 要 學 溝 要 我 」 。 而 。 ， ， ， 但 ， 要 ， 普 都 班 有\n",
      "， 的 台 灣 朋 友 們 都 很 會 唱 歌 ， 而 且 唱 的 很 好 聽 ， 上 課 的 時 候 也 有 教 我 們 中 文 歌 。 我 我 我 而 我 。 我 我 有 唱 唱 唱 唱 。 我 我 我 我 也 有 很 ， ， 我 我 ， 的 的 朋 也 也 都 都 會 唱 歌 ， 都 歌 歌 的 的 的 也 ， ， 也 也 也 很 唱 唱 唱 教 唱 唱 唱 歌 唱 。 。 。 在 。 。 。 。 。 教 教 唱 歌 唱 朋 的\n",
      "唱 多 人 一 起 唱 歌 的 時 候 最 重 要 的 是 就 是 多 一 點 練 習 ， 明 天 下 課 以 後 大 家 一 起 討 論 吧 ！ （ 最 最 再 最 再 再 再 再 再 去 去 ~ ~ 最 （ 最 最 最 最 最 ， ， 在 在 在 ， ， 歌 的 最 ， 最 重 最 的 ， 的 是 多 多 點 點 ， ！ （ 最 最 最 最 ， ， ， 再 去 去 去 去 吧 ~ ） ） [UNK] ！ ！ ！ ！ ！ ！ ！ 再 人 在\n",
      "， 場 不 只 大 ， 可 以 讓 跳 舞 時 的 走 來 去 ， 跳 來 跳 去 ； 為 了 安 全 起 見 ， 放 鞭 炮 ， 也 比 較 安 全 因 為 是 室 外 的 。 場 場 場 場 也 也 也 ， 也 ， ， ， ， 跳 ， 也 也 夠 大 大 ， ， 讓 人 人 時 人 走 跳 走 ， ， ， 跳 ， ， 也 也 讓 人 人 時 ， 人 ， ， ， ， ， ， 不 ， ， ， ， 是 有 很 。 。 。 。 也 不 夠\n",
      "， 也 有 老 師 幫 我 們 找 時 間 在 操 場 ， 以 免 我 們 要 用 時 ， 正 好 有 其 他 人 也 想 用 。 」 有 有 有 是 說 說 [UNK] 說 說 說 」 有 有 有 有 有 讓 說 讓 說 找 在 在 ， ， 有 有 有 有 有 說 我 ， 找 找 間 在 在 在 ， ， 讓 在 在 在 在 說 讓 找 找 找 ， 在 在 用 ， ， 讓 讓 在 在 說 。 。 。 。 。 。 。 。 ， 。 有 有 有 叫\n",
      "的 次 看 到 買 帽 子 的 攤 子 都 會 十 分 興 奮 ， 但 每 次 不 敢 買 因 為 總 有 朋 友 在 我 背 上 潑 了 冷 水 地 說 ， 「 你 不 適 合 我 」 的 這 一 句 話 。 每 每 每 每 每 我 每 每 每 我 我 買 那 的 時 我 都 會 會 會 很 的 每 我 每 也 都 都 都 ， ， ， 我 總 是 會 會 我 我 背 潑 潑 潑 了 ， 說 「 你 你 我 不 我 我 我 。 。 這\n",
      "我 直 到 拿 到 成 績 單 那 時 刻 我 嚇 了 一 跳 ， 我 真 的 很 高 興 自 己 總 算 進 了 前 五 名 ， 我 父 母 親 也 很 高 興 。 我 我 我 我 我 我 我 和 和 也 也 很 很 我 我 我 我 我 我 我 我 到 的 的 的 的 的 ， 我 嚇 嚇 嚇 了 ， ， 我 也 很 很 我 就 ， 總 就 總 以 進 了 前 ， ， 我 的 和 也 很 很 很 也 我 。 我 。 。 。 我 我 我\n",
      "。 們 選 了 一 台 電 腦 。 可 是 老 闆 說 ， 這 台 電 腦 看 起 來 很 漂 亮 。 但 是 電 腦 的 用 量 不 大 ， 也 不 好 用 。 而 他 我 我 他 就 很 大 大 ， 很 不 很 我 他 他 他 他 他 只 挑 了 中 ， ， ， 。 。 是 ， 是 卻 ， ， ， 這 台 它 是 ， ， 。 。 。 是 ， ， 他 ， 的 並 並 大 大 大 很 很 不 。 。 。 我 。 。 。 ， 。 買 了\n",
      "。 所 有 的 錢 都 用 光 了 買 這 台 電 腦 。 我 怎 麼 辦 ？ 不 可 以 換 。 香 港 太 遠 。 飛 機 票 買 不 起 。 我 。 。 。 。 。 。 。 怎 。 買 買 。 我 我 。 。 。 。 。 。 。 怎 怎 怎 。 怎 。 。 。 。 。 這 。 這 。 怎 該 怎 怎 怎 。 。 。 。 。 。 。 。 。 。 。 。 。 。 怎 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。\n",
      "！ 有 是 台 灣 小 吃 我 最 喜 歡 。 他 們 小 吃 又 好 吃 又 有 很 多 種 類 像 是 炒 米 粉 ， 臭 豆 腐 ， 烤 魷 魚 ， 麵 線 ． ． ． 等 等 ！ 還 還 還 還 還 ， ， ， ， ， ． 等 還 還 還 還 還 的 的 ， ， ！ ！ ！ 還 還 還 ！ 還 還 還 還 ！ 的 種 種 種 種 。 。 ！ ！ 的 又 ， 還 ， 還 ， ， 烤 ， ， ， 等 ． 等 等 還 還 還 有 還\n",
      "的 果 老 師 把 一 個 安 靜 的 氣 氛 換 成 熱 鬧 活 潑 的 ， 整 個 人 的 精 神 都 很 好 ， 那 堂 課 的 效 果 會 很 不 錯 的 。 如 如 如 如 如 如 如 如 如 如 ， 如 如 如 如 如 如 如 如 如 能 如 能 一 很 一 的 ， ， 換 ， 如 成 活 、 一 每 每 每 每 一 一 ， ， ， 換 ， 熱 如 一 如 也 也 也 很 很 很 。 。 如 如 如 如 如 如 如 如 你\n",
      "我 果 我 是 老 師 ， 我 喜 歡 當 中 文 老 師 ， 因 為 我 學 中 文 學 了 差 不 多 五 年 了 ， 在 學 習 中 文 的 過 程 當 中 我 已 經 找 到 了 我 前 所 未 有 的 興 趣 。 如 如 如 如 如 如 而 我 ， ， 我 當 當 當 當 當 當 當 ， 如 而 我 是 做 做 的 我 當 當 當 當 當 當 ， ， 我 我 我 我 的 的 的 的 中 我 我 我 已 經 我 我 我 我 的\n",
      "師 自 己 認 為 當 老 師 除 了 有 模 範 能 力 之 外 還 要 有 敬 業 精 神 。 另 外 ， 還 要 不 斷 吸 收 新 知 充 實 自 己 才 能 為 學 生 做 了 好 榜 樣 。 我 我 我 我 我 模 自 模 業 ， 自 ， ， 當 的 ， ， 了 有 辯 範 範 學 力 ， ， 做 。 我 ， 當 ， ， 規 辯 辯 領 範 業 要 吸 吸 新 識 識 ， ， ， ， ， ， 能 為 做 做 做 的 。 做\n",
      "老 統 的 教 學 方 式 就 是 老 師 說 什 麼 ， 學 生 就 知 道 什 么 ， 學 生 都 認 為 老 師 說 出 來 的 話 不 會 錯 的 ， 所 以 不 會 自 己 研 究 發 揮 一 下 本 身 的 創 新 性 。 傳 我 傳 傳 的 的 是 是 是 ， 師 想 要 麼 麼 麼 生 就 就 就 聽 聽 ， ， 是 ， 要 麼 話 麼 他 他 話 話 話 是 是 是 有 錯 ， 就 就 以 不 會 去 去 發 ， ，\n",
      "我 周 末 過 得 怎 麼 樣 ？ 我 今 天 在 複 習 我 們 這 個 星 期 學 習 的 內 容 。 周 我 周 們 周 末 我 末 我 。 我 我 我 我 我 周 我 周 末 周 末 我 得 周 ， 。 我 我 你 周 你 周 末 周 末 過 得 還 得 ？ 。 ， 們 正 在 學 在 我 們 周 們 週 要 要 的 呢 。 。 們 正 在 在 在 。 們 們 的 們 週 要 。 。 。 。 。 在 。 。 。 們 你 末 過\n",
      "我 「 如 果 雖 然 我 很 用 功 ， 可 是 我 不 知 道 很 多 是 我 們 學 習 的 部 分 ， 我 不 能 趕 上 您 的 課 。 」 」 」 」 但 但 但 但 ， ， ， ， 」 」 我 」 您 ， ， ， ， 我 ， ， ， ， ， ， 我 很 很 很 ， ， 是 我 我 我 有 有 ， 是 是 是 我 很 很 很 ， ， ， 我 我 能 能 。 。 的 。 。 」 」 」 。 。 。 。 。 。 。 。 。 ，\n",
      "！ 謝 您 願 意 擔 任 我 們 國 華 系 的 會 話 教 師 ！ 也 由 衷 的 感 恩 您 這 開 學 以 來 的 關 懷 與 照 顧 ！ 也 再 謝 也 感 謝 感 感 謝 ！ 謝 再 再 也 感 感 謝 感 際 感 感 感 感 感 謝 ！ 感 感 謝 感 們 國 大 會 大 會 話 會 師 ！ 衷 由 衷 的 衷 的 大 國 大 會 大 的 ！ ！ ！ ！ ！ 感 ！ 衷 ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ 謝 ！\n",
      "的 兩 個 禮 拜 的 時 間 有 在 老 師 的 關 照 之 下 ， 對 於 台 灣 的 生 活 我 慢 慢 的 覺 得 習 慣 ， ， ， ， ， 也 也 的 的 的 的 的 得 ， ， ， ， 。 ， ， ， ， 也 我 的 也 的 ， ， ， ， ， ， ， ， ， 老 老 的 的 的 ， ， ， ， ， 在 在 ， 的 ， ， 我 我 的 的 的 的 得 。 。 。 。 。 。 。 ， 也 也 的 的 的 的 的 。 ， ，\n",
      "、 可 讓 我 能 很 快 速 地 連 起 到 名 詞 的 意 思 、 很 快 地 了 解 到 課 程 的 內 容 。 可 可 這 可 讓 可 地 可 這 這 這 這 這 這 這 這 這 這 可 地 能 地 能 夠 地 想 連 想 到 、 我 能 我 能 地 地 地 地 聯 想 到 到 名 的 的 、 、 地 地 地 地 地 聯 地 聯 想 到 到 名 、 。 。 。 地 地 地 地 地 地 地 到 到 。 。 。 [UNK] 。 我 能 我\n",
      "老 望 在 未 來 學 習 的 期 間 我 會 好 好 的 努 力 ， 不 會 讓 老 師 失 望 。 希 希 希 希 希 希 希 我 我 我 。 。 希 希 希 希 我 很 希 。 希 希 我 我 我 我 。 希 希 希 我 的 的 的 ， 我 我 我 ， ， ， ， ， 我 會 的 好 的 的 的 ， ， 會 我 在 ， 。 ， ， 我 會 的 的 的 的 的 。 。 也 會 去 。 。 。 。 。 。 。 。 。 。 。 在 我\n",
      "我 學 已 經 過 了 兩 個 禮 拜 ， 幸 運 的 是 ， 我 上 到 很 好 的 老 師 的 課 。 而 現 現 大 我 大 我 大 我 ， 。 ， 我 我 我 的 同 ， 同 我 大 我 大 我 大 。 。 我 我 我 上 的 開 是 開 經 開 了 一 了 ， ， ， 很 是 的 是 是 是 能 能 了 了 了 大 一 。 。 ， 是 是 的 是 是 是 我 能 了 很 了 。 。 。 。 。 是 是 是 ， ， 我 我 我\n",
      "課 望 老 師 您 可 以 體 諒 我 的 煩 惱 而 改 變 上 課 的 方 式 ， 也 許 是 我 不 夠 聰 明 ， 但 我 還 是 希 望 在 上 老 師 您 可 以 學 到 一 些 東 西 。 希 希 希 我 您 您 您 ， 希 您 您 您 您 能 以 因 諒 我 的 的 ， ， ， 而 我 我 的 的 ， ， 我 我 我 我 的 ， ， ， 而 我 我 我 我 我 我 我 在 上 您 您 您 您 您 您 多 些 些 西 您\n",
      "課 望 老 師 您 可 以 幫 我 解 決 這 個 問 題 ， 這 門 課 是 我 最 難 也 最 重 要 的 課 ， 我 當 然 希 望 可 以 拿 到 最 好 的 分 數 而 讓 父 母 感 到 驕 傲 。 我 我 我 希 我 希 希 希 您 您 您 能 夠 幫 我 我 決 我 個 的 ， ， 這 是 課 是 是 是 我 的 我 我 我 我 這 的 這 我 我 我 也 到 我 到 能 我 我 的 的 的 ， 而 而 到 感 到 到\n",
      "。 們 實 在 對 我 太 好 了 ， 我 不 想 辜 負 了 他 們 的 愛 心 ， 希 望 以 後 可 以 好 好 的 孝 順 他 們 老 人 家 。 我 我 我 我 的 我 會 他 ， 好 的 老 他 唉 他 他 他 也 也 也 是 也 實 在 是 也 太 好 了 ， 我 也 也 再 再 辜 他 他 他 的 的 ， ， ， 我 ， 我 也 也 的 的 的 的 的 好 的 老 老 的 他 我 我 。 我 能 的 的 的 。 ， 在\n",
      "！ 近 天 氣 冷 了 ， 您 的 身 體 舒 服 ？ 真 的 似 乎 來 秋 天 了 ！ 啊 ~ ~ ！ ！ ！ ！ ！ 是 是? 了 了 ！ ！ 很 ！ ！ ！ ？ ！ ！ ！ ！ 是 ！ 了 ！ ！ ！ ！ 不 不 不 ？ ？ ？ ， 的 ， 很 ， ， 您 的 是 不 不 不 ？ ？ ？ 要 是 是 到 到 了 ？ ？ 的 不 不 不 不 ？ ？ ？ 是 是 是 是 了 ！ ！ ！ ？ ？ ？ ？ ？ ？ ？ ！ ， ， 太\n",
      "， 的 課 聽 得 好 ， 總 是 教 得 努 力 ， 我 很 感 動 。 」 [UNK] [UNK] 。 [UNK] 。 我 很 。 。 。 我 我 我? 。 。 。 。 我 得 。 。 。 。 。 。 。 。 。 。 。 。 。 。 很 得 ， ， ， ， 我 我 教 得 。 。 總 是 教 得 很 很 ， ， 讓 很 。 。 。 。 很 。 ， 是 很 得 很 很 力 ， 也 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 很 教\n",
      "灣 教 台 灣 的 情 況 、 藝 術 、 各 地 的 消 信 、 有 意 思 的 故 事 ， 我 們 都 覺 得 您 的 書 很 有 意 思 。 」 」 我 」 您 講 講 書 書 書 書 書 很 」 」 教 您 、 、 、 、 的 、 、 、 的 的 的 的 、 、 、 、 、 、 的 通 通 、 、 、 、 的 的 的 、 、 、 、 、 的 講 的 書 書 書 書 很 。 。 。 。 。 ， 和 和 講 的 是 是 是 的 的\n",
      "我 一 次 上 課 的 時 候 我 很 緊 張 ， 老 師 說 的 話 完 全 聽 不 懂 。 我 很 生 氣 了 對 我 自 己 ， 「 為 甚 麼 我 一 個 人 聽 不 懂 ？ 」 。 」 。 但 我 我 我 我 我 ， 。 我 。 我 我 ， 我 我 ， ， 我 我 緊 說 ， 說 我 我 我 我 我 我 我 我 我 ， 我 我 說 我 我 ， ， ， 說 說 「 你 你 是 我 我 我 就 都 人 ？ ？ ？ ？ 後 我 在\n",
      "我 記 著 嗎 ？ 我 去 找 您 以 後 ， 說 「 請 幫 助 我 」 ， 那 天 以 後 ， 您 每 天 幫 助 我 。 好 好 」 [UNK] ， ， ， ， ， 都 都 都 。 記 好 」 ？ ？ ， ， ？ ， 您 ， ， 您 我 您 嗎 記 嗎 ？ ？ ， ， 您 您 後 ， ， ， 我 我 我 我 從 好 從 ， ， ， ， ， ， ， 都 都 都 。 。 。 。 。 。 ， 。 。 。 ， 。 ， ， 請 。 。 嗎 ？ 嗎\n",
      "我 有 一 天 越 來 越 聽 得 懂 您 的 課 ， 我 很 感 謝 您 的 幫 助 。 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 到 我 我 我 我 我 我 我 我 我 我 我 到 到 懂 懂 懂 懂 ， 我 我 我 我 我 我 我 懂 懂 懂 懂 懂 我 了 ， 也 很 也 ， ， 我 我 我 懂 懂 懂 懂 懂 課 了 ， 我 也 想 ， 您 的 我 懂 到 懂 懂 。 ， ， 我 我 我 我 我 我\n",
      "。 生 王 天 華 為 上 。 [UNK] [UNK] [UNK] [UNK] 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 為 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 為\n",
      "的 鮮 蝦 子 是 以 後 加 鳳 梨 罐 頭 再 加 沙 拉 醬 的 。 我 們 孩 子 也 很 喜 歡 吃 鳳 梨 蝦 球 ， 所 以 去 溫 泉 時 我 們 常 常 點 鳳 梨 蝦 球 ， 味 道 真 好 。 吃 還 還 還 吃 新 是 可 蒸 是 可 是 是 是 ， ， ， 再 再 再 加 醬 吃 吃 我 ， 我 有 也 也 也 也 也 也 吃 吃 的 ， ， ， 在 在 泡 玩 時 也 也 也 也 會 點 點 球 ， ， 吃\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "， 母 很 忙 ， 忙 的 時 候 吃 快 餐 ， 我 們 平 常 吃 很 少 菜 ， 所 以 吃 中 國 菜 的 時 候 會 吃 很 多 菜 。 我 父 父 父 父 我 父 父 父 吃 多 很 多 吃 父 我 父 父 很 父 父 父 父 父 很 忙 ， 忙 的 忙 忙 會 快 快 ， 而 而 父 也 也 很 少 少 菜 吃 有 吃 吃 吃 中 的 的 的 也 會 吃 很 多 多 。 我 。 。 。 。 。 。 。 。 。 吃 很 忙\n",
      "。 本 菜 跟 中 國 菜 不 一 樣 ， 日 本 菜 比 中 國 菜 比 較 清 爽 的 。 吃 而 日 日 日 日 日 。 日 。 。 。 。 。 。 日 日 日 。 日 日 本 。 日 。 。 。 。 。 。 。 。 ， 日 的 日 。 日 和 日 是 是 不 ， ， ， ， 的 菜 是 是 本 的 是 是 是 。 。 。 。 。 。 ， 。 的 是 比 。 的 是 是 ， 。 。 。 。 。 。 。 。 。 。 。 。 日 和\n",
      "， 剛 來 台 灣 的 時 候 ， 吃 含 有 蒜 料 理 的 話 ， 我 的 胃 部 很 痛 ， 沒 辦 法 常 吃 ， 但 是 已 經 習 慣 了 ， 我 最 近 做 料 理 的 時 候 常 用 蒜 ， 薑 。 薑 ， 我 ， ， ， ， ， ， ， ， ， ， ， 吃 含 有 蒜 的 的 的 話 ， ， ， ， 會 ， 薑 ， 吃 吃 有 蒜 的 ， ， ， ， 我 常 用 用 ， ， 在 在 在 在 做 的 的 ， ， 用 用\n",
      "的 覺 得 中 國 食 物 的 特 色 是 顯 出 了 材 料 本 來 的 味 道 ， 反 而 韓 國 食 物 的 特 色 是 除 了 材 料 的 味 道 以 外 顯 出 其 他 的 東 西 的 味 道 。 我 我 我 我 我 是 了 了 了 的 韓 的 的 的 的 是 是 是 顯 顯 了 了 了 本 本 本 的 的 的 的 的 是 是 是 是 的 的 的 是 是 是 了 當 的 本 ， 以 ， ， ， 顯 的 的 的 的 的 的 韓\n",
      "的 為 我 覺 得 中 國 菜 蠻 油 膩 ， 還 有 聞 得 起 來 很 特 別 的 味 道 ， 沒 有 日 本 菜 的 那 麼 口 味 好 。 因 因 因 因 因 因 因 因 因 因 因 ， 因 因 很 蠻 蠻 蠻 蠻 因 因 因 因 因 我 因 ， 很 蠻 蠻 蠻 蠻 蠻 ， ， 是 聞 聞 聞 來 來 的 的 的 的 蠻 蠻 蠻 蠻 ， 很 ， 很 ， 的 很 的 的 的 的 。 。 因 。 。 因 我 因 。 因 。 我 ，\n",
      "的 不 好 的 菜 是 臭 豆 腐 ， 最 不 喜 歡 的 是 聞 ， 我 一 聞 就 想 吐 ， 味 道 也 不 太 好 。 這 個 菜 是 外 國 人 的 口 味 不 一 樣 的 菜 。 這 跟 跟 跟 跟 跟 ， ， ， 最 。 。 。 最 是 菜 是 臭 ， ， ， 最 最 最 歡 的 聞 聞 聞 聞 ， 聞 聞 聞 ， ， ， 我 我 我 ， 聞 。 ， 跟 跟 跟 跟 跟 跟 跟 ， ， ， ， ， 的 菜 菜 這 這 。\n",
      "。 國 菜 和 韓 國 菜 不 一 樣 。 中 國 菜 是 一 點 膩 ， 可 是 韓 國 菜 是 大 部 分 的 ， 一 點 辣 。 而 而 而 是 。 。 。 是 是 是 是 不 。 中 而 而 的 的 是 ， 。 ， ， ， 是 是 是 辣 和 中 的 的 不 ， 。 。 ， 國 菜 是 是 點 辣 點 辣 而 和 的 是 。 是 是 是 是 是 是 是 是 。 。 。 。 。 。 。 。 。 。 的 是 是 是 是 。 。 和\n",
      "我 常 常 跟 朋 友 一 起 逛 夜 市 ， 我 台 灣 人 的 朋 友 們 個 個 很 親 切 ， 每 次 介 紹 我 不 同 的 台 灣 小 吃 。 還 也 還 我 還 吃 吃 吃 吃 的 台 的 和 。 還 我 我 我 我 跟 ， 跟 常 跟 我 去 去 逛 逛 逛 ， ， 跟 跟 跟 的 的 ， ， 們 也 們 都 很 逛 。 。 ， 跟 跟 跟 給 我 的 ， ， ， 都 。 。 。 。 。 。 。 。 。 。 。 我 跟\n",
      "灣 覺 得 去 夜 市 讓 你 們 感 到 台 灣 的 氣 氛 ， 還 有 台 灣 很 有 名 美 味 的 食 物 ， 可 能 你 想 吃 的 東 西 什 麼 都 有 ！ 有 而 還 你 你 什 的 什 什 什 什 什 有 很 很 的 ， ， ， 去 ， 夜 讓 能 能 感 到 有 到 的 的 的 ， ， 有 有 有 很 很 很 能 很 到 有 很 的 的 ， 你 你 什 要 的 什 什 什 什 有 都 有 ！ 我 ！ ， ， 得 去\n",
      "。 一 點 我 忘 記 要 請 你 幫 我 帶 來 一 些 東 西 。 那 些 東 西 我 可 以 請 我 家 人 給 你 。 我 家 人 已 經 知 道 了 你 要 來 台 灣 所 以 他 們 也 很 開 心 。 我 點 我 點 。 。 。 我 我 我 我 要 我 我 。 我 帶 我 一 來 的 了 。 那 點 這 的 開 我 我 我 我 我 我 。 來 。 。 一 我 我 很 很 很 我 你 你 你 要 來 。 。 我 很 很 很\n",
      "她 們 這 樣 認 識 了 ， 可 是 她 原 來 我 們 系 的 學 姐 。 這 麼 巧 ！ 不 過 她 因 為 剛 轉 中 文 系 ， 而 且 也 剛 開 始 學 中 文 。 所 就 她 就 她 她 她 她 她 她 她 是 是 是 是 是 就 就 就 。 她 。 她 她 是 是 是 是 是 是 是 是 是 。 這 。 巧 啊 巧 ， ， 是 是 是 是 是 轉 轉 ， 我 且 且 且 且 剛 剛 學 學 的 我 。 我 就 我 就\n",
      "的 子 裡 布 置 的 家 具 跟 房 間 的 安 排 也 滿 不 錯 ， 我 還 好 滿 足 啦 。 房 房 房 家 房 布 佈 佈 佈 佈 ， 。 」 [UNK] 房 房 房 房 房 房 屋 房 佈 裡 佈 房 。 房 屋 房 房 房 ， 房 布 佈 布 置 佈 布 ， ， 跟 間 的 的 都 都 都 不 ， ， 佈 佈 放 就 布 跟 跟 ， 的 的 都 都 都 都 ， ， ， ， 也 ， 。 。 。 。 。 。 房 。 房 布 面 布\n",
      "。 日 本 也 可 能 住 很 久 的 話 ， 當 然 會 有 東 西 的 壽 命 。 可 是 好 像 台 灣 的 壽 命 比 日 本 短 很 多 。 還 而 而 台 也 也 也 也 也 很 很 很 很 唉 。 而 在 台 ， ， 會 會 有 日 也 住 很 住 住 住 話 話 ， 也 也 會 有 有 有 有 的 的 很 長 。 。 。 ， ， 台 的 的 要 要 很 要 很 很 很 很 。 。 。 。 ， 的 ， 。 。 。 要 短\n",
      "。 個 地 區 上 算 是 很 划 算 的 不 動 產 。 租 費 只 要 一 萬 塊 ， 不 過 設 備 都 全 美 無 瑕 。 再 加 上 還 有 一 個 在 台 灣 難 得 的 好 處 ： 木 頭 地 板 。 還 還 還 還 在 還 還 還 還 還 還 是 很 好 ， 的 不 。 。 。 。 金 租 費 只 有 一 ， 還 ， 還 。 。 ， 租 租 ， 。 費 還 還 它 還 還 還 還 還 很 一 很 很 的 的 ： ： 有\n",
      "我 我 房 子 樓 下 有 一 個 小 商 店 ， 商 店 裡 有 三 個 兄 弟 姊 妹 ， 我 常 常 跟 他 們 聊 天 。 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 我 ， 在 ， 我 在 我 在 我 的 我 有 有 小 小 商 ， ， 小 我 裡 有 有 我 兄 姊 姊 妹 ， 小 小 商 ， ， 我 我 我 。 我 。 。 姊 妹 。 。 。 。 。 。 。 。 。 。 。 。 在\n",
      "。 最 難 忘 的 一 個 人 是 我 的 奶 奶 。 現 在 他 沒 有 了 。 以 前 他 還 有 的 時 候 我 們 常 常 跟 他 一 塊 兒 去 買 東 西 ， 去 玩 兒 。 我 我 在 在 跟 ， ， ， 的 。 。 。 我 。 我 。 。 。 是 是 是 是 的 的 的 。 。 。 他 他 他 以 。 。 以 以 跟 我 的 的 。 。 。 。 。 會 會 去 一 們 ， 去 ， 買 ， 兒 ， 去 玩 他 。 。 。\n",
      "。 是 從 中 國 來 的 。 所 以 他 會 說 中 文 ， 他 說 的 很 好 。 他 常 常 教 我 們 說 中 文 。 他 說 我 們 要 會 說 中 文 ， 所 以 我 現 在 到 台 灣 來 學 中 文 。 他 他 他 我 他 我 他 他 他 來 他 。 他 他 他 他 他 他 他 他 他 的 的 的 的 來 學 。 他 他 他 他 他 說 他 他 他 他 他 我 要 要 要 要 說 。 。 。 我 說 我 要 要 要 來\n",
      "。 很 喜 歡 做 飯 ， 他 做 得 很 好 吃 。 她 每 天 做 飯 給 我 們 吃 。 我 們 很 喜 歡 他 做 的 菜 。 他 也 很 喜 歡 唱 歌 兒 ， 他 唱 得 不 錯 。 他 他 他 他 ， 很 很 好 好 。 她 也 他 他 他 他 ， 他 也 很 很 很 很 。 她 也 會 也 飯 給 他 。 我 。 很 很 很 很 也 。 她 他 。 好 。 他 也 也 很 唱 。 兒 兒 兒 ， 也 很 錯 好 他 他\n",
      "。 以 前 是 中 文 老 師 所 以 他 很 耐 心 。 我 們 都 很 愛 他 。 她 是 真 好 的 奶 奶 。 我 很 想 他 。 我 。 我 他 我 。 。 。 。 。 我 以 。 我 他 他 他 。 他 他 。 。 。 。 他 以 以 是 他 個 學 。 。 。 他 也 很 愛 。 。 也 都 都 很 她 。 。 。 。 。 他 很 。 。 。 。 我 她 。 。 。 。 。 。 。 。 。 。 。 。 。 。 以 他 是\n",
      "， 然 房 租 很 貴 ， 我 不 要 搬 家 ， 我 的 房 間 很 大 ， 也 有 客 廳 ， 再 加 上 房 東 對 我 很 好 。 雖 雖 雖 我 雖 我 雖 雖 雖 雖 雖 我 雖 雖 雖 雖 雖 但 雖 再 雖 我 雖 雖 雖 但 很 我 很 雖 ， 但 但 我 搬 。 。 但 雖 我 家 也 很 我 ， 但 ， 但 ， 我 。 再 。 。 我 我 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 我\n",
      "。 剛 到 台 灣 的 時 候 有 運 氣 認 識 一 個 要 搬 家 的 外 國 人 。 他 說 他 的 房 租 太 貴 ， 所 以 他 找 便 宜 的 房 子 。 他 他 他 說 說 說 說 就 找 找 租 租 ， 到 ， 識 到 識 他 在 到 ， 的 ， ， 有 ， 識 認 識 識 識 到 到 有 到 的 ， ， ， ， 有 ， 識 認 識 識 識 說 說 他 就 就 找 找 租 的 的 。 。 說 說 說 。 。 。 到 ，\n",
      "美 美 國 交 通 不 方 便 ， 地 鐵 從 我 家 很 遠 。 美 國 的 房 子 的 環 境 很 安 靜 ， 我 的 房 子 很 乾 淨 ， 可 是 房 租 比 台 灣 的 貴 。 美 美 美 我 美 我 也 也 也 也 比 還 還 還 也 的 也 不 不 ， ， 但 鐵 也 比 也 比 很 。 。 但 的 的 的 的 的 不 不 也 也 也 也 。 貴 也 我 不 不 我 。 我 我 ， 還 還 比 很 貴 貴 在 。 我 也\n",
      "房 們 有 三 間 房 間 ， 每 房 間 都 有 冷 氣 、 床 、 桌 書 、 椅 子 、 架 書 、 電 腦 。 他 它 他 在 有 ， 在 每 ， 的 。 也 。 。 他 他 共 有 有 有 。 ， 每 每 ， 每 ， ， ， 他 共 共 一 個 大 ， 每 每 每 每 都 都 有 、 。 。 。 、 和 的 的 每 每 每 每 每 都 都 有 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 的 兩\n",
      "， 搬 家 以 前 住 在 新 店 山 上 的 國 際 學 舍 。 因 為 山 上 ， 空 氣 很 好 ， 可 以 看 到 山 下 的 城 市 的 夜 景 ， 而 且 學 舍 有 管 理 員 而 比 較 安 全 。 因 在 在 在 是 是 是 ， ， 是 是 在 新 店 的 的 的 的 際 宿 的 ， ， 在 在 在 是 的 是 的 的 的 的 的 ， ， ， 在 ， ， 的 的 的 ， ， ， 且 在 且 有 管 管 管 ， ， ，\n",
      "的 為 台 灣 的 氣 候 很 濕 的 關 係 ， 浴 室 裡 還 有 衣 櫃 裡 有 一 點 發 黃 ， 洗 不 掉 。 因 因 因 因 因 因 因 因 因 。 因 因 因 因 因 因 因 因 因 因 因 。 ， ， ， 因 因 因 因 因 因 ， 的 的 ， ， ， 的 關 ， ， 在 為 裡 還 ， 的 ， ， 會 ， ， ， 濕 關 ， ， ， ， ， 。 因 。 。 ， ， 。 。 黑 。 。 。 。 。 。 因 。 。 ，\n",
      "的 的 地 方 應 有 安 全 感 。 房 子 和 學 校 的 距 離 不 要 太 遠 ， 走 路 就 可 以 到 學 校 的 等 等 。 住 住 住 住 住 在 住 住 住 住 住 。 不 在 不 在 在 在 住 在 住 在 住 住 住 的 不 ， 應 在 有 。 。 。 房 房 和 和 房 學 不 不 不 不 不 不 太 。 。 。 在 房 。 和 。 。 不 不 不 不 不 。 。 。 。 。 。 。 。 。 。 。 。 不 應\n",
      "。 覺 得 三 重 市 的 環 境 不 太 好 。 因 為 我 住 的 地 方 是 郊 區 ， 所 以 還 不 發 展 。 我 我 我 說 說 。 。 也 。 。 也 。 我 我 而 而 說 說 的 的 的 也 也 也 很 。 。 。 。 我 得 市 重 市 的 的 也 也 太 好 。 又 為 和 住 的 住 的 郊 市 的 的 也 也 很 很 。 。 。 和 在 住 的 是 是 是 。 。 。 。 。 。 。 。 。 。 在 得 市\n",
      "房 動 產 公 司 幫 房 東 找 房 客 的 話 ， 房 東 不 得 要 算 手 續 費 。 2 2 2 而 而 ， 在 在 要 要 ， 續 。 而 而 而 。 而 在 在 ， 要 在 要 要 ， 。 。 。 而 。 ， ， ， ， ， ， 要 要 要 要 找 ， 找 找 客 時 話 ， ， ， 就 要 要 付 付 幫 找 找 ， ， ， ， ， ， ， ， 會 要 付 付 交 手 續 。 。 。 。 。 。 。 。 。 。 。 要\n",
      "房 果 那 個 房 子 不 滿 意 的 話 ， 再 從 第 一 次 去 找 。 再 3 如 再 如 。 再 。 再 去 去 尋 找 。 如 。 如 。 。 。 。 如 。 如 。 找 。 如 如 如 。 如 。 如 ， 如 ， 如 ， 如 對 找 對 你 你 不 不 ， ， ， ， 再 再 再 去 來 去 去 。 如 ， 。 不 ， 。 ， ， ， 再 再 去 去 找 尋 。 。 。 。 。 。 。 。 。 。 。 。 如 。 如 對\n",
      "房 實 ， 除 了 房 子 大 小 跟 房 租 之 外 ， 兩 個 房 子 裡 面 的 東 西 都 差 不 多 。 其 其 其 其 其 ， ， ， 。 。 。 。 其 其 其 其 ， 其 ， 其 ， ， 其 ， 其 ， 之 之 之 其 其 其 ， ， 了 房 的 之 之 之 房 之 之 之 之 ， ， 這 個 的 的 的 的 之 之 之 之 之 之 之 其 ， ， ， ， ， 的 ， ， ， 。 ， 。 之 。 其 。 其 其 ， ，\n",
      "我 常 常 去 我 家 附 近 的 餐 廳 吃 包 子 和 蛋 花 湯 吃 飯 ， 那 裡 的 店 員 已 經 認 識 我 ， 所 以 每 次 我 去 跟 他 們 聊 天 。 也 也 也 也 ， 都 都 都 都 廳 吃 吃 吃 ， 吃 和 和 ， 在 我 在 的 的 的 的 廳 吃 吃 吃 、 、 包 湯 的 和 和 在 的 的 廳 的 ， 吃 吃 煮 和 和 ， ， ， 都 都 都 都 都 都 都 都 聊 。 。 在 我 在 在 在\n",
      "的 市 裡 有 各 式 各 種 的 地 攤 ， 小 吃 的 ， 用 的 ， 穿 的 什 麼 都 有 。 市 在 在 在 。 。 市 。 。 ， 。 。 。 。 。 在 。 。 。 。 。 市 。 的 。 。 。 。 。 在 。 。 。 。 ， 裡 有 各 ， ， ， 的 。 攤 ， ， ， ， ， 玩 玩 穿 穿 的 ， ， ， 。 。 。 。 。 ， ， 。 。 ， 的 的 ， 的 。 。 。 。 。 。 。 。 。 。 。 。 有\n",
      "信 天 早 上 我 非 常 的 高 興 ， 因 為 我 今 天 收 到 了 信 。 我 的 最 好 的 朋 友 ， 她 的 名 字 叫 美 真 ， 她 和 我 一 樣 已 經 二 十 三 歲 。 今 她 她 她 她 她 我 也 是 我 今 我 我 ， 我 非 的 的 的 的 ， ， ， 我 我 我 我 我 了 。 。 。 。 的 的 最 ， ， ， 是 ， 的 她 ， 美 美 美 ， ， ， 和 我 已 經 是 經 是 了 了 她 我\n",
      "。 你 到 台 灣 我 會 帶 你 去 花 蓮 。 你 你 你 想 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 到 。 ， 。 。 。 。 。 看 。 。 」 。 。 。 。 。 ， ， ， ， 陪 陪 去 看 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 ， 。\n",
      "。 的 好 朋 友 跟 我 說 他 可 以 借 給 我 汽 車 。 所 以 很 方 便 。 我 們 在 那 裡 可 以 釣 魚 ， 還 有 在 海 邊 玩 。 還 還 在 釣 釣 釣 。 ， 還 在 在 在 。 還 我 我 他 借 借 的 。 。 。 。 。 他 。 。 說 以 借 借 我 的 的 。 。 。 很 很 很 。 。 。 還 還 借 借 釣 釣 釣 釣 ， 還 在 在 在 在 。 。 在 在 在 在 釣 。 。 。 。 。\n",
      "天 灣 的 天 氣 會 容 易 變 ， 還 有 常 常 會 下 雨 。 所 以 你 一 定 要 帶 長 袖 ， 免 得 會 感 冒 ！ ps 灣 還 ！ 台 ~ ！ 還 ！ 會 你 喔 ！ ~ ！ 灣 灣 還 ！ ！ 台 ！ 台 還 也 台 雨 氣 天 氣 氣 很 改 變 冷 還 也 也 也 也 會 ！ ！ 喔 ！ 你 氣 要 要 要 ！ 還 也 也 會 會 會 ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ 氣 天\n",
      "。 來 台 灣 以 後 我 想 跟 妳 一 起 去 陽 明 山 。 在 山 上 風 景 很 美 。 我 們 可 以 一 邊 吃 野 餐 一 邊 欣 賞 風 景 。 很 我 在 在 在 在 。 。 一 賞 賞 賞 賞 跟 我 我 我 去 。 。 。 。 以 。 我 想 跟 妳 們 去 去 去 去 看 。 。 。 。 。 。 。 。 跟 跟 去 去 去 去 。 。 。 。 。 。 。 看 賞 賞 。 。 。 在 。 。 。 。 。 。 。\n",
      "。 台 灣 的 交 通 很 方 便 。 我 們 可 以 坐 公 車 去 。 這 個 時 候 你 的 天 氣 變 化 很 大 ， 所 以 衣 服 你 應 該 多 帶 一 點 。 去 我 你 你 你 你 你 你 也 多 多 一 。 這 去 去 這 這 的 你 也 也 。 。 。 你 你 你 你 你 去 到 。 。 。 。 。 個 。 。 。 你 變 會 變 會 。 。 我 你 你 你 也 要 要 多 一 。 一 。 。 。 。 。 。 的\n",
      "你 們 先 去 台 灣 的 政 府 在 印 度 ， 帶 你 們 的 護 照 ， 如 果 他 們 給 你 簽 發 ， 你 再 可 以 去 台 灣 。 你 你 你 你 你 ， 你 你 你 再 去 去 ， 去 你 你 你 在 印 度 ， ， 你 ， 你 去 ， ， ， ， 在 在 印 度 印 ， 帶 帶 的 的 的 ， 去 。 ， ， 印 度 印 印 ， ， ， 再 再 去 去 去 。 。 。 。 。 給 。 ， ， ， ， 去 去 去 去\n",
      "， 現 在 台 北 天 氣 不 錯 ， 你 們 不 用 帶 很 多 冬 天 衣 ， 現 在 台 北 的 天 氣 跟 印 尼 一 樣 ， 我 們 可 以 適 應 了 。 現 現 現 我 現 現 現 現 我 我 我 現 現 現 現 現 現 我 現 現 現 ， ， 也 很 ， ， 你 也 不 不 穿 穿 多 冬 天 的 ， ， ， ， ， 你 你 你 不 印 印 印 度 度 ， 們 都 感 就 就 了 。 現 現 現 印 度 尼 台 灣 ，\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "。 要 帶 你 們 去 阿 里 山 ， 在 臺 灣 很 有 名 的 地 方 。 不 但 帶 你 們 欣 賞 看 山 ， 我 也 打 算 帶 你 們 去 花 蓮 吃 好 吃 的 東 西 ， 看 海 邊 。 看 花 去 花 蓮 很 很 很 很 很 很 ， 去 看 看 看 是 ， 是 很 很 很 很 很 有 很 的 ， 看 ， 去 看 是 是 是 很 很 很 很 很 ， 看 花 帶 帶 花 花 蓮 蓮 蓮 蓮 蓮 的 的 ， 看 看 看 去\n",
      "。 「 我 要 給 你 們 看 台 北 的 夜 市 。 我 們 一 起 逛 街 ， 買 一 些 東 西 ， 試 試 看 台 灣 的 小 吃 ， 尤 其 是 ： 臭 豆 腐 。 」 」 」 ── ── ， ， 看 ： ： ： ： 和 和 」 」 」 ， ， ， ， 看 看 一 看 的 的 的 。 。 跟 一 要 去 去 ， ， 一 一 一 的 的 的 和 。 。 看 。 的 的 的 。 。 ： ： ： ： 和 豆 。 。 。 。 。 ， ，\n",
      "。 我 來 的 時 候 ， 我 的 計 畫 是 要 帶 您 去 臺 中 。 那 邊 的 風 景 好 漂 亮 。 我 們 可 以 爬 山 。 臺 灣 有 很 多 山 。 我 們 到 臺 灣 坐 高 鐵 去 。 再 坐 坐 們 到 您 帶 您 回 您 。 的 回 ， 我 的 我 ， 是 您 帶 您 您 您 去 。 。 。 我 。 的 。 很 是 。 您 。 您 要 去 去 。 。 。 。 有 有 很 。 。 。 到 到 坐 坐 坐 坐 坐\n",
      "。 在 是 夏 天 ， 所 以 不 要 帶 很 多 衣 服 。 這 裡 賣 的 衣 服 很 便 宜 ， 等 一 下 我 們 買 幾 件 。 現 現 現 在 現 在 現 現 。 我 我 。 這 在 現 在 現 現 現 現 在 。 。 。 現 現 現 在 現 現 你 你 你 要 帶 帶 多 的 。 。 這 裡 裡 裡 賣 裡 也 我 你 要 帶 帶 。 。 。 現 。 這 。 這 。 。 。 。 。 。 。 。 。 。 。 裡 這 裡 。\n",
      "我 小 到 大 ， 我 媽 媽 育 養 我 們 很 辛 苦 ， 是 因 為 我 家 裡 說 起 來 也 比 一 般 窮 ， 而 且 家 裡 有 五 個 小 孩 。 我 我 我 我 我 我 我 我 我 我 我 ， 我 我 我 我 我 辛 ， ， 從 ， ， 我 說 說 都 得 我 我 很 辛 辛 辛 ， 是 是 ， ， 我 我 說 都 我 我 辛 很 辛 人 窮 我 我 我 我 有 我 小 小 我 我 我 我 我 我 。 。 我 。 我\n",
      "。 想 告 訴 大 家 要 好 好 的 去 照 顧 自 己 的 媽 媽 ， 要 懂 得 孝 順 父 母 。 也 也 也 也 也 想 ， ， 要 要 的 的 的 。 要 。 。 。 。 。 也 ， ， ， ， 的 的 的 的 的 的 ， ， ， ， ， ， ， ， 的 的 的 的 的 去 好 好 好 的 和 ， ， ， 要 的 的 的 的 的 的 顧 好 好 的 的 媽 ， ， 要 要 的 的 的 的 。 。 。 。 。 。 。 ， ，\n",
      "。 是 個 既 溫 柔 又 幽 默 的 人 ， 在 友 圈 裡 沒 有 一 個 不 喜 歡 他 的 人 。 他 他 他 他 他 他 他 很 很 他 他 他 他 他 他 他 他 他 他 他 他 很 很 很 很 默 默 他 他 他 他 他 他 他 是 很 有 有 又 幽 默 默 默 ， 朋 朋 朋 也 也 也 是 是 不 很 又 有 默 默 默 ， 朋 朋 朋 裡 ， ， 是 是 不 不 說 他 。 。 。 。 他 。 。 。 個 有 很\n",
      "的 有 一 點 讓 我 更 欣 賞 他 的 就 是 很 孝 順 父 母 ， 也 總 是 以 樂 觀 的 情 形 看 待 每 件 事 情 。 而 而 而 而 而 他 他 他 他 ， ， 。 而 而 而 我 很 欣 賞 賞 ， ， ， 他 他 ， ， ， ， 我 更 欣 賞 賞 ， ， ， 他 他 很 的 的 ， 他 他 我 更 欣 賞 賞 ， ， ， 他 很 他 的 父 而 而 他 很 他 賞 他 ， 他 ， 去 ， 。 。 他 ， 我\n",
      "、 我 心 情 很 低 落 時 ， 他 會 安 慰 我 ， 使 我 有 了 更 多 的 自 信 、 也 不 斷 地 鼓 勵 我 要 做 一 個 堅 強 的 人 ， 別 輕 易 放 棄 。 3 當 我 在 他 他 他 他 會 會 會 鼓 當 當 在 在 在 很 ， ， 他 他 會 會 會 我 並 鼓 我 我 我 有 了 更 更 ， 會 會 會 會 會 鼓 鼓 鼓 鼓 ： ： ， 要 要 、 、 ， 要 不 別 輕 地 鼓 [UNK] [UNK] 在 。\n",
      "他 從 我 認 識 他 以 來 ， 我 的 人 生 也 幾 乎 完 全 的 改 觀 ， 再 也 不 以 悲 觀 的 心 情 對 待 人 生 。 他 他 他 他 他 他 他 他 他 我 去 去 他 他 他 但 但 他 ， ， ， ， ， ， ， 我 認 他 他 他 ， ， 我 的 的 的 就 就 就 被 的 的 的 的 他 ， ， ， 我 的 的 的 ， 的 在 去 的 不 的 他 他 他 他 我 也 以 在 的 的 去 的 。 他 他\n",
      "。 最 欣 賞 的 一 個 人 是 紐 約 洋 基 隊 ＂ 台 灣 之 光 ＂ 王 建 民 。 他 現 在 不 但 亞 洲 人 最 棒 的 投 手 ， 而 且 洋 基 隊 的 王 牌 投 手 。 是 他 是 是 是 是 是 的 基 的 的 的 棒 的 的 ， ， ， ， ， 約 的 基 的 的 的 灣 的 的 的 的 的 的 。 是 是 的 的 的 的 是 是 是 的 的 的 的 是 是 是 是 是 是 王 最 牌 一 牌 最 他 他\n",
      "。 得 高 高 瘦 瘦 的 ， 臉 有 點 像 金 正 武 的 ， 他 不 是 別 人 而 是 我 親 哥 哥 。 長 長 我 長 我 ， 。 我 ， ， 我 ， 。 小 長 。 長 我 長 ， ， ， 。 ， ， ， ， ， 。 正 長 ， 長 ， ， ， ， ， ， ， 點 點 黃 正 正 正 剛 。 。 。 ， ， 。 ， ， ， 點 像 是 正 正 。 。 。 。 。 。 。 。 。 。 。 。 。 正 。 。 。 ， ， ，\n",
      "我 我 的 朋 友 跟 我 說 我 爸 爸 之 前 救 他 過 他 的 命 ， 要 不 然 他 那 時 候 就 被 壞 人 殺 死 了 。 我 我 我 我 我 我 我 我 我 我 我 。 我 我 我 我 我 我 我 說 我 說 還 跟 我 我 我 我 我 我 我 ， 我 說 在 死 沒 沒 要 要 要 的 命 的 ， 我 我 我 我 說 說 說 沒 還 要 要 的 他 。 我 我 我 我 說 說 說 說 沒 要 要 給 。 我 我 我\n",
      "他 覺 得 我 旁 邊 就 有 他 ， 如 果 不 在 我 身 邊 ， 但 是 也 算 是 在 我 心 裡 ， 於 是 我 應 該 會 過 好 好 過 的 日 子 。 好 好 好 好 好 好 好 好 好 過 過 過 過 他 他 他 他 他 他 好 他 很 很 就 有 他 ， 他 他 他 他 他 在 他 ， 他 他 他 他 他 過 他 他 他 他 他 他 他 好 好 好 好 好 過 過 過 過 過 過 。 。 [UNK] 。 。 想 他 想 他\n",
      "， 如 說 從 悲 觀 的 一 個 人 變 成 一 個 很 樂 觀 的 人 ， 短 視 的 思 考 變 成 遠 視 的 思 考 。 比 比 悲 比 。 。 。 。 ， 。 。 。 。 。 比 悲 悲 從 悲 悲 ， ， ， ， 悲 很 悲 悲 悲 很 悲 悲 悲 的 ， 人 變 變 變 很 很 很 很 的 人 ， 樂 悲 悲 的 ， ， 變 變 很 悲 很 的 。 。 。 。 。 。 。 。 。 變 。 。 。 。 。 。 。 悲 很\n",
      "。 很 想 帶 你 去 看 台 灣 的 風 景 ， 還 有 帶 你 吃 最 好 吃 的 台 灣 菜 。 台 灣 菜 又 好 吃 又 便 宜 。 而 吃 吃 灣 灣 灣 灣 灣 。 又 ， 又 又 。 。 吃 。 。 。 台 灣 。 。 想 。 。 帶 你 去 吃 吃 吃 的 美 。 ， ， 想 想 你 吃 最 吃 最 吃 吃 吃 。 。 。 。 。 灣 想 想 。 最 最 最 。 。 。 。 。 。 。 。 。 。 。 。 。 。 吃\n",
      "。 可 以 安 排 很 多 的 東 西 也 可 以 帶 他 去 玩 一 玩 、 參 觀 參 觀 ， 她 想 做 什 麼 我 就 帶 她 去 做 什 麼 。 等 她 她 她 她 她 就 她 陪 她 去 做 等 等 她 也 也 你 她 帶 她 ， 她 去 她 她 她 的 。 ， ， 。 以 帶 他 去 逛 、 看 、 參 、 參 觀 。 。 。 也 是 帶 ， 我 就 她 去 去 做 。 。 。 。 。 。 。 。 。 。 。 。 以 去\n",
      "。 hello ， 你 們 好 ？ 我 是 政 治 。 我 來 台 灣 十 個 多 月 了 。 我 說 中 文 說 得 越 來 越 好 。 我 ， 我 ， ， ， ， 也 說 ， 也 。 。 我 hello hello 我 我 ， ， 府 府 說 治 治 ， 政 hello ， hello 們 好 ， ， 治 政 治 治 治 在 我 在 十 十 十 。 政 好 。 我 治 政 治 治 治 在 。 有 。 十 。 。 。 。 。 。 治 治 。 。 。 。 。 。 hello ， 大\n",
      "。 且 我 知 道 你 特 別 喜 歡 動 物 。 那 我 們 一 起 去 動 物 園 吧 。 聽 說 台 北 的 動 物 園 是 亞 洲 最 大 的 。 而 而 而 而 也 園 園 園 是 是 、 最 的 。 。 那 而 。 。 。 而 。 且 。 。 。 也 也 喜 。 動 。 。 那 那 那 就 就 去 去 去 。 。 。 。 。 。 。 。 園 園 園 園 園 是 是 最 。 。 。 。 。 。 。 。 。 。 。 園 。 。\n",
      "機 回 來 那 一 天 舅 舅 會 叫 我 接 你 們 ， 我 們 先 把 行 李 放 好 再 去 出 吃 飯 ， 反 正 飛 機 上 的 東 西 不 好 吃 ， 你 們 很 餓 吧 ！ 回 回 回 回 我 說 你 說 也 也 也 也 回 回 回 的 ， ， ， 說 會 叫 叫 讓 來 接 很 餓 再 回 回 的 ， ， ， 叫 叫 叫 讓 來 門 接 也 我 是 我 的 的 的 也 都 不 ， 你 就 你 也 很 餓 回 回 來 的\n",
      "的 二 天 氣 好 的 話 可 以 去 淡 水 玩 ， 那 邊 的 風 景 還 不 錯 ， 而 且 吃 的 東 西 多 ， 我 真 的 愛 惜 臺 灣 的 傳 統 美 食 尤 其 是 這 邊 的 小 吃 棒 極 了 。 第 三 第 三 第 一 一 很 也 ， 話 ， 就 去 去 玩 玩 ， ， ， 的 的 的 還 還 還 好 ， 去 去 去 ， ， 很 很 的 真 的 的 的 的 ， 惜 的 的 ， ， ， ， ， ， ， 的 ， ，\n",
      "， 了 吃 東 西 以 外 ， 我 們 也 可 以 參 觀 一 零 一 ， 中 正 紀 念 堂 什 麼 的 ， 只 怕 媽 媽 會 覺 得 無 聊 。 」 我 」 除 我 除 我 除 很 很 很 很 無 趣 」 除 除 除 除 除 ， 除 ， ， 吃 趣 無 趣 ， 我 ， 還 還 去 參 看 三 零 一 ， ， ， 台 一 館 ， ， 還 還 會 參 ， 我 會 很 很 很 無 趣 無 。 。 我 。 。 ， ， ， ， 會 ， 了\n",
      "。 照 我 的 計 畫 你 們 有 什 麼 意 見 ？ 請 告 訴 我 你 們 還 想 去 哪 裡 ， 我 才 能 安 排 好 。 謝 我 我 我 我 我 我 安 能 安 安 安 安 按 按 按 按 ， ， ， 能 能 有 安 能 安 按 ， 我 的 ， ， ， 們 有 能 麼 好 好 ？ 我 我 訴 我 ， 我 還 還 還 想 會 能 有 安 我 安 安 我 。 。 。 。 我 還 想 。 。 我 。 我 安 我 我 我 。 計 的\n",
      "！ 而 你 在 台 灣 只 能 過 三 天 兩 夜 ， 不 過 我 已 經 好 好 的 安 排 好 了 帶 你 去 故 宮 博 物 館 你 知 道 吧 ！ 來 臺 灣 一 定 要 去 的 ！ 帶 ~ 帶 是 是 ！ ！ ！ ， ， ！ ！ 帶 ！ ！ ， ， ， ， 安 玩 過 ， ！ ！ ！ ！ ！ 帶 ！ 而 ！ 安 安 安 安 安 的 ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ， 一 你 一 去 ！ ！ ！ ！ ！\n",
      "。 下 個 月 我 等 你 們 來 。 我 捨 不 得 過 日 子 因 為 我 真 想 你 們 了 。 我 我 我 我 。 。 我 。 。 我 。 。 我 我 我 我 還 我 。 。 。 。 。 我 。 。 。 我 我 我 捨 我 捨 我 。 。 。 我 會 著 們 來 還 我 恨 捨 捨 捨 得 子 。 。 。 真 真 不 。 來 還 我 捨 捨 捨 捨 過 。 。 。 。 。 真 。 。 。 我 。 捨 。 。 。 。 。 。 。\n",
      "， 來 台 灣 沒 有 去 上 中 文 課 ， 她 買 書 回 家 自 己 學 ， 很 用 功 地 學 ， 三 四 年 後 ， 她 的 文 章 ， 現 在 她 不 是 很 會 講 而 看 報 紙 像 臺 灣 人 的 速 度 。 她 她 她 她 她 她 她 ， 她 她 她 ， ， ， 她 就 她 買 書 ， ， ， 像 是 是 她 的 速 她 她 她 她 她 ， 她 的 的 章 章 ， 章 是 是 是 不 是 講 ， ， ， ， ， 是\n",
      "越 覺 得 中 文 可 以 跟 台 灣 人 溝 通 ， 她 開 始 做 生 意 。 她 在 桃 園 開 一 家 越 南 店 ， 賣 越 南 菜 ， 生 意 很 好 ， 客 人 大 部 分 是 越 南 外 勞 。 她 她 她 她 她 她 她 她 用 用 用 人 也 跟 跟 跟 人 人 ， ， 她 就 就 就 做 做 做 人 她 在 跟 的 人 人 做 做 做 賣 賣 賣 賣 賣 菜 的 也 也 也 也 ， ， 的 大 也 是 是 是 外\n",
      "。 什 麼 他 是 我 最 難 忘 的 人 ？ 因 為 他 是 我 的 初 戀 。 他 去 美 國 以 後 ， 一 次 也 沒 看 到 他 。 可 是 我 還 是 好 想 他 。 可 因 可 可 可 他 我 我 我 。 他 好 他 他 他 他 他 他 他 他 ？ 他 ？ ？ 他 。 他 他 他 他 他 他 他 。 。 。 。 。 呢 。 他 。 他 他 他 他 他 他 他 他 。 我 我 我 還 好 好 好 想 他 他 他 他 。 他\n",
      "人 約 兩 年 前 ， 我 跟 朋 友 去 綠 島 旅 行 時 ， 遇 到 了 綠 島 的 空 軍 軍 人 ， 他 們 就 是 最 難 忘 的 人 。 在 在 在 我 在 我 在 大 在 我 我 我 的 在 在 在 在 在 在 在 。 ， ， ， ， ， ， 在 跟 在 在 在 在 島 。 ， ， ， ， 了 了 了 了 了 的 在 在 在 在 在 。 ， 。 我 了 了 了 了 了 。 在 在 。 。 。 。 。 。 。 了 了 了\n",
      "。 不 會 忘 記 那 天 的 事 情 。 我 最 愛 的 哥 去 世 了 ， 也 跟 我 爸 見 過 面 。 但 我 們 並 沒 有 打 招 呼 。 過 了 那 天 他 也 沒 有 出 現 在 我 們 的 見 面 了 。 就 但 我 爸 哥 也 。 。 。 。 。 我 我 。 我 的 我 的 哥 也 哥 了 ， 想 他 我 爸 說 了 說 。 的 的 哥 他 也 現 再 再 在 。 。 。 。 來 ， ， 再 再 出 現 現 現 在\n",
      "我 完 歌 之 後 他 們 還 送 了 我 一 隻 很 大 的 無 尾 熊 ， 我 簡 直 太 感 動 了 ， 高 興 得 無 法 控 制 我 的 眼 淚 流 出 來 。 」 之 在 之 之 之 我 之 還 要 要 了 [UNK] 之 在 之 之 之 之 之 後 之 們 還 還 還 我 我 我 隻 隻 很 無 小 無 熊 ， 之 還 還 還 還 我 我 我 很 我 得 得 我 要 我 ， ， 要 要 要 了 了 了 我 我 [UNK] 歌 之 歌 之\n",
      "。 後 我 們 還 拍 了 不 少 照 片 ， 玩 的 很 開 心 。 我 心 裡 真 的 很 感 謝 他 們 幫 我 慶 祝 這 麼 難 忘 的 生 日 。 最 最 最 後 後 後 後 我 我 我 我 。 」 最 後 然 後 後 的 後 的 這 後 還 拍 也 玩 的 ， 玩 玩 玩 的 的 的 心 的 我 我 我 我 ， 。 。 。 真 的 ， ， ， 我 這 這 我 我 。 的 啊 最 最 最 後 。 。 ， 。 。 。 。 我\n",
      "的 一 年 可 是 我 最 難 忘 的 生 日 ， 我 也 會 好 好 地 珍 惜 我 們 的 友 誼 。 [UNK] [UNK] [UNK] [UNK] 。 。 。 。 ， ， 的 誼 吧 。 [UNK] [UNK] [UNK] [UNK] 這 。 會 。 。 。 們 。 。 誼 。 。 這 這 這 是 是 是 是 是 最 最 我 的 日 日 ， 我 會 會 去 去 地 去 地 。 好 的 的 的 。 。 。 我 會 會 去 去 去 地 ， 惜 。 的 誼 誼 。 。 。 。 。 。 。 。 是\n",
      "。 到 後 馬 上 報 了 名 旅 行 團 。 旅 行 團 的 行 程 雖 然 很 緊 湊 ， 可 是 不 會 辛 苦 。 也 可 以 吃 中 國 的 色 香 味 俱 全 的 菜 和 看 各 式 各 樣 的 中 國 的 文 物 。 還 也 然 後 也 是 報 了 一 一 的 團 。 這 這 艱 的 艱 ， 也 也 很 很 很 一 了 的 很 很 辛 辛 辛 很 也 很 很 吃 吃 吃 吃 味 味 俱 味 俱 的 ， 和 觀 看 各 看\n",
      "。 完 了 後 我 才 發 現 在 附 近 有 很 多 擺 地 攤 。 那 些 地 方 我 可 以 看 到 名 牌 的 。 而 且 沒 想 到 那 麼 便 宜 的 。 而 。 我 。 我 我 我 我 還 是 是 。 。 。 。 還 。 擺 。 。 。 。 。 附 附 附 附 近 還 還 很 擺 擺 擺 的 的 在 在 在 附 附 附 附 近 還 。 擺 擺 擺 。 。 我 我 能 是 是 是 便 。 。 。 。 擺 。 。 。 。 。\n",
      "。 花 了 很 多 錢 買 名 牌 的 包 包 。 可 是 不 到 一 個 小 時 就 破 壞 了 。 回 家 後 我 到 百 貨 公 司 去 才 知 道 我 買 的 是 真 的 仿 冒 品 ！ 而 且 沒 有 保 證 書 。 還 是 是 是 是 是 是 是 也 了 了 的 包 。 。 。 是 只 是 不 到 一 就 它 就 就 是 的 。 。 。 後 後 到 到 的 的 去 去 。 。 是 是 是 是 是 是 是 的 是 ！ ！ 是\n",
      "我 痛 下 決 心 瘦 身 ， 我 要 使 全 世 界 刮 目 相 看 ， 不 會 再 讓 任 何 人 罵 我 胖 。 我 我 我 我 ， ， [UNK] ， ， ， ， ， [UNK] [UNK] [UNK] [UNK] 。 我 我 我 要 ， ， ， ， ， ， ， 。 我 我 要 要 要 ， ， ， 我 要 使 使 對 都 對 都 ， ， ， 要 再 再 再 再 讓 讓 讓 對 對 都 為 。 。 。 。 。 再 讓 讓 讓 讓 讓 說 說 。 。 。 。 。 。 ，\n",
      "。 以 ， 媽 媽 死 了 以 後 他 們 的 家 一 點 都 沒 有 音 樂 。 因 為 他 們 的 爸 爸 想 音 樂 的 時 候 ， 一 定 想 太 太 。 所 所 所 所 所 所 所 ， 會 會 會 太 所 所 所 ， 所 也 都 都 都 ， ， 死 了 以 ， ， ， 的 家 一 也 都 都 都 都 有 都 ， ， ， ， ， ， 的 一 也 ， 的 的 一 ， 一 一 會 會 她 太 所 所 所 。 所 。 所 所 ， ，\n",
      "。 她 是 一 個 又 可 愛 又 漂 亮 的 機 器 女 人 （ 其 實 有 雞 蛋 的 樣 子 ） 。 她 也 不 會 說 話 。 所 以 ， 第 一 個 鐘 頭 裡 連 一 個 句 子 都 沒 有 。 她 她 她 她 她 她 的 。 她 她 她 她 很 漂 漂 又 漂 亮 亮 亮 小 的 。 的 字 話 她 她 很 漂 漂 又 漂 亮 亮 亮 她 不 不 。 。 。 她 在 在 在 在 鐘 頭 ， 頭 她 ， 一 的 的 說 。\n",
      "。 最 喜 歡 的 電 影 是 「 龍 貓 」 。 它 是 日 本 宮 崎 駿 的 漫 畫 。 是 是 是 是 是 是 是 是 的 的 。 。 。 它 。 。 。 。 。 。 。 是 。 。 。 。 。 。 是 。 。 。 。 。 。 。 。 。 是 是 是 是 是 是 是 貓 貓 。 。 是 是 是 由 的 的 的 是 是 是 是 是 貓 貓 。 。 是 是 是 本 的 的 的 的 。 。 。 。 。 。 。 是 。 。 最 的 是\n",
      "。 代 大 約 ５０ 年 前 日 本 。 有 兩 個 姐 妹 關 於 媽 媽 住 的 醫 院 的 關 係 搬 過 來 鄉 下 。 父 親 是 一 個 大 學 的 教 授 。 他 們 開 始 住 在 鄉 下 的 很 舊 的 家 屋 。 在 大 大 大 在 ５０ ５０ ５０ 。 。 。 。 。 。 。 為 因 於 因 於 因 在 住 在 。 。 。 。 。 。 。 。 。 。 於 是 是 住 是 大 的 人 。 。 子 。 大 也 大 是 大 在\n",
      "。 一 天 兩 個 姐 妹 看 奇 妙 的 動 物 。 然 後 遇 到 不 可 思 議 的 體 驗 。 看 這 個 電 影 。 我 想 出 來 小 時 候 的 鄉 下 的 感 覺 。 讓 讓 讓 我 了 了 在 在 在 在 。 。 。 。 。 。 一 個 一 去 去 看 看 的 的 。 。 。 。 就 就 了 了 不 的 的 去 看 看 的 的 。 。 。 。 。 讓 讓 我 了 了 了 在 鄉 鄉 鄉 下 的 。 。 。 。 。 。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "。 外 公 住 在 一 個 小 島 的 河 邊 。 國 小 的 暑 假 跟 寒 假 的 時 候 我 跟 家 人 找 他 。 在 在 在 在 在 。 。 跟 。 。 跟 跟 。 跟 。 跟 跟 跟 在 在 在 在 。 。 。 。 。 跟 跟 跟 跟 住 在 在 在 小 河 的 河 邊 邊 在 在 很 的 的 跟 。 在 在 。 的 河 上 河 河 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 在 。 。 。 。 。 。 。\n",
      "。０９ 年 １２ 月 「 葉 問 」 就 開 場 了 。 我 一 部 電 影 在 雅 加 達 印 尼 ， 我 第 一 次 看 到 部 電 影 的 海 報 的 時 候 ， 我 以 為 部 電 影 是 超 人 故 事 。 那 但 那 那 那 部 那 部 ， ， ， 葉 問 」 就 要 開 了 。 第 一 部 一 部 是 是 在 在 ， ， ， ， 。 我 在 看 這 這 這 這 這 的 的 的 的 的 ， ， 我 為 部 這 部 是 是\n",
      "。 這 是 什 麼 電 影 啊 ？ 結 果 我 跟 朋 友 們 一 起 看 部 電 影 「 敢 問 」 。 請 問 我 問 這 問 這 。 。 。 問 請 。 我 問 這 問 這 問 這 問 這 。 。 。 請 問 問 。 這 說 這 知 這 。 這 是 ？ ？ 啊 ？ 」 是 ， 跟 跟 跟 去 看 去 看 看 看 。 來 無 無 請 。 ， 。 我 跟 看 。 看 。 看 。 「 無 無 。 。 。 。 。 問 。 知 這 是 這\n",
      "華 部 電 影 讓 所 有 的 華 人 要 自 信 更 加 油 站 出 來 照 顧 華 人 的 文 化 ， 不 要 被 別 人 、 別 的 國 家 欺 騙 ， 華 人 也 是 人 ， 有 生 活 目 的 ， 有 生 命 ！ 要 這 要 這 ， 站 ， ， ， 有 的 的 要 要 要 ， ， 站 站 站 站 站 站 來 ， ， ， 要 ， 要 要 站 站 站 站 、 、 、 、 、 人 欺 騙 騙 人 也 人 也 是 有 要 有 ， ，\n",
      "！ 在 我 們 也 可 以 看 「 葉 問 二 」 了 。 我 已 經 看 兩 次 了 ！ 跟 葉 問 二 一 樣 超 好 看 ！ ！ ！ ！ ！ ！ ！ 二 二 二 ！ 二 超 ！ ！ ！ ！ ！ ！ ！ ！ ！ 問 二 二 ！ ！ ！ ！ ！ 也 也 ！ 看 看 到 看 問 二 二 了 ！ ！ 我 都 看 看 ！ ！ ！ 看 看 葉 問 二 二 ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ 二 二 二 二 二 ！ ！ ！ ！ 也\n",
      "的 喜 歡 的 是 這 個 孩 子 跟 這 些 玩 具 一 起 玩 得 很 開 心 的 樣 子 。 喜 我 我 我 我 我 。 ， ， 是 ， 。 。 這 這 我 這 。 很 。 ， 。 。 。 ， ， ， 。 ， 我 這 他 他 ， ， ， ， 的 是 ， ， ， ， 跟 這 這 這 具 ， ， ， 得 ， ， ， ， ， 孩 跟 跟 這 這 這 在 ， ， ， ， 得 ， ， ， ， 。 。 這 。 ， ， ， 。 。 的 是 的\n",
      "外 家 只 有 一 個 外 婆 ， 外 婆 已 經 九 十 歲 了 ， 可 是 還 很 健 康 。 外 婆 的 生 活 很 辛 苦 ， 沒 有 錢 ， 每 天 外 婆 都 去 工 作 。 外 外 家 但 但 ， 但 外 婆 家 家 快 很 ， 了 有 她 個 ， ， ， ， 婆 也 婆 快 經 八 了 ， ， 她 她 她 還 ， ， 。 家 家 家 很 很 辛 辛 苦 ， 也 也 也 ， ， 但 ， 都 都 都 去 。 外 家 家 有\n",
      "。 時 候 男 生 說 當 然 他 並 不 要 愛 這 位 女 生 。 後 來 是 因 為 每 天 都 在 一 起 ， 男 生 對 女 生 很 好 。 男 生 已 經 愛 上 這 女 生 了 。 當 但 而 說 說 說 經 說 他 這 這 。 。 。 生 說 ， ， 他 他 不 不 不 愛 這 這 生 。 。 。 是 是 是 是 為 是 是 很 是 。 。 。 生 也 生 生 生 生 。 說 說 經 就 經 上 這 這 。 。 這 。\n",
      "。 天 六 夜 的 最 後 天 比 牠 大 的 一 隻 公 獅 把 小 鹿 給 吃 了 。 那 隻 母 獅 看 起 來 很 傷 心 地 流 淚 。 然 後 我 也 流 淚 。 那 天 我 最 傷 心 的 一 天 。 那 那 是 那 。 母 。 把 給 。 。 一 。 。 。 大 的 的 隻 小 小 把 把 給 給 給 給 一 。 。 。 。 。 。 。 。 我 。 。 。 我 。 那 。 。 。 我 也 流 。 是 是 是 是 是 一\n",
      "電 看 的 電 影 不 多 ， 因 為 我 比 較 喜 歡 卡 通 片 。 我 最 喜 歡 看 的 卡 通 片 是 「 雲 南 」 。 。 ， ， ， ， ， ， ， ： 雲 雲 指 。 。 。 。 。 ， 。 。 ， ， 我 我 我 雲 南 ， ， 並 ， ， ， 但 我 我 我 我 不 看 看 東 。 。 。 ， 我 我 ， ， ， 我 我 我 是 看 雲 。 西 。 。 。 我 。 。 。 ， ， ， ： 雲 雲 。 。 。 。\n",
      "南 柯 南 」 不 像 普 通 的 卡 通 片 ， 因 為 看 「 柯 南 」 會 讓 我 們 想 一 想 到 底 發 生 什 麼 事 情 ， 讓 我 們 動 一 動 腦 筋 結 果 是 誰 殺 人 ？ 或 或 再 再 想 ， ， 看 「 看 「 並 也 更 不 看 看 動 通 片 ， 而 ， 看 「 柯 「 柯 時 片 會 看 看 看 ， ， 想 會 底 會 底 「 ， ， ， 再 再 動 動 動 ， 動 ， ， ， 誰 ？ 誰 ？\n",
      "。 柯 南 的 解 釋 我 覺 得 我 增 加 一 個 知 識 。 柯 我 我 我 我 我 我 我 我 我 我 我 柯 柯 我 柯 我 。 。 。 我 我 我 我 我 。 。 。 柯 我 我 我 。 。 。 。 我 是 我 我 我 柯 他 柯 他 的 我 ： ： 是 是 是 是 是 我 個 了 個 。 柯 我 我 ： ： 讓 是 是 是 是 是 我 了 了 雲 我 我 我 。 。 。 。 我 我 我 。 。 。 。 柯 南 柯 南 的\n",
      "。 一 天 一 個 公 司 來 到 鄉 下 。 這 個 女 生 就 想 要 試 試 看 。 後 來 因 為 太 多 人 公 司 人 員 決 定 用 大 風 吹 的 遊 戲 來 找 人 。 於 於 於 就 於 用 用 她 來 找 遊 。 她 她 她 她 她 想 要 要 了 她 她 。 她 那 女 說 說 想 說 想 想 想 想 。 說 。 她 。 她 太 女 。 。 決 就 就 要 用 用 大 遊 風 的 遊 來 來 。 於 於 在 。\n",
      "。 想 學 舞 但 是 沒 有 錢 後 來 她 找 一 個 方 法 就 是 在 舞 蹈 的 公 司 打 工 。 然 後 她 在 那 邊 工 作 ， 一 邊 工 作 ， 一 邊 學 舞 。 她 她 她 她 她 她 她 她 她 她 她 她 她 。 然 ， 。 是 她 。 。 。 她 她 她 她 她 她 她 她 她 她 她 去 她 。 她 她 她 她 她 她 她 就 就 她 去 工 ， ， 她 她 學 學 ， ， 一 去 學 她 她 她 。\n",
      "。 是 說 她 做 了 自 己 的 挑 戰 。 她 想 要 看 外 面 的 環 境 。 然 後 在 她 遇 到 失 敗 的 時 候 她 怎 麼 去 跳 出 去 從 那 個 失 敗 。 然 然 然 去 去 去 去 去 去 去 從 失 敗 她 是 說 她 做 在 在 去 去 去 敗 。 她 想 去 去 看 看 看 的 。 。 。 她 。 她 她 她 遇 她 失 敗 。 她 要 要 去 去 去 。 。 面 從 那 個 失 敗 。 是 說 去\n",
      "。 「 達 人 」 是 在 日 本 告 別 式 的 時 候 ， 來 參 加 幫 忙 的 人 弄 好 。 例 如 ， 幫 他 洗 乾 淨 ， 換 衣 服 ， 化 妝 。 等 妝 或 ， ， ， ， 妝 做 化 化 妝 妝 等 妝 或 「 「 「 達 人 ， 在 在 在 送 的 儀 式 的 的 ， ， 來 來 忙 的 忙 的 忙 的 的 的 儀 儀 的 ， ， 來 他 ， ， 他 洗 他 做 化 妝 化 妝 。 。 。 。 。 。 。 。\n",
      "的 最 喜 歡 的 一 部 電 影 是 叫 做 「 二 見 鍾 情 」 。 這 部 電 影 是 大 概 ２０ 年 前 製 作 的 電 影 。 它 在 它 在 在 在 ２０ 在 是 ， 。 。 。 他 他 他 他 是 他 他 一 一 第 一 見 鍾 他 他 的 他 的 ， ， ， ， 一 第 一 見 鍾 情 鍾 情 。 ， 他 ， 是 是 是 「 一 「 一 再 鍾 鍾 。 。 。 。 。 。 。 。 在 在 「 一 在 。 。 。 。 。\n",
      "。 個 故 事 講 我 們 一 位 女 生 的 生 活 。 那 位 女 生 的 爸 爸 有 新 的 老 婆 。 那 位 老 婆 的 個 性 真 的 不 好 ， 老 婆 不 喜 歡 白 雪 公 主 。 她 婆 她 婆 婆 婆 婆 婆 很 那 。 。 。 講 講 講 是 們 的 生 的 婆 。 婆 。 位 的 的 的 的 的 有 有 的 婆 婆 婆 婆 。 婆 的 的 的 ， 也 很 的 很 ， 婆 婆 婆 婆 婆 婆 歡 白 。 。 。\n",
      "的 個 城 市 的 名 字 還 是 西 班 牙 文 的 ， 翻 譯 的 意 思 是 「 白 色 的 房 子 」 。 這 這 這 譯 這 這 這 在 是 在 是 在 這 這 這 這 這 這 譯 的 這 是 這 是 是 是 是 這 這 這 這 個 這 的 的 ， ， 還 是 是 班 中 語 的 譯 譯 譯 譯 譯 的 譯 是 是 是 是 是 是 是 ， ， 這 譯 譯 譯 的 的 成 是 是 是 的 的 。 。 。 譯 這 這 的 的 的\n",
      "。 天 ， 那 位 美 國 人 以 前 的 女 朋 友 就 進 去 他 的 酒 吧 。 兩 年 後 再 見 面 ， 可 是 現 在 她 的 先 生 陪 她 。 她 她 後 後 那 說 的 的 的 的 有 沒 在 。 他 那 他 的 ， 和 和 ， 他 的 他 的 人 的 的 的 的 的 ， ， 就 了 了 了 的 的 ， 說 他 的 的 的 的 的 說 說 說 的 的 的 的 的 沒 在 陪 。 。 那 。 。 。 。 。 。 的 的\n",
      "、 我 來 說 ， 那 個 活 動 真 的 有 意 思 、 一 方 面 可 以 了 解 很 多 不 同 的 活 動 、 另 外 一 方 面 可 以 認 識 那 個 球 員 。 對 對 對 對 我 我 我 對 對 的 的 的 很 的 對 對 對 的 說 的 我 個 很 的 的 的 的 的 的 的 我 我 我 了 我 了 了 了 很 的 的 的 的 的 很 的 我 也 也 也 了 也 了 了 了 那 球 的 球 」 對 對 對 我 我 我\n",
      "你 聽 說 你 跟 你 的 女 朋 友 訂 婚 了 ， 恭 喜 你 們 ！ 不 過 您 說 我 有 一 點 失 望 你 都 沒 有 邀 請 我 參 加 訂 婚 禮 ， 不 過 我 可 以 去 你 們 十 月 的 婚 禮 吧 ？ 我 我 我 我 我 去 去 我 們 的 的 的 結 ， ， 我 我 我 ， 我 ， 我 過 你 們 的 的 去 ， ， 我 ！ 我 我 ！ ！ 過 過 我 們 的 的 的 ， ， ， 我 我 我 去 參 去 看\n",
      "你 了 訂 婚 禮 之 外 你 暑 假 過 得 好 嗎 ？ 我 希 望 你 找 到 了 打 工 的 機 會 ， 讓 你 有 賺 錢 的 方 法 。 但 另 你 我 你 我 你 你 你 你 你 之 你 另 你 你 ── 你 你 你 ， 你 除 了 之 之 之 之 ， ， ， 的 過 過 過 好 ？ 我 ， 你 你 你 你 之 。 。 ， 還 過 過 ， 你 你 你 有 能 。 。 我 你 我 你 。 ， 。 ， ， 你 你 你 了 禮 之\n",
      "拉 園 會 有 十 幾 位 有 名 的 拉 麵 廚 師 擺 地 攤 賣 拉 麵 和 各 種 各 樣 的 肉 、 魚 、 蔬 菜 等 等 。 還 有 有 社 社 社 社 社 社 社 ， 社 社 也 有 還 有 社 的 的 社 社 ， ， ， 社 社 有 有 有 有 著 的 的 拉 拉 ， ， ， ， ， ， ， ， ， ， 和 的 的 的 拉 麵 ， ， 在 ， ， ， ， ， ， ， ， ， 的 ， ， ， 。 。 。 。 。 ， 。\n",
      "她 應 該 知 道 楊 丞 琳 是 我 的 大 學 研 究 所 畢 業 ， 她 不 只 是 一 個 很 漂 亮 的 明 星 歌 手 ， 她 也 非 常 的 努 力 又 聰 明 。 她 她 她 她 她 的 的 的 的 又 又 很 的 的 的 ， ， ， 道 ， ， 琳 是 很 的 的 的 的 的 的 的 的 ， ， ， 是 是 是 很 很 的 很 的 的 的 的 ， ， 很 也 很 的 的 的 的 又 又 又 又 很 她 我 我 我 ，\n",
      "她 不 知 道 她 喜 不 喜 歡 拉 麵 ， 可 是 我 一 想 到 楊 丞 琳 吃 拉 麵 就 很 興 奮 了 。 我 [UNK] 楊 楊 楊 她 琳 她 。 。 [UNK] [UNK] [UNK] 我 [UNK] 我 [UNK] 楊 她 琳 說 她 。 。 。 。 。 。 我 我 我 她 她 楊 說 她 吃 吃 吃 麵 ， ， 是 她 是 她 到 楊 丞 琳 丞 琳 吃 吃 吃 的 。 。 。 。 她 楊 楊 楊 楊 丞 琳 要 ， ， 就 。 。 。 。 。 。 。 她 她\n",
      "。 些 活 動 一 定 會 讓 你 有 意 思 。 你 來 參 加 這 些 活 動 ， 很 開 心 得 不 得 了 。 你 這 這 這 這 這 這 會 會 ， 。 這 你 來 參 而 。 ， ， 會 會 會 會 會 很 。 。 這 參 這 ， 這 ， ， 會 會 會 很 有 意 的 。 你 來 參 加 參 加 這 會 會 會 會 很 很 參 的 。 能 來 來 參 加 加 。 。 ， 。 。 。 。 。 。 。 來 。 這 ， 一 ，\n",
      "我 跟 你 說 喔 ， 我 參 加 了 我 們 學 校 的 一 個 活 動 ， 你 猜 他 們 找 誰 來 。 我 我 我 說 我 說 ， ， ， 是 我 來 是 我 說 我 。 我 。 ， ， ， ， 我 ， ， ， 我 們 我 們 我 說 ， ， ， ， 我 們 我 了 我 們 們 們 的 的 的 的 的 ， ， ， 我 是 我 們 我 們 們 們 的 的 的 個 ， 。 ， ， 說 ， 跟 去 們 。 。 。 。 。 。 ， ，\n",
      "的 覺 得 這 個 活 動 真 的 很 棒 ， 不 僅 是 因 為 他 們 有 找 李 大 明 來 表 演 也 是 因 為 大 家 都 玩 得 很 開 心 。 我 我 我 我 我 得 我 得 的 得 得 的 也 很 很 我 我 我 我 我 我 得 這 們 是 動 真 的 很 棒 ， 也 是 是 是 為 是 為 有 有 到 找 到 的 很 棒 ， ， 也 是 為 是 得 玩 得 得 得 得 很 。 。 [UNK] 。 。 為 ， 為 這 得 這\n",
      "你 久 不 見 ， 我 是 李 大 明 還 記 得 我 嗎 ， 最 近 過 得 還 好 嗎 ？ 你 你 你 ， ， ， ， 的 過 得 得 得 很 ， ， ， ， ， ， ， ， 好 ， 的 得 還 的 ， ， ， ， ， ， ， ， ， ， ， ， ， ， 是 李 ， ， 你 你 ， 我 我 你 你 得 你 得 得 得 還 。 ， ， 你 ， ， 我 我 ， 你 你 得 得 得 還 ， ， ？ ， ？ ？ 。 。 。 ， ， ， ，\n",
      "我 天 我 參 加 了 學 校 舉 辦 的 一 場 活 動 ， 我 很 喜 歡 ， 所 以 我 想 寫 一 封 信 給 你 ， 想 跟 你 談 談 我 今 天 的 感 情 。 今 今 今 我 我 我 我 我 我 我 我 一 的 一 ， 今 今 我 我 我 我 我 我 我 的 的 一 一 新 一 天 我 我 我 我 我 我 我 我 的 的 的 一 一 的 一 我 我 我 我 我 我 我 ， 我 的 的 的 感 。 這 今 今 天 我 我\n",
      "， 在 這 裡 念 書 ， 念 得 很 好 ， 老 師 們 都 喜 歡 他 。 他 他 他 他 得 。 他 。 ， 說 他 他 他 他 他 他 他 得 書 得 ， 他 ， 他 。 。 。 他 在 他 。 他 得 得 得 得 ， ， 他 他 他 他 他 他 他 ， 念 得 得 好 好 ， 老 們 也 很 說 他 。 他 他 他 得 得 得 得 ， ， ， 們 也 都 很 。 。 。 。 。 得 。 。 。 。 。 。 。 。 。 的 他\n",
      "他 大 學 邀 請 他 來 是 因 為 讓 學 生 們 可 以 從 他 身 上 學 到 一 些 東 西 。 我 很 羨 慕 他 ， 真 想 跟 他 一 樣 。 我 我 大 大 大 大 邀 邀 邀 邀 邀 做 他 我 大 大 大 大 大 大 校 邀 邀 請 他 來 ， ， ， 讓 讓 大 們 們 們 從 從 從 們 請 他 來 ， ， ， ， 讓 ， 也 也 想 ， ， ， ， 他 做 做 。 [UNK] 。 [UNK] 。 。 大 。 大 邀 們 邀\n",
      "！ 祝 你 天 天 快 樂 ！ 祝! 祝 祝 ！ ！ ！ ！ ！ 祝 ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ 祝 祝 祝 祝? ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ 祝 ！ 祝 ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ 祝 能 能 的 ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ 祝 你 祝 能 能\n",
      "。 ！ 說 到 這 ， 我 就 想 要 聽 聽 看 他 的 歌 了 。 嗯 嗯 嗯 。 。 。 想 。 。 。 。 。 。 。 啊 。 。 ， 。 。 想 想 。 。 。 。 。 。 。 。 。 。 ， 。 。 。 想 。 。 。 。 。 。 。 ， ， ， ， ， 想 去 去 去 他 他 的 的 。 。 。 。 ， ， ， ， 想 去 去 去 他 他 的 的 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 這 ，\n",
      "加 為 他 很 紅 ， 所 以 參 加 的 人 很 多 ， 排 隊 線 很 長 ， 座 位 很 快 就 滿 了 ， 還 有 很 多 人 站 著 參 加 。 因 因 因 因 因 因 因 因 人 都 站 著 他 因 因 因 因 因 因 因 因 因 為 很 很 ， ， 他 座 他 的 人 很 多 ， 但 為 也 人 也 很 但 座 座 他 人 人 人 也 ， ， ， ， 座 ， 站 著 ， 。 。 。 。 。 。 。 。 。 因 很 紅 很\n",
      "！ 個 人 都 好 想 看 見 他 ， 聽 他 講 話 ！ 每 每 每 ！ 想 想 想 每 ！ 一 ！ ！ ！ 每 每 每 每 好 想 想 想 ！ ！ 每 一 ！ 每 ！ 每 每 每 ！ 每 想 想 想 ！ ！ 每 每 每 一 每 每 每 每 每 想 好 想 ！ 想 ！ ！ 聽 聽 聽 ！ ！ 每 每 每 ！ 想 想 想 想 想 ， ， 聽 聽 ， ！ 每 每 每 ！ ！ ！ ！ ！ ！ ！ ！ 一 ！ ！ ！ 每 每 每 每 好 想\n",
      "！ 望 你 在 英 國 一 切 也 好 ， 學 習 過 得 順 利 ！ 祝 祝 祝 ！ ！ ！ ！ 祝 得 過 得 得 ！ ！ 祝 ！ ！ ！ ！ ！ ！ ！ ！ 得 ！ 得 ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ， 得 得 得 ！ ！ 在 ， ， ， ， 都 都 ， 都 得 過 得 得 的 ！ ！ ！ ！ ！ ！ ！ ！ ， ， ， 得 也 得 得 ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！\n",
      "講 久 以 後 ， 楊 先 生 就 回 來 了 開 始 演 講 。 很 一 在 在 楊 也 回 到 回 來 。 。 。 。 。 。 在 。 ， 也 也 到 來 回 來 。 。 。 。 。 。 。 。 。 ， 回 到 回 到 回 到 ， 。 很 而 ， ， 楊 又 回 回 回 到 回 京 ， ， 。 。 。 。 。 ， ， 從 回 回 回 到 回 ， ， ， 。 。 。 。 。 。 。 在 。 在 。 。 。 。 。 。 。 。 。 ，\n",
      "我 望 你 在 大 學 時 也 有 自 私 的 機 會 ， 如 果 有 的 話 不 要 忘 記 寫 信 給 我 ！ 希 希 祝 希 ！ ！ ！ ， 你 我 我 啊 謝 ！ 希 [UNK] 希 [UNK] 希 你 ， ！ 你 ！ 我 私 我 私 私 希 希 讓 你 在 我 能 時 有 自 自 私 私 私 了 喔 ， 你 你 ， ， ， ， ， 自 私 自 私 私 私 機 ！ ！ ！ ！ ！ ！ ！ ， ！ ！ ！ 我 ！ ！ ！ ！ ！ ！ ！ 你 在\n",
      "她 姐 姐 也 在 這 裡 念 書 。 她 已 經 畢 業 了 差 不 多 了 。 她 的 同 學 都 很 高 興 ， 因 為 他 們 爸 媽 打 算 舉 行 晚 會 好 好 兒 地 慶 祝 慶 祝 。 她 她 她 她 她 她 她 她 她 和 她 在 在 在 裡 。 。 。 她 也 經 都 經 了 差 有 差 了 。 。 她 的 的 們 也 也 很 也 ， 她 他 他 他 他 們 都 都 去 好 個 ， ， 兒 兒 兒 地 他 她 和\n",
      "的 們 都 知 道 他 愛 看 韓 國 的 電 影 ， 還 有 喜 歡 看 美 國 籃 球 比 賽 。 他 他 我 他 我 ， ， ， ， ， 球 球 。 而 。 而 。 我 我 我 。 ， ， ， ， 球 。 。 的 而 。 。 ， 我 ， ， ， ， ， ， 看 看 的 的 的 ， ， 他 也 他 他 看 看 看 ， 也 看 看 的 的 的 。 。 。 。 他 ， 看 看 的 球 的 球 。 。 。 。 。 。 。 。 。 ， ，\n",
      "的 場 活 動 真 熱 鬧 ， 為 了 看 他 的 演 出 ， 很 多 人 特 別 來 了 我 們 學 校 ， 來 的 人 數 應 該 超 過 了 一 千 多 人 吧 ！ 這 這 這 ， 也 也 是 ， 一 ， ， ， ， ， 這 這 這 ， 真 的 真 真 ， ， ， 看 看 看 的 演 出 出 ， ， 還 還 還 都 ， ， ， ， 看 看 ， ， 這 ， 也 也 也 也 過 一 一 一 人 人 ！ 吧 ！ 這 ！ 這 ， 真 很\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "！ 我 在 等 你 的 連 絡 。 最 近 天 氣 開 始 變 熱 了 ， 小 心 不 要 中 暑 ！ 請 ！ 在 ！ 在 在 在 在 在 ， ！ ！ ！ ！ ！ 在 ！ 在 ！ 在 在 在 在 在 ， ， ！ ， ， 在 在 在 在 在 在 在 在 在 ， ， 。 ！ ！ 們 的 的 又 又 變 ， ， ， 在 在 的 ， ！ 。 ！ ， 的 的 又 又 很 ， ， ， ！ ， ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ 在 在 在 和\n",
      "！ 我 很 可 惜 妳 生 病 不 能 來 參 加 ！ 可 是 不 用 擔 心 ， 我 一 定 會 去 醫 院 看 妳 和 給 妳 看 照 片 ， 我 拍 很 多 照 片 了 ！ 我 我 我 我 給 給 給 都 都 很 很 多 ！ ！ 我 我 我 ， ， ， ， ， ， 不 來 來 ！ ！ ！ 我 我 我 我 ， ， ， ， ， 不 來 來 來 ！ ！ ！ ！ ！ ！ ！ 妳 的 ， 給 都 拍 很 多 照 ！ 我 我 我 我 ， ，\n",
      "你 幾 天 了 沒 看 到 你 ， 現 在 你 怎 麼 樣 ， 我 希 望 你 什 麼 都 好 。 現 在 我 也 很 好 ， 沒 有 什 麼 需 要 關 心 。 我 我 我 我 我 我 需 需 需 需 需 需 你 你 管 你 又 很 ， ， 我 了 ， 有 看 你 ， ， 管 看 怎 怎 怎 麼 我 我 我 你 你 你 都 都 ， 你 管 你 也 也 也 好 我 我 需 需 需 需 需 需 你 你 。 你 你 你 。 我 我 我 了\n",
      "。 麼 樣 ？ 我 最 近 非 常 不 開 心 ， 你 知 道 為 什 麼 嗎 ？ 因 為 我 交 了 一 個 女 朋 友 。 她 她 她 她 她 我 我 我 她 她 她 她 她 她 她 她 她 她 我 我 很 不 不 我 不 很 怎 怎 怎 怎 ， 我 我 的 不 的 不 不 不 不 你 你 你 是 。 是 她 她 她 我 不 不 不 不 不 。 不 你 你 是 是 是 她 。 ？ 不 不 不 不 我 她 她 她 。 怎 這 她 ，\n",
      "？ 跟 妳 的 家 人 最 近 過 得 好 嗎 ？ 有 沒 有 什 麼 消 息 ？ 謝 謝 謝 最 呢 好 又 ？ 有 好 有 好? ？ 最 最 最 好 好 嗎 好 嗎 ？ ？ 什 ？ 呢 ？ ？ 跟 最 都 好 嗎 好 嗎 嗎 嗎 ？ 跟 嗎 的 一 ， 最 都 好 不 好 嗎 嗎 ？ ？ 有 什 好 好 謝 ？ ？ 最 都 好 不 好 嗎 嗎 ？ ？ ？ 新 好 好 ？ ？ ？ ？ ？ 嗎 好 ？ ？ ？ ？ ？ ？ 跟 家 的\n",
      "。 一 天 我 跟 朋 友 們 一 起 去 參 加 一 個 生 日 的 派 對 。 那 邊 我 碰 到 一 個 非 常 棒 的 男 生 ！ 他 的 名 字 是 「 大 沙 」 。 他 他 而 他 而 他 的 他 叫 叫 大 大 大 他 我 我 我 我 我 我 跟 們 要 要 去 去 他 他 我 我 大 對 party 對 對 對 在 我 我 是 叫 ： 大 我 大 的 的 他 ！ ！ 他 的 他 叫 叫 大 大 大 沙 大 。 他 他 。 我\n",
      "。 愛 的 ， 從 那 一 天 我 們 一 起 出 去 幾 次 。 所 以 我 又 比 較 多 時 間 了 解 他 是 什 麼 樣 子 的 。 他 真 是 一 個 聰 明 的 人 。 他 他 他 他 他 真 真 真 很 很 。 。 。 。 愛 愛 。 。 。 ， ， ， ， 就 就 。 。 。 愛 愛 愛 我 我 也 我 有 有 我 ， 就 就 。 。 。 。 的 。 他 他 他 真 真 真 是 很 很 聰 。 。 愛 愛 愛 他 。\n",
      "。 愛 的 李 大 明 ， 我 真 不 知 道 怎 麼 幫 ！ 但 是 我 感 覺 是 我 越 來 越 多 愛 他 ． ． ． 妳 有 空 的 話 來 找 我 。 我 希 望 我 的 男 朋 友 介 紹 給 妳 。 我 我 我 我 我 我 我 我 李 啊 啊 啊 我 我 真 該 該 該 辦 辦 辦 辦 ！ 我 ， 我 。 給 我 我 我 說 我 辦 辦 辦 了 請 請 想 也 可 就 來 來 我 我 我 我 我 把 我 把 能 。 給\n",
      "。 的 個 性 很 溫 柔 ， 她 對 別 人 都 很 好 ， 很 有 禮 貌 。 對 我 來 說 誰 都 比 不 上 她 。 她 她 她 她 她 她 她 她 她 她 好 她 她 她 她 她 她 她 她 她 她 她 她 她 她 她 她 她 她 她 她 也 ， ， 。 她 也 人 人 也 很 好 好 她 她 她 她 她 ， 她 。 她 說 我 人 人 很 她 她 她 她 。 她 。 。 。 她 。 她 。 她 。 她 她 她 她 她 她\n",
      "的 們 在 吃 飯 的 時 候 ， 很 像 一 個 電 影 ， 那 個 餐 廳 的 環 境 非 常 適 合 跟 你 喜 歡 的 人 去 吃 飯 。 你 你 很 你 很 你 你 你 你 你 你 你 。 我 我 你 常 就 常 你 你 你 你 你 們 你 你 的 的 ， 就 就 像 是 是 電 看 ， 那 那 那 的 的 的 的 ， 常 很 常 是 常 看 你 。 ， 你 的 的 的 的 。 常 。 常 常 常 ， 常 ， 你 你 在 吃\n",
      "！ 明 ， 改 天 來 基 隆 吧 ！ 我 幫 你 認 識 我 的 男 朋 友 。 確 定 你 會 喜 歡 他 ！ 王 先 生 是 一 個 大 好 人 ！ 他 他 他 王 ！ ！ ， ， ， ， ， 大 ！ ！ 他 你 你 你 ！ 王 ， 你 ， 你 你 來 ！ ！ 吧 吧 我 幫 你 先 你 你 你 你 的 。 。 ， ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ， 大 ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ 你\n",
      "我 天 我 很 興 奮 想 跟 你 分 享 ， 希 望 你 給 我 意 見 我 這 樣 交 朋 友 對 嗎 ？ 因 為 他 會 影 響 到 我 一 輩 子 ！ 我 也 很 當 心 以 後 是 如 何 沒 人 可 料 到 。 我 我 我 我 我 高 奮 開 奮 很 興 我 ， 享 ， 也 也 我 你 我 我 我 ！ 我 心 很 很 以 我 。 我 我 我 我 他 會 會 我 到 我 我 我 ！ ！ 我 也 也 很 擔 我 我 我 好 沒\n",
      "。 有 好 消 息 。 就 是 因 為 我 交 了 一 個 女 朋 友 ， 所 以 我 最 近 很 開 心 。 你 是 我 的 最 好 朋 友 ， 所 以 我 想 要 把 這 個 好 消 息 告 訴 你 。 我 我 我 我 我 我 我 我 我 個 。 。 。 。 是 我 為 我 新 我 新 了 新 新 女 。 。 。 。 。 。 我 我 我 我 我 有 你 最 最 最 最 最 友 。 我 我 我 我 我 我 我 這 的 的 告 。 。\n",
      "她 知 道 嗎 ？ 我 跟 她 在 一 起 的 時 候 ， 我 覺 得 我 們 一 定 是 命 運 的 真 愛 人 。 老 天 爺 把 她 送 給 我 ， 相 信 我 不 會 錯 了 。 我 我 我 我 我 我 我 我 我 我 我 ， 我 我 我 當 當 我 跟 我 在 在 走 的 的 ， ， ， 我 我 我 ， 一 就 是 命 在 在 好 的 真 ， ， 會 會 會 會 會 也 ， ， ， 我 我 我 我 就 做 錯 我 她 [UNK] 我\n",
      "。 是 好 甜 的 女 人 ， 我 要 告 訴 你 。 每 個 週 末 ， 她 到 我 的 地 方 來 幫 我 作 房 事 。 然 後 就 好 好 的 做 菜 給 我 吃 。 她 她 她 她 會 的 的 的 的 的 ， 給 我 我 。 我 我 我 甜 的 。 。 。 我 想 想 你 你 。 。 在 的 的 ， 都 會 會 到 的 來 我 她 。 給 。 。 。 。 她 她 會 的 的 的 的 的 我 給 我 。 。 。 。 。 的 的\n",
      "。 們 都 喜 歡 去 夜 市 ， 看 電 影 ， 買 東 西 什 麼 的 。 她 也 喜 歡 東 部 的 風 景 好 漂 亮 。 她 她 她 。 她 。 的 。 景 景 景 ， ， 她 她 她 她 她 她 。 她 。 ， ， ， ， ， 。 ， 她 她 逛 逛 逛 ， 逛 東 東 ， 買 東 西 西 西 。 。 她 。 。 。 。 。 ， 買 ， 西 西 西 。 。 。 。 。 。 。 。 。 。 。 景 景 景 。 。 。 她 她\n",
      "。 常 常 照 顧 她 姐 姐 的 孩 子 因 為 她 最 喜 歡 孩 子 ， 覺 得 她 要 有 孩 子 我 希 望 你 回 答 我 的 信 。 信 。 我 我 。 你 能 能 看 你 的 信 信 信 。 我 子 子 子 子 子 她 子 她 子 她 子 她 的 子 子 子 子 子 子 她 最 最 最 她 她 ， ， 她 的 孩 子 子 子 子 子 你 要 能 。 你 。 。 信 。 因 。 。 子 子 子 。 。 。 。 。 。 她\n",
      "。 要 把 一 本 書 介 紹 給 你 。 我 是 從 電 視 上 知 道 的 。 簡 單 的 說 內 容 是 怎 麼 過 人 生 。 我 我 再 的 的 說 是 是 是 是 過 過 。 我 我 我 。 的 的 。 。 。 。 。 。 。 我 。 這 。 書 。 。 。 。 。 。 。 我 我 從 上 上 上 。 。 。 。 。 。 。 。 。 。 我 我 。 。 上 。 。 。 。 的 。 的 說 。 。 是 是 。 。 。 。 這\n",
      "， 很 喜 歡 這 本 書 ， 因 為 ， 那 個 人 很 聰 明 的 人 ， 而 且 那 個 時 候 有 他 才 有 公 平 。 所 以 ， 我 真 的 很 喜 歡 ， 。 。 。 。 ， ， ， ， ， ， 很 ， 歡 ， ， ， ， ， ， ， 這 ， ， ， ， ， ， ， 是 是 是 是 很 ， ， ， 且 且 且 且 ， ， ， ， ， 是 是 很 。 歡 ， ， ， ， ， 很 的 歡 。 歡 。 。 。 。 。 這 ，\n",
      "美 國 進 入 第 二 世 界 戰 爭 的 時 候 ， 蔣 介 石 問 美 國 要 打 仗 日 本 。 他 需 要 美 國 的 軍 糧 ， 要 把 國 民 軍 現 代 化 。 美 民 美 民 民 家 民 民 民 民 家 隊 隊 美 ， 中 在 中 在 美 在 要 ， ， 民 的 的 。 ， 蔣 問 就 說 ， ， 要 要 要 打 打 ， 的 ， 說 說 問 要 的 要 ， ， 要 民 民 民 民 隊 隊 代 。 美 美 說 說 要 ，\n",
      "， 介 石 當 史 迪 威 國 民 軍 委 員 的 總 統 ， 說 史 迪 威 可 以 領 導 中 國 的 軍 團 。 蔣 蔣 蔣 威 威 威 威 威 是 說 。 蔣 說 蔣 蔣 蔣 威 威 威 威 威 威 當 威 威 的 的 的 。 蔣 讓 把 當 了 威 威 當 威 當 委 的 的 的 的 ， 說 史 史 威 威 威 威 當 的 當 的 總 的 的 。 蔣 石 說 威 威 威 威 威 威 威 的 。 的 。 。 。 蔣 威 了 威\n",
      "的 介 紹 幾 米 的 「 星 空 」 。 幾 米 是 台 灣 人 的 繪 本 作 家 ， 我 來 台 灣 以 後 去 他 的 展 覽 ， 那 時 候 我 才 知 道 他 ， 他 畫 的 繪 真 的 漂 亮 得 我 很 感 動 。 他 他 他 他 他 他 的 的 的 ， ， 。 「 是 是 是 他 他 他 他 的 ， ， ， ， ， ， 是 是 是 是 是 他 他 畫 看 ， ， 他 我 我 我 才 是 是 他 他 他 他 本 真 ，\n",
      "。 想 我 喜 歡 這 本 書 因 為 它 很 有 意 思 ， 不 太 難 ， 孩 子 看 也 懂 。 這 本 書 還 有 很 多 故 事 ： 忠 、 孝 、 愛 、 仁 。 孝 孝 還 有 故 事 、 孝 、 、 愛 、 愛 、 ， ， ， 我 ， ， ， ， ， ， ， 它 它 有 有 有 也 也 也 難 ， ， ， ， ， ， 。 ， 它 ， ， 還 有 多 多 故 事 ： 如 、 、 愛 、 愛 、 愛 。 。 我 ， ， ，\n",
      "。 時 候 他 給 我 推 薦 了 這 本 書 再 把 一 張 紙 寫 下 來 那 個 書 的 名 字 。 他 還 告 訴 我 在 哪 裡 可 以 買 到 這 本 書 。 他 這 這 這 他 他 他 他 他 到 這 。 這 。 這 他 這 他 他 他 他 他 他 他 了 了 本 。 。 ， ， 用 他 寫 他 寫 寫 寫 他 他 了 。 。 。 。 還 還 他 他 他 在 ， ， 能 買 到 到 書 。 他 。 這 。 這 他 他 他\n",
      "的 在 如 果 我 們 信 神 說 的 話 ， 馬 上 就 他 的 靈 進 去 我 們 的 靈 ， 開 始 住 在 我 們 的 前 面 。 在 在 在 在 在 在 在 在 在 在 前 靈 前 在 在 在 在 在 ， ， ， ， ， 在 ， 在 前 在 我 在 信 神 的 的 話 ， 他 他 就 讓 的 他 下 ， ， 在 的 神 的 的 話 ， 他 ， 在 後 前 靈 裡 後 在 在 在 。 。 。 。 。 。 。 。 。 。 。 在\n",
      "你 跟 你 住 在 一 起 已 經 一 個 月 了 ， 我 覺 得 你 有 不 好 的 生 活 習 慣 ， 你 晚 上 都 很 晚 睡 ， 都 在 聽 音 樂 ！ 很 大 聲 ！ 很 ！ 還 都 ， ！ 在 ！ ！ ！ 很 很 很 ！ 我 ， 我 已 在 已 已 已 快 已 一 一 很 ！ 很 ， 我 你 你 是 有 已 已 已 有 有 快 已 很 ， 都 都 很 很 ， ， 都 在 在 玩 ， 很 很 很 大 很 ！ 你 ！ 在 已\n",
      "， 們 大 家 希 望 你 能 小 聲 一 點 ， 或 者 是 早 一 點 睡 ！ 健 康 也 好 ． ． ． 我 祝 我 祝 我 ！ ， ， ！ ！ ！ ！ ！ 我 ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ 我 ！ 我 者 者 ， ， ， 都 ， 你 能 能 聲 一 點 ， 或 者 者 者 的 點 ！ ！ ！ 你 能 的 聲 一 點 者 者 者 者 是 的 ， 點 ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ！ ， ，\n",
      "的 希 望 你 減 少 玩 遊 戲 的 時 間 ， 注 重 學 習 ， 還 有 晚 上 早 一 點 睡 ， 你 這 樣 子 做 不 但 你 的 成 績 好 ， 而 且 你 的 分 數 會 有 進 步 。 你 你 你 你 讓 讓 讓 要 多 ， 能 你 能 能 你 你 的 的 時 ， ， 要 且 要 ， ， ， 是 要 要 要 你 的 的 ， ， ， 要 要 做 ， ， 會 會 成 成 績 績 且 且 且 且 且 的 也 也 會 。 你\n",
      "， 二 點 之 後 ， 麻 煩 你 關 燈 睡 覺 ， 不 要 再 上 網 跟 朋 友 聊 天 了 ， 好 嗎 ？ 好 ？ ？ ？ ？ ？ ？ ？ ？ 上 上 嗎 了 好 ？ 。 。 ？ ， ， ， ， ， ， 門 門 門 門 門 ， ， ， 門 ， 門 ， 先 上 門 門 門 門 門 門 ， 也 再 去 網 ， ， ， ， 先 門 門 門 門 門 ？ ？ 。 。 去 ， ， ， ， 。 。 了 門 吗 ？ ？ ？ ？ ？ 。 。 ，\n",
      "： 誠 實 地 建 議 你 得 更 注 意 這 些 方 面 ， 以 免 麻 煩 別 人 。 但 但 但 但 ， 你 ， 地 地 地 地 地 ， 你 你 在 在 。 ， 。 。 ， 我 地 地 地 地 地 ， ， ， ， ， ， ， 地 地 地 地 地 地 地 你 ， 要 要 意 你 這 的 ， ， 以 會 會 地 地 地 你 你 多 多 於 這 這 的 ， ， 會 會 會 你 到 。 。 。 。 。 。 。 。 。 。 地 地 說 地\n",
      "， 要 專 心 做 家 事 ， 與 其 很 快 地 ， 但 馬 馬 虎 虎 做 事 ， 不 如 好 好 地 幫 助 對 方 ， 對 不 對 ？ 對 對 要 要 ， ， ？ 方 ？ ， ？ ？ ？ 對 ？ 要 ， 要 要 很 要 很 ， 人 要 要 ， 做 做 事 ， 不 要 很 很 很 ， 就 就 就 地 地 地 地 做 ， ， 要 要 很 很 去 他 方 方 方 。 ？ ？ ？ ？ ？ ？ 要 去 去 地 ， 方 。 方 。 做 。\n",
      "你 實 在 的 ， 目 前 我 真 感 覺 很 悶 ， 我 不 要 因 這 個 小 小 的 事 情 而 會 被 迫 請 師 長 的 房 間 。 跟 你 住 在 一 起 的 日 子 我 過 得 很 愉 快 。 我 我 我 我 我 我 ， ， ， 的 的 ， 我 我 我 真 的 的 很 悶 悶 ， 請 想 想 在 為 為 小 的 我 真 的 的 很 很 你 你 去 回 回 的 的 去 我 你 你 你 你 ， 起 ， 我 我 也 的 很 你\n",
      "助 助 別 人 是 一 個 好 的 行 動 ， 我 曾 經 幫 助 很 多 人 ， 不 過 我 最 印 象 的 事 是 我 幫 助 一 個 小 學 學 生 。 我 我 我 我 我 我 我 我 我 我 我 的 ， 我 我 象 我 我 我 幫 幫 我 人 ， 我 很 我 的 事 動 我 我 我 我 也 經 過 過 人 ， 我 我 我 象 印 象 深 象 的 是 我 我 我 我 了 小 的 人 。 我 象 我 象 象 象 。 我 。 。 。\n",
      "她 一 次 我 的 朋 友 給 我 打 電 話 ， 就 說 她 有 一 個 問 題 。 她 一 邊 哭 一 邊 告 訴 我 她 的 困 難 ， 她 跟 她 的 男 朋 友 吵 架 了 。 她 她 她 她 她 她 她 她 吵 吵 她 她 她 她 的 她 的 她 她 她 她 她 她 她 她 她 說 她 有 她 有 她 的 ， 她 她 她 她 她 她 ， 她 哭 說 的 的 的 的 說 說 跟 的 她 的 吵 吵 吵 吵 架 她 她 。 她\n",
      "。 住 在 師 大 夜 市 旁 邊 ， 但 是 情 況 很 安 定 。 交 通 非 常 方 便 ， 不 管 要 去 哪 裡 ， 捷 運 、 公 車 都 有 。 住 住 住 住 住 住 住 住 、 、 、 都 ， 也 住 不 不 不 不 不 不 就 就 大 的 的 的 的 ， 住 是 也 是 不 不 全 全 全 。 的 在 很 很 。 。 不 不 也 是 是 ， ， 、 、 、 、 、 都 。 。 。 。 。 。 。 。 。 住 在 的\n",
      "。 住 在 大 樓 附 近 ， 除 了 附 近 有 漂 亮 得 沒 話 說 的 國 立 公 園 。 我 們 跟 我 的 朋 友 可 以 考 幾 個 月 。 我 祝 你 決 定 要 來 這 棟 大 樓 。 我 我 我 祝 你 祝 你 ， 有 ， ， 大 的 。 ， ， 除 了 ， 了 還 有 有 ， ， 的 的 的 的 的 和 。 。 。 ， 還 還 跟 有 有 考 有 考 考 考 考 月 月 祝 祝 祝 你 要 要 要 住 住 棟 。\n",
      "他 的 房 東 是 一 個 很 好 的 人 ， 我 要 甚 麼 幫 助 他 一 定 幫 你 甚 麼 忙 ， 我 總 是 會 喜 歡 那 個 房 間 。 我 我 我 我 我 我 我 我 我 在 的 的 。 我 我 我 人 人 人 人 他 他 他 是 他 是 我 個 很 人 人 人 人 人 要 我 有 他 他 他 他 他 會 有 很 人 人 人 人 我 我 會 喜 很 去 在 的 的 的 。 。 。 。 。 ， 說 我 我 他 ， 是 ，\n",
      "他 們 房 東 也 是 個 好 人 好 像 爸 爸 一 樣 ， 他 有 一 個 女 兒 差 不 和 我 們 一 樣 大 ， 他 也 是 我 的 好 朋 友 ， 你 一 看 到 他 就 會 喜 歡 的 。 他 他 他 他 他 他 像 ， 房 的 房 東 他 是 個 好 像 ， 像 像 像 的 像 ， 他 他 他 他 他 跟 他 ， 像 像 像 像 像 的 的 ， 他 也 是 我 的 好 ， ， ， 要 你 一 他 他 他 會 很 ， 他\n",
      "。 樓 附 近 超 市 ， 便 利 店 ， 夜 市 也 有 。 所 以 ， 生 活 的 環 境 也 不 錯 。 我 覺 得 夜 裡 的 房 子 很 安 靜 ， 交 通 跟 旁 邊 的 環 境 很 好 。 而 樓 而 樓 ， ， ， ， 。 ， ， 有 有 有 ， ， ， ， ， ， ， ， ， 。 。 ， ， ， ， ， ， ， ， ， ， ， 。 。 我 ， ， ， 的 子 也 很 靜 靜 靜 ， ， 全 跟 ， ， ， 也 。 。\n",
      "？ 住 的 大 樓 有 １ 個 空 間 你 想 住 嗎 ？ 」 」 」 「 ？ ？ ？ ？ 間 ， ， 你 ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ 間 ？ ？ 想 ？ ？ ？ ？ ？ ？ ？ ？ ？ ？ 間 間 間 ， ？ ？ ？ ？ ？ ？ ， ， 。 一 間 空 間 ， ， 還 ？ ？ ？ ？ ？ ？ 。 有 有 「 一 間 間 ， ， 住 住 ？ ？ ？ ？ ？ ？ ？ ？ 間 間 間 ？ ？ ？ ？ ？ ？ ？ ？ ？ ？\n",
      "你 是 有 趣 ， 回 信 告 訴 我 ， 我 就 給 你 更 詳 細 的 資 料 。 很 很 很 很 很 很 很 很 更 很 詳 很 很 很 很 你 。 。 你 很 很 很 更 詳 很 詳 。 你 你 你 。 。 。 。 你 很 很 很 很 很 ， ， 你 你 ， 我 我 ， 你 會 會 最 最 詳 詳 詳 的 很 你 你 你 ， 我 ， 你 你 會 最 更 更 詳 的 的 和 。 。 。 。 。 。 很 。 。 更 詳 。 的 你\n",
      "他 的 房 東 一 個 老 人 。 他 一 切 中 點 很 嚴 格 ， 他 住 跟 我 一 樣 的 大 樓 ， 可 是 他 常 常 去 出 國 旅 行 ， 所 以 大 部 分 不 在 家 裡 。 他 他 他 他 一 是 一 是 一 一 一 是 是 是 是 是 是 ， 一 的 一 的 一 中 有 又 很 。 他 是 他 是 是 是 一 的 一 一 一 有 有 他 他 不 要 要 和 ， ， ， 他 他 他 他 都 都 都 住 。 他 是\n",
      "的 且 交 通 很 方 便 ， 有 一 個 捷 運 站 、 公 車 站 ， 從 這 裡 到 你 的 學 校 的 公 車 也 很 多 。 而 而 而 而 。 。 。 。 。 而 。 而 。 而 。 而 。 。 。 。 。 。 。 。 。 而 。 而 。 也 。 ， 。 有 有 個 個 個 ， ， 。 ， 從 ， 從 從 從 從 。 到 。 的 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。 。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "。 是 房 東 另 外 沒 有 什 麼 特 別 不 好 的 地 方 所 以 住 起 來 很 不 錯 。 所 以 啦 ， 你 可 以 去 好 好 地 想 一 想 啦 。 哈 哈 我 我 再 好 去 好 去 去 啦 啦 啦 啦 ， 的 ， ， ， ， 東 ， ， 也 也 什 麼 特 特 的 的 的 ， ， ， ， ， ， ， 也 也 有 麼 特 特 特 的 ， ， 好 好 去 好 去 去 去 啦 啦 。 。 。 。 。 。 。 東 ， 東\n",
      "！ 要 是 錯 過 這 麼 好 房 間 ， 沒 有 更 好 的 房 間 喔 ！ 你 應 該 早 點 決 定 吧 ！ 給 我 聯 絡 ， 拜 拜 。 再 ， 再 。 ， ， 絡 ， 絡 電 絡 絡 ， 。 。 。 ， ， ， ， ， ， 你 聯 絡 ， ， ， ， 的 的 ， 就 就 就 再 有 更 房 了 了 ！ 你 ！ 你 ， ， ， ， ， ， ， ！ 絡 絡 絡 絡 絡 ， 。 。 。 。 ！ 。 ！ ！ ！ ！ 絡 絡 絡 ，\n",
      "， 覺 得 那 個 房 間 非 常 好 ， 因 為 房 間 很 大 ， 很 乾 淨 ， 再 加 上 電 費 和 水 費 都 免 費 。 房 間 裡 面 有 一 個 床 ， 櫃 子 ， 電 視 機 ， 光 碟 ， 什 麼 的 。 再 再 再 我 ， ， 個 的 很 ， ， ， ， 再 ， 它 它 也 很 ， 很 很 很 ， ， 再 再 ， 再 再 再 ， ， ， ， ， ， ， 的 還 有 有 有 大 大 床 ， ， ， ， ， ， ，\n",
      "、 一 天 是 我 最 快 樂 的 一 天 ， 因 為 我 跟 家 人 、 朋 友 們 、 男 朋 友 都 一 起 慶 祝 我 的 生 日 ， 幸 虧 他 們 很 愛 我 ， 要 不 然 我 就 死 定 了 。 幸 幸 幸 但 這 這 這 ， 我 是 我 我 我 的 的 ， ， ， ── 為 我 我 我 我 我 、 、 、 、 ， ， ， ， 們 在 在 在 我 的 的 ， ， 幸 幸 幸 為 都 們 很 愛 我 ， ， 我 我 死 死\n",
      "。 媽 媽 平 常 都 忙 著 工 作 ， 連 照 顧 自 己 也 沒 時 間 。 這 次 我 感 覺 到 很 溫 暖 的 母 子 情 。 我 更 多 愛 她 ， 很 愛 她 。 我 我 我 我 更 更 更 的 的 ， 也 很 我 顧 我 己 ， 說 己 都 很 著 ， ， ， 我 我 顧 我 己 己 己 己 ， 。 ， ， ， ， 我 我 我 我 我 自 。 。 。 。 比 更 更 的 的 ， 也 很 很 愛 我 。 但 。 說 說\n",
      "、 為 我 這 時 候 很 高 興 ， 緊 張 ， 又 感 動 、 所 以 不 太 清 楚 我 怎 麼 樣 求 婚 ， 但 是 結 果 我 們 決 定 結 婚 。 因 因 因 因 因 因 因 因 我 我 我 ， 怎 因 因 因 因 又 因 因 因 我 因 又 又 又 又 又 因 因 又 又 又 又 。 。 我 我 我 我 怎 怎 怎 怎 怎 怎 怎 。 因 因 因 因 因 我 我 要 要 、 因 因 因 因 因 因 因 因 因 因 是\n",
      "tensor(371)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>真實標記</th>\n",
       "      <th>預測標記</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>相對的、每位產齡婦女的生育嬰兒個數卻 [持] 續下滑。這表示全球出現適合年齡生育的婦女不想生...</td>\n",
       "      <td>相對的、每位產齡婦女的生育嬰兒個數卻 [持] 續下滑。這表示全球出現適合年齡生育的婦女不想生...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>問 [題] 就在於，從１９５０－１９６０年雖然產齡婦女率低，但平均每位產齡婦女生育的嬰兒率高。</td>\n",
       "      <td>問 [這] 就在於，從１９５０－１９６０年雖然產齡婦女率低，但平均每位產齡婦女生育的嬰兒率高。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>以前戰爭時代沒什麼醫 [療] 設備像現在那麼普遍，但一口家庭生孩子的比率增多，相反地死亡的數...</td>\n",
       "      <td>以前戰爭時代沒什麼醫 [療] 設備像現在那麼普遍，但一口家庭生孩子的比率增多，相反地死亡的數...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>在 [此] ，我想最重要的還是能於培養出一群傑出的資源，能於為國家爭榮、願意貢獻一份力量的人才。</td>\n",
       "      <td>在 [但] ，我想最重要的還是能於培養出一群傑出的資源，能於為國家爭榮、願意貢獻一份力量的人才。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>後來因為人民生得越多，人口大幅 [地] 增加，產生了不少社會問題</td>\n",
       "      <td>後來因為人民生得越多，人口大幅 [度] 增加，產生了不少社會問題</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>你要是錯過這麼好房間，沒有更好的房間喔！你應該早點決定吧！給我 [聯]  [絡] ，拜拜。</td>\n",
       "      <td>你要是錯過這麼好房間，沒有更好的房間喔！你應該早點決定吧！給我 [聯]  [絡] ，拜拜。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>我覺得那個房間非常好，因為房間很大，很乾淨， [再] 加上電費和水費都免費。房間裡面有一個床...</td>\n",
       "      <td>我覺得那個房間非常好，因為房間很大，很乾淨， [再] 加上電費和水費都免費。房間裡面有一個床...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>那一天是我最快樂的一天，因為我跟家人、朋友們、男朋友都一起慶祝我的生日， [幸] 虧他們很愛...</td>\n",
       "      <td>那一天是我最快樂的一天，因為我跟家人、朋友們、男朋友都一起慶祝我的生日， [幸] 虧他們很愛...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>我媽媽平常都忙著工作，連照顧自 [己] 也沒時間。這次我感覺到很溫暖的母子情。我更多愛她，很愛她。</td>\n",
       "      <td>我媽媽平常都忙著工作，連照顧自 [己] 也沒時間。這次我感覺到很溫暖的母子情。我更多愛她，很愛她。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>因為我這時候很高興，緊張，又感動、所以不太清楚我 [怎] 麼樣求婚，但是結果我們決定結婚。</td>\n",
       "      <td>因為我這時候很高興，緊張，又感動、所以不太清楚我 [怎] 麼樣求婚，但是結果我們決定結婚。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>380 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  真實標記  \\\n",
       "0    相對的、每位產齡婦女的生育嬰兒個數卻 [持] 續下滑。這表示全球出現適合年齡生育的婦女不想生...   \n",
       "1      問 [題] 就在於，從１９５０－１９６０年雖然產齡婦女率低，但平均每位產齡婦女生育的嬰兒率高。   \n",
       "2    以前戰爭時代沒什麼醫 [療] 設備像現在那麼普遍，但一口家庭生孩子的比率增多，相反地死亡的數...   \n",
       "3     在 [此] ，我想最重要的還是能於培養出一群傑出的資源，能於為國家爭榮、願意貢獻一份力量的人才。   \n",
       "4                     後來因為人民生得越多，人口大幅 [地] 增加，產生了不少社會問題   \n",
       "..                                                 ...   \n",
       "375      你要是錯過這麼好房間，沒有更好的房間喔！你應該早點決定吧！給我 [聯]  [絡] ，拜拜。   \n",
       "376  我覺得那個房間非常好，因為房間很大，很乾淨， [再] 加上電費和水費都免費。房間裡面有一個床...   \n",
       "377  那一天是我最快樂的一天，因為我跟家人、朋友們、男朋友都一起慶祝我的生日， [幸] 虧他們很愛...   \n",
       "378  我媽媽平常都忙著工作，連照顧自 [己] 也沒時間。這次我感覺到很溫暖的母子情。我更多愛她，很愛她。   \n",
       "379      因為我這時候很高興，緊張，又感動、所以不太清楚我 [怎] 麼樣求婚，但是結果我們決定結婚。   \n",
       "\n",
       "                                                  預測標記  \n",
       "0    相對的、每位產齡婦女的生育嬰兒個數卻 [持] 續下滑。這表示全球出現適合年齡生育的婦女不想生...  \n",
       "1      問 [這] 就在於，從１９５０－１９６０年雖然產齡婦女率低，但平均每位產齡婦女生育的嬰兒率高。  \n",
       "2    以前戰爭時代沒什麼醫 [療] 設備像現在那麼普遍，但一口家庭生孩子的比率增多，相反地死亡的數...  \n",
       "3     在 [但] ，我想最重要的還是能於培養出一群傑出的資源，能於為國家爭榮、願意貢獻一份力量的人才。  \n",
       "4                     後來因為人民生得越多，人口大幅 [度] 增加，產生了不少社會問題  \n",
       "..                                                 ...  \n",
       "375      你要是錯過這麼好房間，沒有更好的房間喔！你應該早點決定吧！給我 [聯]  [絡] ，拜拜。  \n",
       "376  我覺得那個房間非常好，因為房間很大，很乾淨， [再] 加上電費和水費都免費。房間裡面有一個床...  \n",
       "377  那一天是我最快樂的一天，因為我跟家人、朋友們、男朋友都一起慶祝我的生日， [幸] 虧他們很愛...  \n",
       "378  我媽媽平常都忙著工作，連照顧自 [己] 也沒時間。這次我感覺到很溫暖的母子情。我更多愛她，很愛她。  \n",
       "379      因為我這時候很高興，緊張，又感動、所以不太清楚我 [怎] 麼樣求婚，但是結果我們決定結婚。  \n",
       "\n",
       "[380 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num, df = get_test_result(test_dataloader)\n",
    "print(num)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-31743c054d7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# Compute the average accuracy and loss over the validation set.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0my_real_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_test_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m#df.to_csv('test/Bert{}EpochErr_{}.csv'.format(epochs,mode))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-31743c054d7c>\u001b[0m in \u001b[0;36mget_test_result\u001b[1;34m(dataloader)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# Load batch to GPU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mb_input_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_masks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# Compute logits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "mode = \"sentence1times_eWord\"\n",
    "\n",
    "def get_test_result(dataloader):\n",
    "    y_real_s = []\n",
    "    y_pred_s = []\n",
    "    cor_count = 0\n",
    "    \n",
    "    data = {\n",
    "    \"真實標記\":[],\n",
    "    \"預測標記\":[]\n",
    "    }\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_labels, b_masks = tuple(t.to(device) for t in batch)    \n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = bert_classifier(input_ids = b_input_ids, labels = b_labels, attention_mask=b_masks)    \n",
    "        # Compute loss\n",
    "        loss = logits[0]\n",
    "\n",
    "        # Get the predictions    \n",
    "        pred = torch.max(logits[1], 1)[1].data\n",
    "        y_pred_s += list(pred.cpu().numpy())          \n",
    "        y_real_s += list(b_labels.cpu().numpy())\n",
    "\n",
    "        #sentence = tokenizer.decode(b_input_ids)\n",
    "    #df = pd.DataFrame(data)\n",
    "    return  y_real_s, y_pred_s\n",
    "\n",
    "# Compute the average accuracy and loss over the validation set.\n",
    "\n",
    "y_real_s, y_pred_s = get_test_result(val_dataloader)\n",
    "\n",
    "#df.to_csv('test/Bert{}EpochErr_{}.csv'.format(epochs,mode))\n",
    "#df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_result(dataloader):\n",
    "    y_real_s = []\n",
    "    y_pred_s = []\n",
    "    cor_count = 0\n",
    "    \n",
    "    data = {\n",
    "    \"真實標記\":[],\n",
    "    \"預測標記\":[]\n",
    "    }\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_labels, b_mask = tuple(t.to(device) for t in batch)    \n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = bert_classifier(input_ids = b_input_ids, labels = b_labels)    \n",
    "        # Compute loss\n",
    "        loss = logits[0]\n",
    "\n",
    "        # Get the predictions    \n",
    "\n",
    "        for logit, real, b_input_id in zip(logits[1], b_labels, b_input_ids):\n",
    "            token = [int(i) for i in b_input_id.cpu().numpy() if i != 0] \n",
    "            pred = torch.max(logit, 1)[1].data\n",
    "            sentence = tokenizer.decode(b_input_id[:len(token)]) #轉成中文句\n",
    "            realList = sentence.split()\n",
    "            predList = sentence.split()\n",
    "            predLabel = torch.nonzero(pred)\n",
    "            realLabel = torch.nonzero(real)\n",
    "\n",
    "            if realLabel.size()[0] != 0:\n",
    "                if realLabel[0].cpu().numpy() >= len(realList):continue\n",
    "                if realLabel[-1].cpu().numpy() >= len(realList):continue\n",
    "\n",
    "            if predLabel.size()[0] != 0:\n",
    "                if predLabel[0].cpu().numpy() >= len(predList):continue\n",
    "                if predLabel[-1].cpu().numpy() >= len(predList):continue  \n",
    "\n",
    "            if predLabel.size()[0] == 1:            \n",
    "                predList[predLabel[0]] = '['+predList[predLabel[0]]+']'\n",
    "            elif predLabel.size()[0] > 1:\n",
    "                for i in predLabel:\n",
    "                    predList[i[0]] = '['+predList[i[0]]+']'\n",
    "            data[\"預測標記\"].append(''.join(predList)) \n",
    "\n",
    "            if realLabel.size()[0] == 1:            \n",
    "                realList[realLabel[0][0]] = '['+realList[realLabel[0][0]]+']'\n",
    "            elif realLabel.size()[0] > 1:\n",
    "                for i in realLabel:\n",
    "                    realList[i[0]] = '['+realList[i[0]]+']'\n",
    "            data[\"真實標記\"].append(''.join(realList))\n",
    "\n",
    "            corr = torch.zeros((1,max_len)).type(torch.int64)\n",
    "            corr = corr.to(device)\n",
    "\n",
    "            if torch.equal(real, corr[0]):y_real_s += [1]\n",
    "            else:y_real_s += [0]\n",
    "\n",
    "            if torch.equal(pred, corr[0]):y_pred_s += [1] \n",
    "            elif torch.equal(pred, real):\n",
    "                y_pred_s += [0]\n",
    "                cor_count += 1\n",
    "            else:\n",
    "                y_pred_s += [0]\n",
    "\n",
    "        #sentence = tokenizer.decode(b_input_ids)\n",
    "    df = pd.DataFrame(data)\n",
    "    return cor_count, df, y_real_s, y_pred_s\n",
    "\n",
    "# Compute the average accuracy and loss over the validation set.\n",
    "\n",
    "cor_count, df, y_real_s, y_pred_s = get_test_result(val_dataloader)\n",
    "print(cor_count)\n",
    "df.to_csv('test/Bert{}EpochErr_{}.csv'.format(epochs,mode))\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-901ba2673924>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mshow_confusion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_real_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-64-901ba2673924>\u001b[0m in \u001b[0;36mshow_confusion\u001b[1;34m(y_real_s, y_pred_s)\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfmat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mva\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'center'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'center'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'tp'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mconfmat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fn'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mconfmat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fp'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mconfmat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tn'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mconfmat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 180x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "def show_confusion(y_real_s, y_pred_s):\n",
    "    confmat = confusion_matrix(y_true=y_real_s, y_pred=y_pred_s)\n",
    "    fig, ax = plt.subplots(figsize=(2.5, 2.5))\n",
    "    ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
    "    for i in range(confmat.shape[0]):\n",
    "        for j in range(confmat.shape[1]):\n",
    "            ax.text(x=j, y=i, s=confmat[i,j], va='center', ha='center')\n",
    "\n",
    "    cm = {'tp': confmat[0, 0], 'fn': confmat[0, 1], 'fp': confmat[1, 0], 'tn': confmat[1, 1]}\n",
    "    print(cm)\n",
    "    total = sum(cm.values())\n",
    "    print(\"accuracy:\", (cm['tp']+cm['tn'])/total)\n",
    "\n",
    "    recall = (cm['tp'])/(cm['tp']+cm['fn'])\n",
    "    print(\"recall:\", recall)\n",
    "\n",
    "    precision = (cm['tp'])/(cm['tp']+cm['fp'])\n",
    "    print(\"precision:\", precision)\n",
    "\n",
    "    print(\"f1-score:\", 2/((1/precision)+(1/recall)))\n",
    "    plt.xlabel('Prediction')        \n",
    "    plt.ylabel('Real Label')\n",
    "    plt.show()\n",
    "    \n",
    "show_confusion(y_real_s, y_pred_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2/((1/0.972)+(1/1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessing_for_bert_At' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-172-bc8da368bdc3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m ''' \n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mtest_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_masks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing_for_bert_At\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessing_for_bert_At' is not defined"
     ]
    }
   ],
   "source": [
    "labels, texts = [], []\n",
    "with open(\"1209\\\\中央社已處理\\\\test_answer.txt\",'r',encoding='utf-8') as label:\n",
    "    while True:\n",
    "        line = label.readline().strip()\n",
    "        if not line:break\n",
    "        labels.append(1 if '1' in line else 0)\n",
    "        #labels.append([int(i) for i in line]+[0 for j in range(MAX_LEN - len(line))])\n",
    "    \n",
    "with open(\"1209\\\\中央社已處理\\\\test_input.txt\",'r',encoding='utf-8') as text:    \n",
    "    while True:\n",
    "        line = text.readline().strip()\n",
    "        if not line:break\n",
    "        texts.append(line)\n",
    "'''       \n",
    "with open(\"1209\\\\testCorpus.txt\",'r',encoding='utf-8') as test:\n",
    "    while True:\n",
    "        line = test.readline().strip()\n",
    "        if not line:break\n",
    "        texts.append(line)\n",
    "        labels.append([0 for j in range(MAX_LEN)])\n",
    "''' \n",
    "        \n",
    "test_inputs, test_masks = preprocessing_for_bert(texts)\n",
    "test_labels = torch.tensor(labels)\n",
    "print(test_labels.size(), test_inputs.size())\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_labels, test_masks)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "#cor_count, df, y_real_s, y_pred_s = get_test_result(test_dataloader)\n",
    "get_test_result(test_dataloader)\n",
    "\n",
    "#df.to_csv('test/Bert20WLSTMSIGHAN2014.csv'.format(epochs,mode))\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-11ddf39e21d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m '''\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mcor_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_real_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_test_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mshow_confusion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_real_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcor_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-625436f6b47f>\u001b[0m in \u001b[0;36mget_test_result\u001b[1;34m(dataloader)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# Load batch to GPU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mb_input_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# Compute logits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "'''\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs)\n",
    "checkpoint = torch.load('ckpt/bert_weight_10.h5')\n",
    "bert_classifier.load_state_dict(checkpoint)\n",
    "bert_classifier.eval()\n",
    "'''\n",
    "\n",
    "cor_count, df, y_real_s, y_pred_s = get_test_result(test_dataloader)\n",
    "show_confusion(y_real_s, y_pred_s)\n",
    "print(cor_count)\n",
    "df.to_csv('test/Bert{}EpochErr_{}_SIGHAN_40w_16.csv'.format(epochs,mode))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_bert",
   "language": "python",
   "name": "pytorch_bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
